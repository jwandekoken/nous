This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.cursor/
  rules/
    git-conventional-commits.mdc
    node-pnpm.mdc
    python-commands-uv.mdc
    python-fastapi.mdc
    python-uv.mdc
apps/
  api/
    app/
      core/
        __init__.py
        authentication.py
        authorization.py
        schemas.py
        settings.py
      db/
        postgres/
          __init__.py
          auth_session.py
          graph_connection.py
        __init__.py
      features/
        auth/
          dtos/
            __init__.py
            auth_dto.py
          routes/
            api_keys.py
            login.py
            tenants.py
            users.py
          usecases/
            __init__.py
            create_api_key_usecase.py
            create_user_usecase.py
            delete_api_key_usecase.py
            list_api_keys_usecase.py
            login_usecase.py
            signup_tenant_usecase.py
          __init__.py
          models.py
          router.py
        graph/
          dtos/
            __init__.py
            knowledge_dto.py
          models/
            __init__.py
            base_model.py
            entity_model.py
            fact_model.py
            identifier_model.py
            source_model.py
            utils.py
          repositories/
            __init__.py
            age_repository.py
            base.py
            types.py
          routes/
            assimilate.py
            lookup.py
          services/
            langchain_fact_extractor.py
          usecases/
            __init__.py
            assimilate_knowledge_usecase.py
            get_entity_usecase.py
          router.py
        __init__.py
      __init__.py
      main.py
    compose/
      postgres/
        docker-compose.yml
        Dockerfile
    docs/
      plans/
        .gitkeep
      graph_db_schema_age.md
      graph_db_schema.md
      naming_convention.md
      project_architecture.md
      project_scope.md
    migrations/
      versions/
        17010deff34c_fresh_migration_from_models.py
      env.py
      README.md
      script.py.mako
    scripts/
      create_super_admin.py
      setup_postgres_schema.py
    tests/
      db/
        postgres/
          test_postgres_integration.py
      features/
        auth/
          usecases/
            test_create_api_key_usecase_integration.py
            test_create_user_usecase_integration.py
            test_delete_api_key_usecase_integration.py
            test_list_api_keys_usecase_integration.py
            test_login_usecase_integration.py
            test_signup_tenant_usecase_integration.py
        graph/
          repositories/
            test_age_repository_integration.py
          services/
            test_langchain_fact_extractor.py
          usecases/
            test_assimilate_knowledge_usecase_integration.py
            test_get_entity_usecase_integration.py
      utils/
        __init__.py
        database.py
      conftest.py
      TEST_ISOLATION_SUMMARY.md
    .env.example
    alembic.ini
    package.json
    pyproject.toml
    README.md
  web/
    .vscode/
      extensions.json
    docs/
      plans/
        api-layer.md
        architecture.md
    public/
      vite.svg
    src/
      api/
        index.ts
        useApiFetch.ts
      assets/
        styles/
          .gitkeep
        vue.svg
      components/
        layout/
          navigation/
            index.ts
            Navigation.vue
          .gitkeep
        ui/
          alert/
            Alert.vue
            AlertDescription.vue
            AlertTitle.vue
            index.ts
          button/
            Button.vue
            index.ts
          card/
            Card.vue
            CardAction.vue
            CardContent.vue
            CardDescription.vue
            CardFooter.vue
            CardHeader.vue
            CardTitle.vue
            index.ts
          input/
            index.ts
            Input.vue
          menubar/
            index.ts
            Menubar.vue
            MenubarCheckboxItem.vue
            MenubarContent.vue
            MenubarGroup.vue
            MenubarItem.vue
            MenubarLabel.vue
            MenubarMenu.vue
            MenubarRadioGroup.vue
            MenubarRadioItem.vue
            MenubarSeparator.vue
            MenubarShortcut.vue
            MenubarSub.vue
            MenubarSubContent.vue
            MenubarSubTrigger.vue
            MenubarTrigger.vue
          .gitkeep
      features/
        graph/
          api/
            graphApi.ts
          components/
            .gitkeep
          views/
            HomeView.vue
          routes.ts
        login/
          components/
            .gitkeep
          views/
            LoginView.vue
          routes.ts
      lib/
        utils.ts
      router/
        index.ts
      stores/
        .gitkeep
      types/
        .gitkeep
        api.ts
      App.vue
      main.ts
      style.css
    .gitignore
    components.json
    index.html
    package.json
    README.md
    tsconfig.app.json
    tsconfig.json
    tsconfig.node.json
    vite.config.ts
.gitignore
package.json
pnpm-workspace.yaml
pyrightconfig.json
README.md
test_delete_api_key.py
turbo.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/git-conventional-commits.mdc">
---
description: apply this rule when using git
alwaysApply: false
---

# Git Conventional Commits

When using git, always use conventional commits following the format: `type(scope): description`

Common types: feat, fix, docs, style, refactor, test, chore

Examples:

- `feat(auth): add login functionality`
- `fix(api): resolve null pointer exception`
- `docs(readme): update installation instructions`

# Git Conventional Commits

When using git, always use conventional commits following the format: `type(scope): description`

Common types: feat, fix, docs, style, refactor, test, chore

Examples:

- `feat(auth): add login functionality`
- `fix(api): resolve null pointer exception`
- `docs(readme): update installation instructions`
</file>

<file path=".cursor/rules/node-pnpm.mdc">
---
alwaysApply: true
---

# Node Package Manager: pnpm

All Node.js package management operations **must** use `pnpm` instead of `npm` or `yarn`.

## ‚úÖ Always use `pnpm`

- Never use `npm install`, `npm run`, or other npm commands directly
- Never use `yarn add`, `yarn install`, or other yarn commands
- Always prefix commands with `pnpm`

## üîÅ Correct usage

```bash
# Instead of: npm install
pnpm install

# Instead of: npm run build
pnpm build

# Instead of: npm run dev
pnpm dev

# Instead of: yarn add vue-router
pnpm add vue-router
```

## ‚ùå Incorrect usage

```bash
# Don't do this:
npm install
yarn add lodash
npm run build
```

This ensures consistency across the project and leverages pnpm's superior performance and disk efficiency.
</file>

<file path=".cursor/rules/python-commands-uv.mdc">
---
description: Always run Python commands using uv run instead of direct python execution
alwaysApply: true
---

# Python Command Execution with `uv`

All Python commands in this project **must** be executed using `uv run` instead of calling `python` or `python3` directly.

**‚úÖ Always use `uv run`**

- Never run `python script.py`, `python -m pytest`, or similar commands directly
- Always prefix Python execution with `uv run`

**üîÅ Correct usage**

```bash
# Instead of: python script.py
uv run python script.py

# Instead of: python -m pytest tests/
uv run python -m pytest tests/

# Instead of: python -m mymodule
uv run python -m mymodule
```

**‚ùå Incorrect usage**

```bash
# Don't do this:
python script.py
python -m pytest tests/
python3 -c "print('hello')"
```

This ensures that all Python execution uses the correct virtual environment and dependencies managed by `uv`.
</file>

<file path=".cursor/rules/python-fastapi.mdc">
---
description:
globs:
alwaysApply: false
---

You are an expert in Python, FastAPI, and scalable API development.

Key Principles

- Write concise, technical responses with accurate Python examples.
- Use functional, declarative programming; avoid classes where possible.
- Prefer iteration and modularization over code duplication.
- Use descriptive variable names with auxiliary verbs (e.g., is_active, has_permission).
- Use lowercase with underscores for directories and files (e.g., routers/user_routes.py).
- Favor named exports for routes and utility functions.
- Use the Receive an Object, Return an Object (RORO) pattern.

Python/FastAPI

- Use def for pure functions and async def for asynchronous operations.
- Use type hints for all function signatures. Prefer Pydantic models over raw dictionaries for input validation.
- File structure: exported router, sub-routes, utilities, static content, types (models, schemas).
- Avoid unnecessary curly braces in conditional statements.
- For single-line statements in conditionals, omit curly braces.
- Use concise, one-line syntax for simple conditional statements (e.g., if condition: do_something()).

Error Handling and Validation

- Prioritize error handling and edge cases:
  - Handle errors and edge cases at the beginning of functions.
  - Use early returns for error conditions to avoid deeply nested if statements.
  - Place the happy path last in the function for improved readability.
  - Avoid unnecessary else statements; use the if-return pattern instead.
  - Use guard clauses to handle preconditions and invalid states early.
  - Implement proper error logging and user-friendly error messages.
  - Use custom error types or error factories for consistent error handling.

Dependencies

- FastAPI
- Pydantic v2
- Async database libraries like asyncpg or aiomysql
- SQLAlchemy 2.0 (if using ORM features)

FastAPI-Specific Guidelines

- Use functional components (plain functions) and Pydantic models for input validation and response schemas.
- Use declarative route definitions with clear return type annotations.
- Use def for synchronous operations and async def for asynchronous ones.
- Minimize @app.on_event("startup") and @app.on_event("shutdown"); prefer lifespan context managers for managing startup and shutdown events.
- Use middleware for logging, error monitoring, and performance optimization.
- Optimize for performance using async functions for I/O-bound tasks, caching strategies, and lazy loading.
- Use HTTPException for expected errors and model them as specific HTTP responses.
- Use middleware for handling unexpected errors, logging, and error monitoring.
- Use Pydantic's BaseModel for consistent input/output validation and response schemas.

Performance Optimization

- Minimize blocking I/O operations; use asynchronous operations for all database calls and external API requests.
- Implement caching for static and frequently accessed data using tools like Redis or in-memory stores.
- Optimize data serialization and deserialization with Pydantic.
- Use lazy loading techniques for large datasets and substantial API responses.

Key Conventions

1. Rely on FastAPI‚Äôs dependency injection system for managing state and shared resources.
2. Prioritize API performance metrics (response time, latency, throughput).
3. Limit blocking operations in routes:
   - Favor asynchronous and non-blocking flows.
   - Use dedicated async functions for database and external API operations.
   - Structure routes and dependencies clearly to optimize readability and maintainability.

Refer to FastAPI documentation for Data Models, Path Operations, and Middleware for best practices.
</file>

<file path=".cursor/rules/python-uv.mdc">
---
description: this rule must be applied when managing the project, installing or removing dependencies and when running the project in development mode
alwaysApply: false
---

# Package Management with `uv`

These rules define strict guidelines for managing Python dependencies in this project using the `uv` dependency manager.

**‚úÖ Use `uv` exclusively**

- All Python dependencies **must be installed, synchronized, and locked** using `uv`.
- Never use `pip`, `pip-tools`, or `poetry` directly for dependency management.

**üîÅ Managing Dependencies**

Always use these commands:

```bash
# Add or upgrade dependencies
uv add <package>

# Remove dependencies
uv remove <package>

# Reinstall all dependencies from lock file
uv sync
```

**üîÅ Scripts**

```bash
# Run script with proper dependencies
uv run script.py
```

You can edit inline-metadata manually:

```python
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "torch",
#     "torchvision",
#     "opencv-python",
#     "numpy",
#     "matplotlib",
#     "Pillow",
#     "timm",
# ]
# ///

print("some python code")
```

Or using uv cli:

```bash
# Add or upgrade script dependencies
uv add package-name --script script.py

# Remove script dependencies
uv remove package-name --script script.py

# Reinstall all script dependencies from lock file
uv sync --script script.py
```
</file>

<file path="apps/api/app/core/__init__.py">
# Empty file to make core a package
</file>

<file path="apps/api/app/core/schemas.py">
import enum
import uuid

from pydantic import BaseModel


class UserRole(enum.Enum):
    """Defines the roles a user can have."""

    SUPER_ADMIN = "super_admin"
    TENANT_ADMIN = "tenant_admin"
    TENANT_USER = "tenant_user"


class AuthenticatedUser(BaseModel):
    """Represents an authenticated user."""

    user_id: uuid.UUID
    tenant_id: uuid.UUID | None
    role: UserRole
</file>

<file path="apps/api/app/db/__init__.py">
# Empty file to make db a package
</file>

<file path="apps/api/app/features/auth/usecases/create_user_usecase.py">
"""Use case for creating a new user within a tenant."""

from contextlib import AbstractAsyncContextManager
from typing import Callable, Protocol

from fastapi import HTTPException, status
from sqlalchemy.exc import IntegrityError
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.schemas import AuthenticatedUser, UserRole
from app.features.auth.dtos import CreateUserRequest, CreateUserResponse
from app.features.auth.models import User


class PasswordHasher(Protocol):
    """Protocol for password hashing operations."""

    def hash(self, secret: str | bytes, **kwargs) -> str:
        """Hash a password or secret."""
        ...


class CreateUserUseCaseImpl:
    """Implementation of the create user use case."""

    def __init__(
        self,
        password_hasher: PasswordHasher,
        get_db_session: Callable[[], AbstractAsyncContextManager[AsyncSession]],
    ):
        """Initialize the use case with dependencies.

        Args:
            password_hasher: Service for hashing passwords
            get_db_session: Function to get database session
        """
        self.password_hasher = password_hasher
        self.get_auth_db_session = get_db_session

    async def execute(
        self, request: CreateUserRequest, admin_user: AuthenticatedUser
    ) -> CreateUserResponse:
        """Create a new user within the admin's tenant.

        Args:
            request: The user creation request containing user details
            admin_user: The authenticated admin user performing the action

        Returns:
            Response with success message and user ID

        Raises:
            HTTPException: With appropriate status codes for validation and creation errors
        """
        # Validate admin has a tenant
        if admin_user.tenant_id is None:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Admin has no tenant",
            )

        # Validate role restrictions
        if request.role == UserRole.TENANT_ADMIN:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Cannot create another admin",
            )

        if request.role == UserRole.SUPER_ADMIN:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Cannot create a super admin",
            )

        # Hash password
        hashed_password = self.password_hasher.hash(request.password)

        async with self.get_auth_db_session() as session:
            try:
                # Create user
                new_user = User(
                    email=request.email,
                    hashed_password=hashed_password,
                    tenant_id=admin_user.tenant_id,
                    role=request.role,
                )
                session.add(new_user)
                await session.commit()
                await session.refresh(new_user)

                return CreateUserResponse(
                    message="User created successfully",
                    user_id=str(new_user.id),
                    email=new_user.email,
                    role=new_user.role,
                )

            except IntegrityError:
                await session.rollback()
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="User with this email already exists in the tenant",
                )
            except Exception as e:
                await session.rollback()
                # Log the error (in production, use proper logging)
                print(f"Create user error: {e}")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="Failed to create user",
                )
</file>

<file path="apps/api/app/features/auth/__init__.py">
"""Authentication and authorization module."""

from .router import router

__all__ = ["router"]
</file>

<file path="apps/api/app/features/graph/dtos/__init__.py">
"""Graph database DTOs package.

This package contains all Data Transfer Objects for API responses
and requests in the graph database feature.
"""

from .knowledge_dto import (
    AssimilateKnowledgeRequest,
    AssimilateKnowledgeResponse,
    EntityDto,
    FactDto,
    IdentifierDto,
    SourceDto,
)

__all__ = [
    "EntityDto",
    "FactDto",
    "IdentifierDto",
    "AssimilateKnowledgeRequest",
    "AssimilateKnowledgeResponse",
    "SourceDto",
]
</file>

<file path="apps/api/app/features/graph/dtos/knowledge_dto.py">
"""Knowledge assimilation DTOs for API requests and responses.

This module defines Data Transfer Objects for the knowledge assimilation API.
"""

from datetime import datetime, timezone
from uuid import UUID

from pydantic import BaseModel, Field


class EntityDto(BaseModel):
    """DTO for Entity API responses."""

    id: UUID = Field(..., description="Unique system identifier")
    created_at: datetime = Field(
        ..., description="When this entity was created in the system"
    )
    metadata: dict[str, str] | None = Field(
        default_factory=dict, description="Flexible metadata as key-value pairs"
    )


class FactDto(BaseModel):
    """DTO for Fact API responses."""

    name: str = Field(..., description="The name or value of the fact")
    type: str = Field(
        ..., description="The category of fact (e.g., 'Location', 'Company', 'Skill')"
    )
    fact_id: str | None = Field(
        default=None, description="Synthetic primary key (e.g., 'Location:Paris')"
    )


class ExtractedFactDto(BaseModel):
    """DTO for facts extracted from text content."""

    name: str = Field(..., description="The name or value of the extracted fact")
    type: str = Field(
        ..., description="The category of the fact (e.g., 'Location', 'Profession')"
    )
    verb: str = Field(
        ..., description="The semantic verb connecting the entity to the fact"
    )
    confidence_score: float = Field(
        default=1.0,
        ge=0.0,
        le=1.0,
        description="Confidence level of the extracted fact (0.0 to 1.0)",
    )


class IdentifierDto(BaseModel):
    """DTO for an Identifier."""

    value: str = Field(
        ..., description="The identifier value (e.g., 'user@example.com')"
    )
    type: str = Field(
        ..., description="Type of identifier (e.g., 'email', 'phone', 'username')"
    )


class AssimilateKnowledgeRequest(BaseModel):
    """Request to process content and associate facts with an entity."""

    identifier: IdentifierDto = Field(
        ..., description="The entity's external identifier."
    )
    content: str = Field(..., description="The textual content to process.")
    timestamp: datetime | None = Field(
        default_factory=lambda: datetime.now(timezone.utc),
        description="The real-world timestamp of the content's creation.",
    )
    history: list[str] | None = Field(
        default=None,
        description="Optional list of previous conversational turns for context.",
    )


class SourceDto(BaseModel):
    """DTO for a Source."""

    id: UUID = Field(..., description="Unique system identifier")
    content: str = Field(..., description="The original content/source text")
    timestamp: datetime = Field(
        ..., description="Real-world timestamp when the source was created"
    )


class HasFactDto(BaseModel):
    """DTO for the relationship between an Entity and a Fact."""

    verb: str = Field(
        ..., description="Semantic relationship (e.g., 'lives_in', 'works_at')"
    )
    confidence_score: float = Field(
        ge=0.0,
        le=1.0,
        description="Confidence level of this fact (0.0 to 1.0)",
    )
    created_at: datetime = Field(
        ..., description="When this relationship was established"
    )


class AssimilatedFactDto(BaseModel):
    """DTO grouping a fact with its relationship to the entity."""

    fact: FactDto = Field(..., description="The extracted fact")
    relationship: HasFactDto = Field(
        ..., description="The relationship between the entity and the fact"
    )


class AssimilateKnowledgeResponse(BaseModel):
    """Response after successfully assimilating knowledge."""

    entity: EntityDto = Field(
        ..., description="The entity the knowledge was assimilated for."
    )
    source: SourceDto = Field(..., description="The source created from the content.")
    assimilated_facts: list[AssimilatedFactDto] = Field(
        ...,
        description="A list of facts extracted with their relationships to the entity.",
    )


class HasIdentifierDto(BaseModel):
    """DTO for the relationship between an Entity and an Identifier."""

    is_primary: bool = Field(
        ..., description="Whether this is the primary identifier for the entity"
    )
    created_at: datetime = Field(
        ..., description="When this relationship was established"
    )


class IdentifierWithRelationshipDto(BaseModel):
    """DTO grouping an identifier with its relationship to the entity."""

    identifier: IdentifierDto
    relationship: HasIdentifierDto


class FactWithSourceDto(BaseModel):
    """DTO grouping a fact with its relationship and source."""

    fact: FactDto
    relationship: HasFactDto
    source: SourceDto | None = Field(
        None, description="The source of the fact, if available."
    )


class GetEntityResponse(BaseModel):
    """Response for getting an entity by identifier."""

    entity: EntityDto
    identifier: IdentifierWithRelationshipDto
    facts: list[FactWithSourceDto]
</file>

<file path="apps/api/app/features/graph/models/__init__.py">
"""Graph database models package.

This package contains all Pydantic models for the KuzuDB graph schema
including nodes, relationships, API models, and utility functions.
"""

# Base models
from .base_model import GraphBaseModel

# Node models
from .entity_model import Entity
from .fact_model import Fact, HasFact
from .identifier_model import HasIdentifier, Identifier

# Relationship models
from .source_model import DerivedFrom, Source

# Utility models and helper functions
from .utils import (
    EntityWithRelations,
    GraphQueryResult,
    create_entity_with_identifier,
    create_fact_with_source,
)

__all__ = [
    # Base models
    "GraphBaseModel",
    # Node models
    "Entity",
    "Identifier",
    "Fact",
    "Source",
    # Relationship models
    "HasIdentifier",
    "HasFact",
    "DerivedFrom",
    # Utility models
    "GraphQueryResult",
    "EntityWithRelations",
    # Helper functions
    "create_entity_with_identifier",
    "create_fact_with_source",
]
</file>

<file path="apps/api/app/features/graph/models/base_model.py">
"""Base models for graph database entities.

This module defines the base configuration and common functionality
for all graph database models.
"""

from pydantic import BaseModel, ConfigDict


# Base configuration for all models
class GraphBaseModel(BaseModel):
    """Base model for all graph entities with common functionality."""

    model_config = ConfigDict(  # pyright: ignore[reportUnannotatedClassAttribute]
        from_attributes=True,
    )
</file>

<file path="apps/api/app/features/graph/models/entity_model.py">
"""Entity models for graph database.

This module defines the Entity model and related entity-specific functionality.
"""

from datetime import datetime, timezone
from uuid import UUID, uuid4

from pydantic import Field

from .base_model import GraphBaseModel


class Entity(GraphBaseModel):
    """Represents a canonical entity in the graph database.

    The Entity is the central node that represents a real-world subject
    (e.g., a person, organization, or concept) with a stable UUID.
    """

    id: UUID = Field(default_factory=uuid4, description="Unique system identifier")
    created_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc),
        description="When this entity was created in the system",
    )
    metadata: dict[str, str] | None = Field(
        default_factory=dict, description="Flexible metadata as key-value pairs"
    )
</file>

<file path="apps/api/app/features/graph/models/fact_model.py">
"""Fact models for graph database.

This module defines the Fact model for representing discrete pieces
of knowledge or named entities.
"""

from datetime import datetime, timezone
from uuid import UUID

from pydantic import Field, field_validator, model_validator

from .base_model import GraphBaseModel


class Fact(GraphBaseModel):
    """Represents a discrete piece of knowledge or named entity.

    Facts can be locations, companies, hobbies, skills, etc.
    The fact_id is a synthetic key combining type and name.
    """

    name: str = Field(..., description="The name or value of the fact")
    type: str = Field(
        ..., description="The category of fact (e.g., 'Location', 'Company', 'Skill')"
    )
    fact_id: str | None = Field(
        default=None,
        description="Synthetic primary key (e.g., 'Location:Paris')",
        # make it non-editable after creation
        frozen=True,
    )

    @field_validator("name", "type")
    @classmethod
    def validate_not_empty(cls, v: str) -> str:
        """Ensure name and type are not empty."""
        if not v or not v.strip():
            raise ValueError("Field cannot be empty")
        return v.strip()

    @model_validator(mode="after")
    def compute_fact_id(self) -> "Fact":
        """Generate the synthetic fact_id from name and type."""
        # This code runs after 'name' and 'type' have been validated.
        # 'self' here is the complete model, so self.name and self.type are safe to access.
        if self.name and self.type:
            # Pydantic v2 protects frozen fields, so we use `object.__setattr__`
            # to assign the computed value.
            object.__setattr__(
                self, "fact_id", self.create_fact_id(self.type, self.name)
            )
        return self

    @classmethod
    def create_fact_id(cls, fact_type: str, name: str) -> str:
        """Helper method to create a synthetic fact_id."""
        return f"{fact_type}:{name}"


class HasFact(GraphBaseModel):
    """Relationship connecting an Entity to a Fact it possesses.

    The verb provides semantic context about how the entity relates to the fact.
    """

    from_entity_id: UUID = Field(..., description="Entity that possesses the fact")
    to_fact_id: str = Field(..., description="Fact being connected")
    verb: str = Field(
        ..., description="Semantic relationship (e.g., 'lives_in', 'works_at')"
    )
    confidence_score: float = Field(
        ge=0.0,
        le=1.0,
        default=1.0,
        description="Confidence level of this fact (0.0 to 1.0)",
    )
    created_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc),
        description="When this relationship was established",
    )

    @field_validator("verb")
    @classmethod
    def validate_verb(cls, v: str) -> str:
        """Ensure verb is a valid semantic relationship."""
        if not v or not v.strip():
            raise ValueError("Verb cannot be empty")
        return v.strip().lower()
</file>

<file path="apps/api/app/features/graph/models/identifier_model.py">
"""Identifier models for graph database.

This module defines the Identifier model for external identifiers
like email addresses, phone numbers, usernames, etc.
"""

from datetime import datetime, timezone
from uuid import UUID

from pydantic import Field, field_validator

from .base_model import GraphBaseModel


class Identifier(GraphBaseModel):
    """Represents an external identifier for an entity.

    Examples: email addresses, phone numbers, usernames, etc.
    The value serves as the primary key for uniqueness.
    """

    value: str = Field(
        ..., description="The identifier value (e.g., 'user@example.com')"
    )
    type: str = Field(
        ..., description="Type of identifier (e.g., 'email', 'phone', 'username')"
    )

    @field_validator("value")
    @classmethod
    def validate_value(cls, v: str) -> str:
        """Ensure identifier value is not empty and properly formatted."""
        if not v or not v.strip():
            raise ValueError("Identifier value cannot be empty")
        return v.strip()

    @field_validator("type")
    @classmethod
    def validate_type(cls, v: str) -> str:
        """Ensure identifier type is valid."""
        valid_types = {"email", "phone", "username", "uuid", "social_id"}
        if v not in valid_types:
            raise ValueError(f"Identifier type must be one of: {valid_types}")
        return v


class HasIdentifier(GraphBaseModel):
    """Relationship connecting an Entity to its external Identifiers.

    This relationship allows entities to have multiple identifiers
    while maintaining a canonical UUID as the primary key.
    """

    from_entity_id: UUID = Field(..., description="Entity that owns the identifier")
    to_identifier_value: str = Field(
        ..., description="Identifier value being connected"
    )
    is_primary: bool = Field(
        default=False,
        description="Whether this is the primary identifier for the entity",
    )
    created_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc),
        description="When this relationship was established",
    )
</file>

<file path="apps/api/app/features/graph/models/source_model.py">
"""Source models for graph database.

This module defines the Source model for tracking the origin
of information in the graph.
"""

from datetime import datetime, timezone
from uuid import UUID, uuid4

from pydantic import Field, field_validator

from .base_model import GraphBaseModel


class Source(GraphBaseModel):
    """Represents the origin of information in the graph.

    Sources track where facts came from (chat messages, emails, documents, etc.)
    enabling traceability and data provenance.
    """

    id: UUID = Field(default_factory=uuid4, description="Unique system identifier")
    content: str = Field(..., description="The original content/source text")
    timestamp: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc),
        description="Real-world timestamp when the source was created",
    )

    @field_validator("content")
    @classmethod
    def validate_content(cls, v: str) -> str:
        """Ensure content is not empty."""
        if not v or not v.strip():
            raise ValueError("Source content cannot be empty")
        return v.strip()


class DerivedFrom(GraphBaseModel):
    """Relationship connecting a Fact to its Source.

    This enables traceability by linking facts back to their origins,
    answering the question: "How do we know this fact?"
    """

    from_fact_id: str = Field(..., description="Fact that was derived")
    to_source_id: UUID = Field(..., description="Source where the fact originated")
</file>

<file path="apps/api/app/features/graph/models/utils.py">
"""Utility models and helper functions for graph database.

This module contains utility models and helper functions that don't
fit into the main model categories.
"""

from datetime import datetime, timezone
from typing import Any

from pydantic import Field

from .base_model import GraphBaseModel
from .entity_model import Entity
from .fact_model import Fact
from .identifier_model import HasIdentifier, Identifier
from .source_model import DerivedFrom, Source


# Utility Models for API Operations
class GraphQueryResult(GraphBaseModel):
    """Result of a graph database query operation."""

    success: bool = Field(..., description="Whether the operation succeeded")
    data: list[dict[str, Any]] | None = Field(
        default_factory=list, description="Query results as list of dictionaries"
    )
    error: str | None = Field(None, description="Error message if operation failed")
    metadata: dict[str, Any] | None = Field(
        default_factory=dict, description="Additional metadata about the operation"
    )


class EntityWithRelations(GraphBaseModel):
    """Entity with its associated identifiers and facts for full representation."""

    entity: Entity
    identifiers: list[Identifier] = Field(default_factory=list)
    facts: list[dict[str, Any]] = Field(
        default_factory=list, description="Facts with relationship metadata"
    )
    primary_identifier: Identifier | None = None


# Helper Functions
def create_entity_with_identifier(
    identifier_value: str,
    identifier_type: str,
    metadata: dict[str, str] | None = None,
) -> tuple[Entity, Identifier, HasIdentifier]:
    """Helper function to create an entity with its primary identifier.

    Args:
        identifier_value: The identifier value (e.g., email)
        identifier_type: Type of identifier
        metadata: Optional entity metadata

    Returns:
        Tuple of (Entity, Identifier, HasIdentifier relationship)
    """
    entity = Entity(metadata=metadata or {})
    identifier = Identifier(value=identifier_value, type=identifier_type)
    relationship = HasIdentifier(
        from_entity_id=entity.id, to_identifier_value=identifier.value, is_primary=True
    )

    return entity, identifier, relationship


def create_fact_with_source(
    name: str,
    fact_type: str,
    source_content: str,
    source_timestamp: datetime | None = None,
) -> tuple[Fact, Source, DerivedFrom]:
    """Helper function to create a fact with its source.

    Args:
        name: Name of the fact
        fact_type: Type/category of the fact
        source_content: Content where the fact was found
        source_timestamp: When the source was created

    Returns:
        Tuple of (Fact, Source, DerivedFrom relationship)
    """
    fact_id = Fact.create_fact_id(fact_type, name)
    fact = Fact(fact_id=fact_id, name=name, type=fact_type)

    source = Source(
        content=source_content, timestamp=source_timestamp or datetime.now(timezone.utc)
    )

    relationship = DerivedFrom(from_fact_id=fact_id, to_source_id=source.id)

    return fact, source, relationship
</file>

<file path="apps/api/app/features/graph/repositories/__init__.py">
"""Graph repositories package."""

from .age_repository import AgeRepository

__all__ = ["AgeRepository"]
</file>

<file path="apps/api/app/features/graph/repositories/base.py">
"""Base classes and protocols for graph repositories."""

from typing import Protocol

from app.features.graph.models import Entity, Fact, HasIdentifier, Identifier, Source
from app.features.graph.repositories.types import (
    AddFactToEntityResult,
    CreateEntityResult,
    FactWithOptionalSource,
    FindEntityByIdResult,
    FindEntityResult,
)


class GraphRepository(Protocol):
    """Protocol for a generic graph repository."""

    async def create_entity(
        self, entity: Entity, identifier: Identifier, relationship: HasIdentifier
    ) -> CreateEntityResult:
        """Create a new entity with an identifier."""
        ...

    async def find_entity_by_identifier(
        self, identifier_value: str, identifier_type: str
    ) -> FindEntityResult | None:
        """Find an entity by its identifier."""
        ...

    async def find_entity_by_id(self, entity_id: str) -> FindEntityByIdResult | None:
        """Find an entity by its ID."""
        ...

    async def delete_entity_by_id(self, entity_id: str) -> bool:
        """Delete an entity by its ID."""
        ...

    async def add_fact_to_entity(
        self,
        entity_id: str,
        fact: Fact,
        source: Source,
        verb: str,
        confidence_score: float = 1.0,
        create_source: bool = True,
    ) -> AddFactToEntityResult:
        """Add a fact to an entity."""
        ...

    async def find_fact_by_id(self, fact_id: str) -> FactWithOptionalSource | None:
        """Find a fact by its ID."""
        ...
</file>

<file path="apps/api/app/features/graph/repositories/types.py">
"""Type definitions for graph repository operations."""

from typing import TypedDict

from app.features.graph.models import (
    DerivedFrom,
    Entity,
    Fact,
    HasFact,
    HasIdentifier,
    Identifier,
    Source,
)


class CreateEntityResult(TypedDict):
    """Result of creating a new entity with its identifier and relationship."""

    entity: Entity
    identifier: Identifier
    relationship: HasIdentifier


class FactWithSource(TypedDict):
    """A fact with its associated source information."""

    fact: Fact
    source: Source | None
    relationship: HasFact


class FactWithOptionalSource(TypedDict):
    """A fact with its optionally associated source information."""

    fact: Fact
    source: Source | None


class IdentifierWithRelationship(TypedDict):
    """Groups the identifier used for the lookup with its relationship to the entity."""

    identifier: Identifier
    relationship: HasIdentifier


class FindEntityResult(TypedDict):
    """Result of finding an entity by its identifier, including facts and sources."""

    entity: Entity
    identifier: IdentifierWithRelationship
    facts_with_sources: list[FactWithSource]


class FindEntityByIdResult(TypedDict):
    """Result of finding an entity by its ID, including facts and sources."""

    entity: Entity
    identifier: IdentifierWithRelationship | None
    facts_with_sources: list[FactWithSource]


class EntityWithRelations(TypedDict):
    """Complete entity data with all its relationships and associated objects."""

    entity: Entity
    identifiers: list[Identifier]
    facts_with_sources: list[FactWithSource]


class AddFactToEntityResult(TypedDict):
    """Result of adding a fact with source to an entity."""

    fact: Fact
    source: Source
    has_fact_relationship: HasFact
    derived_from_relationship: DerivedFrom
</file>

<file path="apps/api/app/features/graph/routes/assimilate.py">
"""Assimilate knowledge route handler."""

from typing import Protocol

from fastapi import APIRouter, Depends

from app.core.authorization import TenantInfo, get_tenant_info
from app.db.postgres.graph_connection import get_graph_db_pool
from app.features.graph.dtos.knowledge_dto import (
    AssimilateKnowledgeRequest,
    AssimilateKnowledgeResponse,
)
from app.features.graph.repositories.age_repository import AgeRepository
from app.features.graph.services.langchain_fact_extractor import LangChainFactExtractor
from app.features.graph.usecases import AssimilateKnowledgeUseCaseImpl


class AssimilateKnowledgeUseCase(Protocol):
    """Protocol for the assimilate knowledge use case."""

    async def execute(
        self, request: AssimilateKnowledgeRequest
    ) -> AssimilateKnowledgeResponse:
        """Process content and associate facts with an entity."""
        ...


# Create the fact extractor instance at module level to avoid instantiation issues
_fact_extractor = LangChainFactExtractor()


async def get_assimilate_knowledge_use_case(
    tenant_info: TenantInfo = Depends(get_tenant_info),
) -> AssimilateKnowledgeUseCase:
    """Dependency injection for the assimilate knowledge use case."""

    pool = await get_graph_db_pool()
    repository = AgeRepository(pool, graph_name=tenant_info.graph_name)
    return AssimilateKnowledgeUseCaseImpl(
        repository=repository, fact_extractor=_fact_extractor
    )


router = APIRouter()


@router.post("/entities/assimilate", response_model=AssimilateKnowledgeResponse)
async def assimilate_knowledge(
    request: AssimilateKnowledgeRequest,
    use_case: AssimilateKnowledgeUseCase = Depends(get_assimilate_knowledge_use_case),
) -> AssimilateKnowledgeResponse:
    """Assimilate knowledge by processing content and associating facts with an entity.

    This endpoint processes textual content, extracts facts, and associates them
    with the specified entity in the knowledge graph.
    """

    return await use_case.execute(request)
</file>

<file path="apps/api/app/features/graph/routes/lookup.py">
"""Entity lookup route handler."""

from typing import Protocol

from fastapi import APIRouter, Depends

from app.core.authorization import TenantInfo, get_tenant_info
from app.db.postgres.graph_connection import get_graph_db_pool
from app.features.graph.dtos.knowledge_dto import GetEntityResponse
from app.features.graph.repositories.age_repository import AgeRepository
from app.features.graph.usecases import GetEntityUseCaseImpl


class GetEntityUseCase(Protocol):
    """Protocol for the get entity use case."""

    async def execute(
        self, identifier_value: str, identifier_type: str
    ) -> GetEntityResponse:
        """Retrieve entity information by identifier."""
        ...


async def get_get_entity_use_case(
    tenant_info: TenantInfo = Depends(get_tenant_info),
) -> GetEntityUseCase:
    """Dependency injection for the get entity use case."""

    pool = await get_graph_db_pool()
    repository = AgeRepository(pool, graph_name=tenant_info.graph_name)
    return GetEntityUseCaseImpl(repository=repository)


router = APIRouter()


@router.get("/entities/lookup", response_model=GetEntityResponse)
async def get_entity(
    type: str,
    value: str,
    use_case: GetEntityUseCase = Depends(get_get_entity_use_case),
) -> GetEntityResponse:
    """Retrieve entity information by identifier.

    This endpoint looks up an entity using an external identifier (e.g., email, phone)
    and returns the entity details along with all associated facts and their sources.
    """

    return await use_case.execute(identifier_value=value, identifier_type=type)
</file>

<file path="apps/api/app/features/graph/services/langchain_fact_extractor.py">
"""Fact extraction service using LangChain and Google's Gemini model."""

from typing import cast

from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from pydantic import BaseModel, Field

from app.core.settings import Settings
from app.features.graph.dtos.knowledge_dto import ExtractedFactDto, IdentifierDto


class ExtractedFact(BaseModel):
    """A single, discrete fact extracted from a text."""

    name: str = Field(
        ...,
        description="The name or value of the fact (e.g., 'Paris', 'Software Engineer')",
    )
    type: str = Field(
        ...,
        description="The category of the fact (e.g., 'Location', 'Profession', 'Hobby')",
    )
    verb: str = Field(
        ...,
        description="The semantic verb connecting the entity to the fact (e.g., 'lives_in', 'works_at', 'is_a')",
    )
    confidence_score: float = Field(
        default=1.0,
        ge=0.0,
        le=1.0,
        description="The confidence score of the extracted fact, from 0.0 to 1.0",
    )


class FactList(BaseModel):
    """A list of facts extracted from a text."""

    facts: list[ExtractedFact]


class LangChainFactExtractor:
    """Extracts facts from text content using LangChain and Gemini.

    This service can process text in any language, but standardizes the output
    by generating fact 'types' and 'verbs' in English, while keeping the fact
    'name' in the original language.
    """

    def __init__(self):
        """Initialize the fact extractor with LangChain and Gemini model."""
        # Create settings that will read from current environment
        settings = Settings()
        if not settings.google_api_key:
            raise ValueError("GOOGLE_API_KEY environment variable not set.")

        # Create the prompt template for fact extraction
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """You are an expert at extracting key facts about a specific entity from text.
The text is a turn in a conversation. Your task is to identify discrete, meaningful facts from the provided text that are relevant to the entity identified by: {entity_identifier}.
You may also be provided with the history of the conversation for context.

Guidelines:
- The input text can be in any language. However, the `type` and `verb` values in your output **MUST be in valid English**. The `name` value should remain in the original language of the input text.
- Extract facts that are specific and verifiable, but also sentiments or opinions if they are stated as facts by the entity (e.g., 'likes chocolate', 'dislikes flying').
- For each fact, provide a 'verb' that describes the relationship from the entity to the fact (e.g., 'lives_in', 'works_at', 'is_a', 'likes', 'dislikes').
- Use clear, concise names and appropriate categories for each fact.
- Provide a confidence score (0.0 to 1.0) indicating how certain you are about the extracted fact.
- Focus on facts that would be useful for building a knowledge graph about the entity's preferences, statements, and characteristics.
- If the text contains no new facts about the entity, return an empty list.
- Avoid extracting subjective opinions or interpretations from *other* people in the conversation, focus on the identified entity.
- Ignore generic statements, meta-comments, or information that isn't a specific characteristic, preference, or action of the entity. For example, from 'This is a test entity with minimal information.', no facts should be extracted.

Example 1:
If the entity is 'email:john.doe@example.com' and the text is 'I really enjoy hiking on weekends.', the output should be:
[
    {{ "name": "Hiking", "type": "Hobby", "verb": "enjoys", "confidence_score": 1.0 }}
]

Example 2:
If the entity is 'email:jane.doe@example.com' and the text is 'I think that new project is a bad idea.', the output could be:
[
    {{ "name": "new project", "type": "Opinion", "verb": "considers_bad_idea", "confidence_score": 0.9 }}
]

Example 3:
If the entity is 'name:Mariele' and the text is 'De tomar a decis√£o correta em uma empresa nova que eu e meu marido vamos abrir', the output could be:
[
    {{ "name": "tomar a decis√£o correta em uma empresa nova", "type": "Goal", "verb": "wants_to_make_right_decision", "confidence_score": 0.9 }}
]""",
                ),
                ("human", "{history_section}Here is the text to analyze:\n\n{content}"),
            ]
        )

        # Initialize the Gemini model
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            temperature=0,
            google_api_key=settings.google_api_key,
        )

        # Create structured output chain
        structured_llm = llm.with_structured_output(FactList)
        self.chain = prompt | structured_llm

    async def extract_facts(
        self,
        content: str,
        entity_identifier: IdentifierDto,
        history: list[str] | None = None,
    ) -> list[ExtractedFactDto]:
        """Extracts facts and converts them to the required dictionary format.

        Args:
            content: The text content to extract facts from
            entity_identifier: The entity identifier payload
            history: Optional list of previous conversational turns for context.

        Returns:
            List of dictionaries containing fact name, type, verb and confidence
        """
        history_section = ""
        if history:
            history_section = (
                "For context, here is the preceding conversation:\n"
                + "\n".join(history)
                + "\n\n"
            )

        response: FactList = cast(
            FactList,
            await self.chain.ainvoke(
                {
                    "content": content,
                    "entity_identifier": f"{entity_identifier.type}:{entity_identifier.value}",
                    "history_section": history_section,
                }
            ),
        )
        return [
            ExtractedFactDto(
                name=fact.name,
                type=fact.type,
                verb=fact.verb,
                confidence_score=fact.confidence_score,
            )
            for fact in response.facts
        ]
</file>

<file path="apps/api/app/features/graph/usecases/__init__.py">
"""Graph database use cases package.

This package contains all use cases for the graph database feature.
"""

from .assimilate_knowledge_usecase import (
    AssimilateKnowledgeUseCaseImpl,
)
from .get_entity_usecase import (
    GetEntityUseCaseImpl,
)

__all__ = [
    "AssimilateKnowledgeUseCaseImpl",
    "GetEntityUseCaseImpl",
]
</file>

<file path="apps/api/app/features/graph/usecases/assimilate_knowledge_usecase.py">
"""Use case for assimilating knowledge into the graph database.

This module defines the use case for processing textual content,
extracting facts, and associating them with entities.
"""

from datetime import datetime, timezone
from typing import Protocol, cast
from uuid import uuid4

from app.features.graph.dtos.knowledge_dto import (
    AssimilatedFactDto,
    AssimilateKnowledgeRequest,
    AssimilateKnowledgeResponse,
    EntityDto,
    ExtractedFactDto,
    FactDto,
    HasFactDto,
    IdentifierDto,
    SourceDto,
)
from app.features.graph.models import (
    Entity,
    Fact,
    HasIdentifier,
    Identifier,
    Source,
)
from app.features.graph.repositories.base import GraphRepository


class FactExtractor(Protocol):
    """Protocol for extracting facts from text content."""

    async def extract_facts(
        self,
        content: str,
        entity_identifier: IdentifierDto,
        history: list[str] | None = None,
    ) -> list[ExtractedFactDto]:
        """Extract facts from text content."""
        ...


class AssimilateKnowledgeUseCaseImpl:
    """Implementation of the assimilate knowledge use case."""

    def __init__(self, repository: GraphRepository, fact_extractor: FactExtractor):
        """Initialize the use case with dependencies.

        Args:
            repository: Repository for graph database operations
            fact_extractor: Service for extracting facts from text
        """
        self.repository: GraphRepository = repository
        self.fact_extractor: FactExtractor = fact_extractor

    async def execute(
        self, request: AssimilateKnowledgeRequest
    ) -> AssimilateKnowledgeResponse:
        """Process content and associate facts with an entity.

        Args:
            request: The request containing identifier, content, and timestamp

        Returns:
            Response containing the entity, source, and extracted facts
        """
        # 1. Find or create entity based on identifier
        entity_result = await self.repository.find_entity_by_identifier(
            request.identifier.value, request.identifier.type
        )

        if entity_result is None:
            # Create new entity with identifier
            new_entity = Entity(id=uuid4(), created_at=datetime.now(timezone.utc))
            identifier = Identifier(
                value=request.identifier.value, type=request.identifier.type
            )
            has_identifier = HasIdentifier(
                from_entity_id=new_entity.id,
                to_identifier_value=identifier.value,
                is_primary=True,
                created_at=datetime.now(timezone.utc),
            )

            create_result = await self.repository.create_entity(
                new_entity, identifier, has_identifier
            )
            entity_result = {
                "entity": create_result["entity"],
                "identifier": create_result["identifier"],
                "relationship": create_result["relationship"],
            }

        entity: Entity = cast(Entity, entity_result["entity"])

        # 2. Create source from content and timestamp
        source = Source(
            id=uuid4(),
            content=request.content,
            timestamp=request.timestamp or datetime.now(timezone.utc),
        )

        # 3. Extract facts using fact_extractor
        extracted_facts_data = await self.fact_extractor.extract_facts(
            request.content, request.identifier, request.history
        )
        assimilated_facts: list[AssimilatedFactDto] = []

        # 4. Create and link facts to entity
        for i, fact_data in enumerate(extracted_facts_data):
            # Create fact model
            fact = Fact(name=fact_data.name, type=fact_data.type)

            # Ensure fact_id is not None (this should be set by the model validator)
            if not fact.fact_id:
                raise ValueError(f"Fact ID cannot be None for fact: {fact.name}")

            # Add fact to entity using repository method
            # Create source only for the first fact
            create_source = i == 0
            result = await self.repository.add_fact_to_entity(
                entity_id=str(entity.id),
                fact=fact,
                source=source,
                verb=fact_data.verb,
                confidence_score=fact_data.confidence_score,
                create_source=create_source,
            )

            # Add to response
            fact_dto = FactDto(
                name=result["fact"].name,
                type=result["fact"].type,
                fact_id=result["fact"].fact_id,
            )
            has_fact_dto = HasFactDto(
                verb=result["has_fact_relationship"].verb,
                confidence_score=result["has_fact_relationship"].confidence_score,
                created_at=result["has_fact_relationship"].created_at,
            )
            assimilated_fact_dto = AssimilatedFactDto(
                fact=fact_dto,
                relationship=has_fact_dto,
            )
            assimilated_facts.append(assimilated_fact_dto)

        # 4. Return response with entity, source, and assimilated facts
        return AssimilateKnowledgeResponse(
            entity=EntityDto(
                id=entity.id,
                created_at=entity.created_at,
                metadata=entity.metadata or {},
            ),
            source=SourceDto(
                id=source.id,
                content=source.content,
                timestamp=source.timestamp,
            ),
            assimilated_facts=assimilated_facts,
        )
</file>

<file path="apps/api/app/features/graph/usecases/get_entity_usecase.py">
"""Use case for retrieving entity information by identifier.

This module defines the use case for fetching entity details including
their identifiers and associated facts with sources.
"""

from fastapi import HTTPException, status

from app.features.graph.dtos.knowledge_dto import (
    EntityDto,
    FactDto,
    FactWithSourceDto,
    GetEntityResponse,
    HasFactDto,
    HasIdentifierDto,
    IdentifierDto,
    IdentifierWithRelationshipDto,
    SourceDto,
)
from app.features.graph.models.fact_model import Fact
from app.features.graph.repositories.base import GraphRepository
from app.features.graph.repositories.types import FindEntityResult


class GetEntityUseCaseImpl:
    """Implementation of the get entity use case."""

    def __init__(self, repository: GraphRepository):
        """Initialize the use case with dependencies.

        Args:
            repository: Repository for graph database operations
        """
        self.repository: GraphRepository = repository

    async def execute(
        self, identifier_value: str, identifier_type: str
    ) -> GetEntityResponse:
        """Retrieve entity information by identifier.

        Args:
            identifier_value: The identifier value (e.g., 'user@example.com')
            identifier_type: The identifier type (e.g., 'email', 'phone')

        Returns:
            GetEntityResponse containing the entity, identifier, and facts

        Raises:
            HTTPException: If the entity is not found (404)
        """
        entity_result: (
            FindEntityResult | None
        ) = await self.repository.find_entity_by_identifier(
            identifier_value, identifier_type
        )

        if entity_result is None:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Entity with identifier '{identifier_type}:{identifier_value}' not found",
            )

        # Map entity to DTO
        entity_dto = EntityDto(
            id=entity_result["entity"].id,
            created_at=entity_result["entity"].created_at,
            metadata=entity_result["entity"].metadata or {},
        )

        # Map identifier and relationship to DTOs
        identifier_dto = IdentifierDto(
            value=entity_result["identifier"]["identifier"].value,
            type=entity_result["identifier"]["identifier"].type,
        )
        has_identifier_dto = HasIdentifierDto(
            is_primary=entity_result["identifier"]["relationship"].is_primary,
            created_at=entity_result["identifier"]["relationship"].created_at,
        )
        identifier_with_relationship_dto = IdentifierWithRelationshipDto(
            identifier=identifier_dto,
            relationship=has_identifier_dto,
        )

        # Map facts with sources to DTOs
        facts_with_sources_dto: list[FactWithSourceDto] = []
        for fact_with_source in entity_result["facts_with_sources"]:
            fact: Fact = fact_with_source["fact"]
            relationship_dto = HasFactDto(
                verb=fact_with_source["relationship"].verb,
                confidence_score=fact_with_source["relationship"].confidence_score,
                created_at=fact_with_source["relationship"].created_at,
            )
            source_dto = None
            if fact_with_source["source"] is not None:
                source_dto = SourceDto(
                    id=fact_with_source["source"].id,
                    content=fact_with_source["source"].content,
                    timestamp=fact_with_source["source"].timestamp,
                )
            fact_with_source_dto = FactWithSourceDto(
                fact=FactDto(
                    name=fact.name,
                    type=fact.type,
                    fact_id=fact.fact_id,
                ),
                relationship=relationship_dto,
                source=source_dto,
            )
            facts_with_sources_dto.append(fact_with_source_dto)

        return GetEntityResponse(
            entity=entity_dto,
            identifier=identifier_with_relationship_dto,
            facts=facts_with_sources_dto,
        )
</file>

<file path="apps/api/app/features/__init__.py">
# Empty file to make features a package
</file>

<file path="apps/api/app/__init__.py">
# Empty file to make app a package
</file>

<file path="apps/api/compose/postgres/Dockerfile">
# Use a specific version of the official PostgreSQL image for reproducibility
ARG PG_VERSION=16
FROM postgres:${PG_VERSION}

# Re-declare the ARG to make it available in the rest of the build
ARG PG_VERSION

# Install build dependencies required for compiling extensions
# Install build dependencies required for compiling extensions
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    git \
    ca-certificates \
    flex \
    bison \
    postgresql-server-dev-${PG_VERSION} \
    && rm -rf /var/lib/apt/lists/*

# --- Install Apache AGE ---
# Clone the Apache AGE repository
WORKDIR /usr/src
RUN git clone https://github.com/apache/age.git

# Build and install the AGE extension
WORKDIR /usr/src/age
RUN make install

# --- Install pgvector ---
# Clone the pgvector repository
WORKDIR /usr/src
RUN git clone --branch v0.7.2 https://github.com/pgvector/pgvector.git

# Build and install the pgvector extension
WORKDIR /usr/src/pgvector
RUN make install

# --- Cleanup ---
# Remove build dependencies and source files to reduce image size
WORKDIR /
RUN apt-get purge -y --auto-remove build-essential git postgresql-server-dev-${PG_VERSION} \
    && rm -rf /usr/src/age /usr/src/pgvector

# Switch back to the postgres user
USER postgres
</file>

<file path="apps/api/docs/graph_db_schema_age.md">
# Apache AGE Graph Schema for a Knowledge Graph

This document outlines the graph schema for Apache AGE, a PostgreSQL extension for graph databases. It is designed to implement the conceptual model defined in `graph_db_schema.md`.

Apache AGE follows a dynamic schema approach, meaning vertex and edge labels (types) do not need to be explicitly defined before use. They are created automatically when the first vertex or edge with that label is created. However, for data integrity and query performance, it is crucial to define unique constraints and indexes on key properties.

## 1. Initial Setup

Before creating any data, you must load the AGE extension and create a graph.

```sql
-- Load the AGE extension
LOAD 'age';

-- Set the search path to include the graph
SET search_path = ag_catalog, "$user", public;

-- Create the graph (if it doesn't exist)
SELECT create_graph('knowledge_graph');
```

## 2. Schema Definition (Constraints and Indexes)

The following commands should be run to enforce the schema's integrity and ensure efficient lookups. These are written in a mix of SQL and Cypher, as executed through AGE's functions.

### Vertex Labels

#### `Entity`

- **Purpose**: The canonical subject of the graph.
- **Commands**:

  ```sql
  -- Although the label is created on first use, we must create a UNIQUE constraint on the 'id' property.
  -- This requires creating at least one node first.
  SELECT * FROM cypher('knowledge_graph', $$
      CREATE (:Entity {id: 'temp'})
  $$) AS (v agtype);

  CREATE UNIQUE INDEX ON entity ((properties->>'id'));

  -- Clean up the temporary node
  SELECT * FROM cypher('knowledge_graph', $$
      MATCH (e:Entity {id: 'temp'}) DELETE e
  $$) AS (v agtype);
  ```

#### `Identifier`

- **Purpose**: An external identifier for an entity.
- **Commands**:

  ```sql
  -- Create a temporary node to establish the label
  SELECT * FROM cypher('knowledge_graph', $$
      CREATE (:Identifier {value: 'temp'})
  $$) AS (v agtype);

  CREATE UNIQUE INDEX ON identifier ((properties->>'value'));

  -- Clean up the temporary node
  SELECT * FROM cypher('knowledge_graph', $$
      MATCH (i:Identifier {value: 'temp'}) DELETE i
  $$) AS (v agtype);
  ```

#### `Fact`

- **Purpose**: A discrete piece of knowledge.
- **Commands**:

  ```sql
  -- Create a temporary node to establish the label
  SELECT * FROM cypher('knowledge_graph', $$
      CREATE (:Fact {fact_id: 'temp'})
  $$) AS (v agtype);

  CREATE UNIQUE INDEX ON fact ((properties->>'fact_id'));

  -- Clean up the temporary node
  SELECT * FROM cypher('knowledge_graph', $$
      MATCH (f:Fact {fact_id: 'temp'}) DELETE f
  $$) AS (v agtype);
  ```

#### `Source`

- **Purpose**: The origin of the information.
- **Commands**:

  ```sql
  -- Create a temporary node to establish the label
  SELECT * FROM cypher('knowledge_graph', $$
      CREATE (:Source {id: 'temp'})
  $$) AS (v agtype);

  CREATE UNIQUE INDEX ON source ((properties->>'id'));

  -- Clean up the temporary node
  SELECT * FROM cypher('knowledge_graph', $$
      MATCH (s:Source {id: 'temp'}) DELETE s
  $$) AS (v agtype);
  ```

### Edge Labels

Edge labels (`HAS_IDENTIFIER`, `HAS_FACT`, `DERIVED_FROM`) are created implicitly when an edge with that label is first created. There are no constraints or indexes applied to edges in this schema.

## 3. Example Cypher Queries for Data Creation

The following `CREATE` queries demonstrate how to insert data that conforms to this schema. Executing these queries will implicitly create the labels if they don't already exist.

```cypher
-- Create an Entity
CREATE (e:Entity {
    id: 'entity-uuid-123',
    created_at: '2023-10-27T10:00:00Z',
    metadata: {
        source_system: 'CRM'
    }
});

-- Create an Identifier and link it to the Entity
CREATE (i:Identifier {
    value: 'user@example.com',
    type: 'email'
});
MATCH (e:Entity {id: 'entity-uuid-123'}), (i:Identifier {value: 'user@example.com'})
CREATE (e)-[:HAS_IDENTIFIER {is_primary: true, created_at: '2023-10-27T10:00:00Z'}]->(i);

-- Create a Fact, a Source, and link them
CREATE (f:Fact {
    fact_id: 'Location:Paris',
    name: 'Paris',
    type: 'Location'
});
CREATE (s:Source {
    id: 'source-uuid-456',
    content: 'User mentioned they live in Paris.',
    timestamp: '2023-10-26T18:30:00Z'
});
MATCH (f:Fact {fact_id: 'Location:Paris'}), (s:Source {id: 'source-uuid-456'})
CREATE (f)-[:DERIVED_FROM]->(s);

-- Link the Fact to the Entity
MATCH (e:Entity {id: 'entity-uuid-123'}), (f:Fact {fact_id: 'Location:Paris'})
CREATE (e)-[:HAS_FACT {verb: 'lives in', confidence_score: 0.95, created_at: '2023-10-27T10:01:00Z'}]->(f);
```
</file>

<file path="apps/api/docs/graph_db_schema.md">
# Conceptual Graph Schema for a Knowledge Graph

This document outlines the conceptual data model for a flexible and scalable knowledge graph. It is designed to be implementation-agnostic and can be adapted to various graph database technologies. The schema's purpose is to capture facts about specific entities derived from textual sources.

## Core Components

The schema consists of four primary node (vertex) types and three primary relationship (edge) types.

### Node Types

#### 1. Entity

- **Purpose**: Represents the abstract, central subject of the graph (e.g., a person, an organization, a concept). It serves as the canonical anchor for all related information.
- **Key Properties**:
  - `id`: A unique, application-managed identifier (e.g., a UUID) that is stable and portable.
  - `created_at`: A timestamp indicating when the entity was created in the system.
  - `metadata`: A flexible container for additional, semi-structured properties related to the entity.

#### 2. Identifier

- **Purpose**: Represents an external, real-world identifier for an `Entity`.
- **Key Properties**:
  - `value`: The value of the identifier (e.g., "user@example.com", "+15551234567"). This should be unique across all identifiers.
  - `type`: The type of the identifier (e.g., "email", "phone_number").

#### 3. Fact

- **Purpose**: Represents a discrete piece of knowledge or a named entity (e.g., a location, a company, a hobby).
- **Key Properties**:
  - `fact_id`: A unique, application-generated identifier for the fact (e.g., a composite key of its type and name).
  - `name`: The name or value of the fact (e.g., "Paris", "Acme Corp").
  - `type`: The category of the fact (e.g., "Location", "Company").

#### 4. Source

- **Purpose**: Represents the origin of the information (e.g., a chat message, an email, a document).
- **Key Properties**:
  - `id`: A unique, application-managed identifier for the source.
  - `content`: The original content from which facts were derived.
  - `timestamp`: The real-world timestamp of when the source was created (e.g., when an email was sent).

### Relationship Types

#### 1. `HAS_IDENTIFIER`

- **Purpose**: A directed relationship connecting an `Entity` to its `Identifier`.
- **Direction**: `(Entity) -[HAS_IDENTIFIER]-> (Identifier)`
- **Key Properties**:
  - `is_primary`: A boolean flag to indicate if this is the primary identifier for the entity.
  - `created_at`: A timestamp for when the relationship was established.

#### 2. `HAS_FACT`

- **Purpose**: A directed relationship connecting an `Entity` to a `Fact` it possesses.
- **Direction**: `(Entity) -[HAS_FACT]-> (Fact)`
- **Key Properties**:
  - `verb`: Describes the relationship between the entity and the fact (e.g., "lives in", "works at").
  - `confidence_score`: A numerical value indicating the confidence in the validity of the fact.
  - `created_at`: A timestamp for when the relationship was established.

#### 3. `DERIVED_FROM`

- **Purpose**: A directed relationship linking a `Fact` back to the `Source` from which it was extracted.
- **Direction**: `(Fact) -[DERIVED_FROM]-> (Source)`
- **Rationale**: This relationship is the cornerstone of data traceability and provenance, answering the question: "How do we know this fact?"

## Schema Rationale and Design Principles

### Identity Management

The separation of the canonical `Entity` from its external `Identifier`(s) is a core design principle. This allows a single `Entity` to be associated with multiple identifiers, preventing duplicate profiles and providing a flexible foundation for identity resolution.

### Traceability

Every `Fact` should be traceable to a `Source`. The `DERIVED_FROM` relationship ensures that the provenance of all information in the graph is maintained.

### Timestamps

A clear distinction is made between two types of timestamps:

- **Event Time (`Source.timestamp`)**: The real-world time an event occurred.
- **System Time (`created_at`)**: The internal audit time when a record or relationship was created in our system.

This separation allows for accurate contextual queries alongside system-level auditing.
</file>

<file path="apps/api/docs/naming_convention.md">
## Python Naming Conventions Cheat Sheet

### General Principles

- **Readability is Key:** Python's philosophy emphasizes clear, readable code. Follow conventions to make your code easy for others (and your future self) to understand.
- **Be Consistent:** Once you choose a convention, stick with it throughout your project.
- **PEP 8:** The official style guide for Python code. This cheat sheet is based on PEP 8.

---

### Key Naming Styles

| Style                                | Format                                         | Example             | When to Use                                                                                                                                                                     |
| ------------------------------------ | ---------------------------------------------- | ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **`snake_case`**                     | All lowercase, words separated by underscores. | `my_variable`       | **Variables**, **functions**, **methods**                                                                                                                                       |
| **`PascalCase`**                     | Each word capitalized, no separators.          | `MyClass`           | **Classes**                                                                                                                                                                     |
| **`ALL_CAPS`**                       | All uppercase, words separated by underscores. | `MY_CONSTANT`       | **Constants** (values that don't change)                                                                                                                                        |
| **`_single_leading_underscore`**     | Single underscore prefix.                      | `_private_variable` | **Internal Use:** A convention to indicate a variable or method is intended for internal use within a class or module. Python doesn't enforce "private" access, this is a hint. |
| **`__double_leading_underscore`**    | Double underscore prefix.                      | `__name_mangling`   | **Name Mangling:** Used to avoid naming conflicts in subclasses. The interpreter renames the attribute to `_ClassName__attribute`.                                              |
| **`__double_trailing_underscore__`** | Double underscore suffix.                      | `__init__`          | **Special/Magic Methods:** Reserved for methods with specific behavior in Python. (e.g., `__init__`, `__str__`, `__len__`).                                                     |

---

### Naming Convention Details

#### 1. **Variables & Functions**

- Use `snake_case`.
- Avoid single-letter variable names unless it's a simple loop counter (`i`, `j`).
- Choose names that are descriptive and self-documenting.
- **Bad:** `x` (what is `x`?)
- **Good:** `user_age`, `calculate_total_price`

#### 2. **Classes**

- Use `PascalCase`.
- Class names are typically singular nouns.
- **Bad:** `user_manager`, `users`
- **Good:** `User`, `UserManager`

#### 3. **Modules & Packages**

- Use `snake_case`.
- Keep names short, all lowercase, and avoid underscores where possible.
- **Good:** `my_module.py`, `data_processing`

#### 4. **Constants**

- Use `ALL_CAPS` with underscores.
- These are variables intended to be treated as immutable.
- **Example:** `MAX_CONNECTIONS = 10`, `PI = 3.14159`

---

### Special Naming Rules

- **Private Members (convention):** `_my_internal_method()`

  - The single leading underscore is a convention to signal to other developers that this is a private implementation detail. You can still access it from outside the class, but you shouldn't.

- **Name Mangling:** `__my_private_method()`

  - Python renames this method internally to `_MyClass__my_private_method()`. This prevents subclasses from accidentally overriding it. It's generally less common than the single underscore for typical use cases.

- **Reserved Names:** `__init__`, `__str__`, `__add__`

  - These are built-in "dunder" (double underscore) or "magic" methods. Never invent your own names using this convention. They are reserved for special language features.

- **Variable `_`:**
  - A single underscore can be used as a placeholder for a variable you don't intend to use.
  - **Example:** `for _ in range(5):` (if you don't need the loop counter)
  - **Example:** `name, _ = full_name.split(' ')` (if you only need the first name)

---

### Summary Table

| What to Name              | Convention                           | Example                        |
| ------------------------- | ------------------------------------ | ------------------------------ |
| Variables                 | `snake_case`                         | `user_name`, `is_active`       |
| Functions & Methods       | `snake_case`                         | `get_data()`, `process_info()` |
| Classes                   | `PascalCase`                         | `User`, `DataProcessor`        |
| Modules                   | `snake_case` (lowercase, no hyphens) | `my_module.py`, `database.py`  |
| Constants                 | `ALL_CAPS`                           | `MAX_SIZE`, `DEFAULT_TIMEOUT`  |
| Internal Use (convention) | `_single_leading_underscore`         | `_internal_method`             |
| Special/Magic Methods     | `__double_trailing_underscore__`     | `__init__`, `__str__`          |
</file>

<file path="apps/api/docs/project_architecture.md">
# Modular Architecture

## Context

We need a scalable and maintainable structure for our growing FastAPI application. The architecture supports PostgreSQL AGE Graph Database and promotes clear separation of concerns.

## Decision

We have implemented a modular, feature-based architecture with **inline tests** and **entity-specific separation**. The project is organized by business domains (e.g., `users`, `graph`), with each feature containing its own routes, logic, models, repositories, and tests. We follow the **Clean Architecture** principles with clear separation between:

- **Presentation Layer** (Routes/APIs)
- **Application Layer** (Use Cases/Business Logic)
- **Domain Layer** (Models/Entities)
- **Infrastructure Layer** (Repositories/Databases)

## Development Environment

Dependency management will be handled by `uv`, with dependencies specified in `pyproject.toml`.

Tests will be run by pointing `pytest` to the application source directory: `uv run pytest app/`.

## Architecture Overview

```plaintext
/nous-api/
|
|-- /app/
|   |-- main.py                    # FastAPI app factory & router inclusion
|   |-- /core/
|   |   |-- security.py
|   |   `-- test_security.py       # Test co-located with code
|   |
|   |-- /db/
|   |   `-- /postgres/             # PostgreSQL AGE connection logic
|   `-- /features/
|       |-- /users/                # User management feature
|       |   |-- router.py
|       |   |-- test_router.py     # Test co-located with code
|       |   |-- service.py
|       |   |-- models.py
|       |   `-- schemas.py
|       |
|       `-- /graph/                # Graph database feature (IMPLEMENTED)
|           |-- router.py          # Main router including all entity routes
|           |-- /models/           # Domain models (Entity, Fact, etc.)
|           |-- /repositories/     # Entity-specific repositories
|           |   |-- entity.py      # EntityRepository
|           |   |-- fact.py        # FactRepository
|           |   |-- identifier.py  # IdentifierRepository
|           |   `-- source.py      # SourceRepository
|           |-- /routes/           # Entity-specific route modules
|           |   |-- entities.py    # Entity CRUD routes
|           |   |-- facts.py       # Fact retrieval routes
|           |   |-- entity_facts.py # Entity-Fact relationship routes
|           |   `-- __init__.py    # Route exports
|           `-- /usecases/         # Business logic use cases
|               |-- create_entity.py
|               |-- get_entity.py
|               |-- add_fact.py
|               `-- ...
|
`-- pyproject.toml
```

### Data Layer Architecture with SQLModel

This project uses **SQLModel**, which unifies database models and API schemas into single class definitions. SQLModel classes serve dual purposes:

1. **Database Tables**: When defined with `table=True`, they create SQLAlchemy table models
2. **API Schemas**: The same classes automatically work as Pydantic models for FastAPI serialization

This approach reduces code duplication and ensures consistency between database structure and API contracts.

**File Usage Patterns:**

- `models.py`: Contains SQLModel classes that serve as both database tables and API schemas
- `schemas.py`: Contains pure Pydantic models for cases where you need API-only schemas (e.g., authentication tokens, complex validation models)

### Component Responsibilities

| Component         | Responsibility                                                                     |
| :---------------- | :--------------------------------------------------------------------------------- |
| `main.py`         | Creates and configures the main `FastAPI` instance with router inclusion.          |
| `core/`           | App-wide concerns (settings, security).                                            |
| `db/`             | Contains isolated connection logic for each database.                              |
| `router.py`       | **Main Router:** Orchestrates entity-specific route modules.                       |
| `routes/`         | **API Layer:** Entity-specific route modules handling HTTP requests/responses.     |
| `usecases/`       | **Application Layer:** Business logic use cases with validation and rules.         |
| `repositories/`   | **Infrastructure Layer:** Entity-specific database operations and queries.         |
| `models/`         | **Domain Layer:** Core business entities, relationships, and domain models.        |
| `schemas.py`      | **Data Layer:** Pure Pydantic models for API I/O (when separate from DB models).   |
| `models.py`       | **Persistence Layer:** SQLModel classes serving as both DB tables and API schemas. |
| `graph_models.py` | **Persistence Layer:** Graph DB node/edge models.                                  |
| `test_*.py`       | **Testing Layer:** Verifies the correctness of each module.                        |

### API Structure

```
FastAPI Application
‚îú‚îÄ‚îÄ /api/v1/graph/
‚îÇ   ‚îú‚îÄ‚îÄ POST /entities           # Create entity with identifier
‚îÇ   ‚îú‚îÄ‚îÄ GET /entities/{id}       # Get entity by ID
‚îÇ   ‚îú‚îÄ‚îÄ GET /entities            # Search entities by identifier
‚îÇ   ‚îú‚îÄ‚îÄ GET /facts/{id}          # Get fact by ID
‚îÇ   ‚îî‚îÄ‚îÄ POST /entities/{id}/facts # Add fact to entity
‚îî‚îÄ‚îÄ /health                      # Health check endpoint
```
</file>

<file path="apps/api/docs/project_scope.md">
# Project Scope: AI Agent Memory System

This document outlines the scope for building a memory system for AI Agents using a knowledge graph.

## Core Concepts

The system is built around four fundamental concepts:

1.  **Entity**: Represents a canonical person, place, or concept. Each `Entity` has a stable, unique identifier.
2.  **Identifier**: An external identifier linked to an `Entity`, such as an email address, phone number, or username. It's used to look up and identify entities.
3.  **Fact**: A discrete piece of information or an attribute related to an `Entity` (e.g., "lives in New York", "works at Acme Corp").
4.  **Source**: The origin of a `Fact`, providing traceability. This could be a chat message, an email, or a document.

## System Workflow

The basic workflow of the memory system is as follows:

1.  An `Entity` is created or identified using an `Identifier`.
2.  Information is processed from a `Source` to extract `Facts`.
3.  These `Facts` are then associated with the corresponding `Entity`.

This structure allows the system to build a rich, interconnected memory graph for an AI agent, with clear provenance for every piece of information.
</file>

<file path="apps/api/migrations/versions/17010deff34c_fresh_migration_from_models.py">
"""Fresh migration from models

Revision ID: 17010deff34c
Revises: 
Create Date: 2025-10-23 19:35:13.377303

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '17010deff34c'
down_revision: Union[str, Sequence[str], None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('tenants',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('name', sa.String(length=255), nullable=False),
    sa.Column('age_graph_name', sa.String(length=100), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('age_graph_name'),
    sa.UniqueConstraint('name')
    )
    op.create_table('api_keys',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('name', sa.String(length=100), nullable=False),
    sa.Column('key_prefix', sa.String(length=10), nullable=False),
    sa.Column('hashed_key', sa.String(length=255), nullable=False),
    sa.Column('tenant_id', sa.UUID(), nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('expires_at', sa.DateTime(timezone=True), nullable=True),
    sa.Column('last_used_at', sa.DateTime(timezone=True), nullable=True),
    sa.ForeignKeyConstraint(['tenant_id'], ['tenants.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('key_prefix'),
    sa.UniqueConstraint('name', 'tenant_id', name='unique_api_key_name_per_tenant')
    )
    op.create_table('users',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('email', sa.String(length=255), nullable=False),
    sa.Column('hashed_password', sa.String(length=255), nullable=False),
    sa.Column('tenant_id', sa.UUID(), nullable=True),
    sa.Column('role', sa.Enum('SUPER_ADMIN', 'TENANT_ADMIN', 'TENANT_USER', name='userrole'), nullable=False),
    sa.Column('is_active', sa.Boolean(), nullable=False),
    sa.Column('failed_login_attempts', sa.Integer(), nullable=False),
    sa.Column('locked_until', sa.DateTime(timezone=True), nullable=True),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.ForeignKeyConstraint(['tenant_id'], ['tenants.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('email')
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('users')
    op.drop_table('api_keys')
    op.drop_table('tenants')
    # ### end Alembic commands ###
</file>

<file path="apps/api/migrations/README.md">
# Managing Database Migrations with Alembic

This project uses Alembic to manage database schema migrations. All migrations are located in the `migrations/versions` directory.

## How it Works

Alembic works by comparing the state of your SQLAlchemy models (e.g., in `app/features/auth/models.py`) against the current state of the database schema. When it detects differences, it can automatically generate a migration script to apply those changes.

## Key Files and Directories

- `env.py`: This is the main runtime configuration script for Alembic. It's where we programmatically define how to connect to our database and where Alembic should look for our SQLAlchemy models (`target_metadata`) to detect changes for autogeneration.

- `script.py.mako`: This is a Mako template file that Alembic uses to generate new migration files. You can edit this file to change the structure of the generated revision scripts.

- `versions/`: This directory contains all the migration scripts. Each file in this directory represents a sequential change to the database schema. These scripts contain two main functions: `upgrade()` to apply the changes and `downgrade()` to revert them.

## Common Commands

All commands should be run from the `apps/api` directory using `uv run`.

### Generating a New Migration

Whenever you make changes to your SQLAlchemy models (e.g., add a new table or a new column), you need to generate a migration script.

```bash
uv run alembic revision --autogenerate -m "A descriptive name for your migration"
```

This will create a new file in `migrations/versions/` containing the Python code to apply (`upgrade`) and revert (`downgrade`) your changes. **Always review the generated migration script** to ensure it's correct before applying it.

### Applying Migrations

To apply all pending migrations to your database, use the `upgrade` command. `head` refers to the latest migration.

```bash
uv run alembic upgrade head
```

You can also upgrade to a specific migration by providing its revision ID.

### Downgrading Migrations

To revert the last applied migration, you can use `downgrade`.

```bash
# Downgrade by one revision
uv run alembic downgrade -1
```

To revert all migrations, you can downgrade to `base`.

```bash
uv run alembic downgrade base
```

### Checking Migration Status

To see the current revision of the database and identify which migrations have not been applied, you can use these commands:

```bash
# Show the current revision
uv run alembic current

# Show the history of migrations
uv run alembic history
```

## Starting Migrations from Scratch

If you need to completely reset your migrations and start fresh (e.g., after major model changes or during development), follow these steps:

### Option 1: Complete Reset (Recommended for Development)

```bash
# 1. Remove all existing migration files
rm -f migrations/versions/*.py

# 2. Clean the database (drop existing tables and types)
# Connect to your database and run:
# DROP TABLE IF EXISTS users, api_keys, tenants, alembic_version CASCADE;
# DROP TYPE IF EXISTS userrole CASCADE;

# 3. Generate fresh migration from current models
uv run alembic revision --autogenerate -m "Fresh migration from models"

# 4. Apply the migration
uv run alembic upgrade head
```

### Option 2: Reset with Docker (If using Docker Compose)

If you're using the provided Docker setup for PostgreSQL:

```bash
# 1. Stop the database container
docker compose -f compose/postgres/docker-compose.yml down

# 2. Remove the database volume (WARNING: This deletes all data!)
docker volume rm nous_postgres-data

# 3. Remove migration files
rm -f migrations/versions/*.py

# 4. Restart the database
docker compose -f compose/postgres/docker-compose.yml up -d

# 5. Generate and apply fresh migrations
uv run alembic revision --autogenerate -m "Fresh migration from models"
uv run alembic upgrade head
```

### ‚ö†Ô∏è Important Notes

- **Data Loss**: Both options will permanently delete all data in your database
- **Review Generated Migrations**: Always inspect the generated migration files before applying them
- **Backup First**: If you have important data, back it up before resetting
- **Team Coordination**: If working in a team, coordinate resets to avoid conflicts
</file>

<file path="apps/api/migrations/script.py.mako">
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, Sequence[str], None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    """Upgrade schema."""
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    """Downgrade schema."""
    ${downgrades if downgrades else "pass"}
</file>

<file path="apps/api/tests/features/auth/usecases/test_create_api_key_usecase_integration.py">
"""Integration tests for the CreateApiKeyUseCase."""

import pytest
from fastapi import HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select

from app.core.schemas import UserRole
from app.features.auth.dtos import CreateApiKeyRequest
from app.features.auth.models import ApiKey, Tenant, User
from app.features.auth.usecases.create_api_key_usecase import CreateApiKeyUseCaseImpl
from app.features.auth.usecases.signup_tenant_usecase import PasswordHasher

# All fixtures are now provided by tests/conftest.py


class TestCreateApiKeyUseCase:
    """Test suite for the CreateApiKeyUseCase."""

    async def test_create_api_key_successfully(
        self,
        db_session: AsyncSession,
        password_hasher: PasswordHasher,
    ):
        """Test the successful creation of an API key for a tenant."""

        # Arrange - Create a tenant and user first
        async with db_session.begin():
            tenant = Tenant(name="test-tenant", age_graph_name="test_graph_123")
            db_session.add(tenant)
            await db_session.flush()

            user_model = User(
                email="user@example.com",
                hashed_password=password_hasher.hash("userpass"),
                tenant_id=tenant.id,
                role=UserRole.TENANT_USER,
            )
            db_session.add(user_model)
            await db_session.flush()

        use_case = CreateApiKeyUseCaseImpl(get_db_session=lambda: db_session)

        request = CreateApiKeyRequest(name="test-api-key")

        # Act
        response = await use_case.execute(request, tenant.id)

        # Assert
        assert response.message == "API key created successfully"
        assert response.api_key is not None
        assert response.key_prefix is not None
        assert response.expires_at is not None

        # Verify the API key has the expected format (prefix.key)
        assert "." in response.api_key
        assert response.api_key.startswith(response.key_prefix + ".")

        # Verify database state
        api_key = (
            await db_session.execute(
                select(ApiKey).filter_by(key_prefix=response.key_prefix)
            )
        ).scalar_one_or_none()
        assert api_key is not None
        assert api_key.name == "test-api-key"
        assert api_key.tenant_id == tenant.id
        assert api_key.expires_at is not None
        # Verify hashed key is stored (not the plaintext)
        assert api_key.hashed_key != response.api_key

    async def test_create_api_key_duplicate_name(
        self,
        db_session: AsyncSession,
        password_hasher: PasswordHasher,
    ):
        """Test that creating an API key with a duplicate name for the same tenant fails."""

        # Arrange - Create a tenant and user first
        async with db_session.begin():
            tenant = Tenant(name="test-tenant", age_graph_name="test_graph_456")
            db_session.add(tenant)
            await db_session.flush()

            user_model = User(
                email="user@example.com",
                hashed_password=password_hasher.hash("userpass"),
                tenant_id=tenant.id,
                role=UserRole.TENANT_USER,
            )
            db_session.add(user_model)
            await db_session.flush()

        use_case = CreateApiKeyUseCaseImpl(get_db_session=lambda: db_session)

        request = CreateApiKeyRequest(name="duplicate-key")

        # Create first API key
        await use_case.execute(request, tenant.id)

        # Act & Assert - Try to create another with the same name
        with pytest.raises(HTTPException) as excinfo:
            await use_case.execute(request, tenant.id)
        assert excinfo.value.status_code == 400
        assert "API key name already exists" in excinfo.value.detail

    async def test_create_api_key_same_name_different_tenant(
        self,
        db_session: AsyncSession,
        password_hasher: PasswordHasher,
    ):
        """Test that creating an API key with the same name for different tenants succeeds."""

        # Arrange - Create two tenants and users
        async with db_session.begin():
            tenant1 = Tenant(name="tenant1", age_graph_name="test_graph_789")
            tenant2 = Tenant(name="tenant2", age_graph_name="test_graph_101")
            db_session.add_all([tenant1, tenant2])
            await db_session.flush()

            user1_model = User(
                email="user1@example.com",
                hashed_password=password_hasher.hash("userpass"),
                tenant_id=tenant1.id,
                role=UserRole.TENANT_USER,
            )
            user2_model = User(
                email="user2@example.com",
                hashed_password=password_hasher.hash("userpass"),
                tenant_id=tenant2.id,
                role=UserRole.TENANT_USER,
            )
            db_session.add_all([user1_model, user2_model])
            await db_session.flush()

        use_case = CreateApiKeyUseCaseImpl(get_db_session=lambda: db_session)

        request = CreateApiKeyRequest(name="same-name-key")

        # Create API key for tenant1
        response1 = await use_case.execute(request, tenant1.id)

        # Create API key with same name for tenant2 - should succeed
        response2 = await use_case.execute(request, tenant2.id)

        # Assert both succeeded
        assert response1.message == "API key created successfully"
        assert response2.message == "API key created successfully"
        assert response1.key_prefix != response2.key_prefix  # Different prefixes

    @pytest.mark.parametrize(
        "name, expected_error",
        [
            ("ab", "API key name must be between 3 and 50 characters"),
            ("", "API key name must be between 3 and 50 characters"),
            ("a" * 51, "API key name must be between 3 and 50 characters"),
        ],
    )
    async def test_create_api_key_invalid_name(
        self,
        db_session: AsyncSession,
        password_hasher: PasswordHasher,
        name: str,
        expected_error: str,
    ):
        """Test that creating an API key with invalid name lengths fails."""

        # Arrange - Create a tenant and user first
        async with db_session.begin():
            tenant = Tenant(name="test-tenant", age_graph_name="test_graph_202")
            db_session.add(tenant)
            await db_session.flush()

            user_model = User(
                email="user@example.com",
                hashed_password=password_hasher.hash("userpass"),
                tenant_id=tenant.id,
                role=UserRole.TENANT_USER,
            )
            db_session.add(user_model)
            await db_session.flush()

        use_case = CreateApiKeyUseCaseImpl(get_db_session=lambda: db_session)

        request = CreateApiKeyRequest(name=name)

        # Act & Assert
        with pytest.raises(HTTPException) as excinfo:
            await use_case.execute(request, tenant.id)
        assert excinfo.value.status_code == 400
        assert expected_error in excinfo.value.detail
</file>

<file path="apps/api/tests/features/auth/usecases/test_create_user_usecase_integration.py">
"""Integration tests for the CreateUserUseCase."""

from uuid import uuid4

import pytest
from fastapi import HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select

from app.core.schemas import AuthenticatedUser, UserRole
from app.features.auth.dtos import CreateUserRequest
from app.features.auth.models import Tenant, User
from app.features.auth.usecases.create_user_usecase import (
    CreateUserUseCaseImpl,
    PasswordHasher,
)

# All fixtures are now provided by tests/conftest.py


class TestCreateUserUseCase:
    """Test suite for the CreateUserUseCase."""

    async def test_create_user_successfully(
        self,
        db_session: AsyncSession,
        password_hasher: PasswordHasher,
    ):
        """Test the successful creation of a user within a tenant."""

        # Arrange - Create a tenant and admin user first
        async with db_session.begin():
            tenant = Tenant(name="test-tenant", age_graph_name="test_graph_123")
            db_session.add(tenant)
            await db_session.flush()

            admin_user_model = User(
                email="admin@example.com",
                hashed_password=password_hasher.hash("adminpass"),
                tenant_id=tenant.id,
                role=UserRole.TENANT_ADMIN,
            )
            db_session.add(admin_user_model)
            await db_session.flush()

        tenant_admin_user = AuthenticatedUser(
            user_id=admin_user_model.id,
            tenant_id=tenant.id,
            role=UserRole.TENANT_ADMIN,
        )

        use_case = CreateUserUseCaseImpl(
            password_hasher=password_hasher,
            get_db_session=lambda: db_session,  # type: ignore
        )

        request = CreateUserRequest(
            email="newuser@example.com",
            password="testpassword",
            role=UserRole.TENANT_USER,
        )

        # Act
        response = await use_case.execute(request, tenant_admin_user)

        # Assert
        assert response.message == "User created successfully"
        assert response.user_id is not None
        assert response.email == "newuser@example.com"
        assert response.role == UserRole.TENANT_USER

        # Verify database state
        user = (
            await db_session.execute(select(User).filter_by(id=response.user_id))
        ).scalar_one_or_none()
        assert user is not None
        assert user.tenant_id == tenant_admin_user.tenant_id

    async def test_create_user_duplicate_email(
        self,
        db_session: AsyncSession,
        password_hasher: PasswordHasher,
    ):
        """Test that creating a user with a duplicate email fails."""

        # Arrange - Create a tenant and admin user first
        async with db_session.begin():
            tenant = Tenant(name="test-tenant", age_graph_name="test_graph_456")
            db_session.add(tenant)
            await db_session.flush()

            admin_user_model = User(
                email="admin@example.com",
                hashed_password=password_hasher.hash("adminpass"),
                tenant_id=tenant.id,
                role=UserRole.TENANT_ADMIN,
            )
            db_session.add(admin_user_model)
            await db_session.flush()

        tenant_admin_user = AuthenticatedUser(
            user_id=admin_user_model.id,
            tenant_id=tenant.id,
            role=UserRole.TENANT_ADMIN,
        )

        use_case = CreateUserUseCaseImpl(
            password_hasher=password_hasher,
            get_db_session=lambda: db_session,  # type: ignore
        )

        request = CreateUserRequest(
            email="duplicate@example.com",
            password="testpassword",
            role=UserRole.TENANT_USER,
        )

        # Create first user
        await use_case.execute(request, tenant_admin_user)

        # Act & Assert

        with pytest.raises(HTTPException) as excinfo:
            await use_case.execute(request, tenant_admin_user)
        assert excinfo.value.status_code == 400
        assert "already exists" in excinfo.value.detail.lower()

    async def test_create_user_admin_without_tenant(
        self,
        db_session: AsyncSession,
        password_hasher: PasswordHasher,
    ):
        """Test that creating a user fails when admin has no tenant."""
        # Arrange
        use_case = CreateUserUseCaseImpl(
            password_hasher=password_hasher,
            get_db_session=lambda: db_session,  # type: ignore
        )

        request = CreateUserRequest(
            email="test@example.com",
            password="testpassword",
            role=UserRole.TENANT_USER,
        )

        # Create an admin user without a tenant
        admin_without_tenant = AuthenticatedUser(
            user_id=uuid4(),  # Use a proper UUID
            tenant_id=None,  # No tenant
            role=UserRole.TENANT_ADMIN,
        )

        # Act & Assert

        with pytest.raises(HTTPException) as excinfo:
            await use_case.execute(request, admin_without_tenant)
        assert excinfo.value.status_code == 400
        assert "admin has no tenant" in excinfo.value.detail.lower()

    @pytest.mark.parametrize(
        "role, expected_error",
        [
            (UserRole.TENANT_ADMIN, "Cannot create another admin"),
            (UserRole.SUPER_ADMIN, "Cannot create a super admin"),
        ],
    )
    async def test_create_user_invalid_roles(
        self,
        db_session: AsyncSession,
        password_hasher: PasswordHasher,
        role: UserRole,
        expected_error: str,
    ):
        """Test that creating users with invalid roles fails."""

        # Arrange - Create a tenant and admin user first
        async with db_session.begin():
            tenant = Tenant(name="test-tenant", age_graph_name="test_graph_789")
            db_session.add(tenant)
            await db_session.flush()

            admin_user_model = User(
                email="admin@example.com",
                hashed_password=password_hasher.hash("adminpass"),
                tenant_id=tenant.id,
                role=UserRole.TENANT_ADMIN,
            )
            db_session.add(admin_user_model)
            await db_session.flush()

        tenant_admin_user = AuthenticatedUser(
            user_id=admin_user_model.id,
            tenant_id=tenant.id,
            role=UserRole.TENANT_ADMIN,
        )

        use_case = CreateUserUseCaseImpl(
            password_hasher=password_hasher,
            get_db_session=lambda: db_session,  # type: ignore
        )

        request = CreateUserRequest(
            email="test@example.com",
            password="testpassword",
            role=role,
        )

        # Act & Assert

        with pytest.raises(HTTPException) as excinfo:
            await use_case.execute(request, tenant_admin_user)
        assert excinfo.value.status_code == 403
        assert expected_error in excinfo.value.detail
</file>

<file path="apps/api/tests/features/auth/usecases/test_list_api_keys_usecase_integration.py">
"""Integration tests for the ListApiKeysUseCase."""

from sqlalchemy.ext.asyncio import AsyncSession

from app.core.schemas import UserRole
from app.features.auth.dtos import CreateApiKeyRequest
from app.features.auth.models import Tenant, User
from app.features.auth.usecases.create_api_key_usecase import CreateApiKeyUseCaseImpl
from app.features.auth.usecases.list_api_keys_usecase import ListApiKeysUseCaseImpl
from app.features.auth.usecases.signup_tenant_usecase import PasswordHasher

# All fixtures are now provided by tests/conftest.py


class TestListApiKeysUseCase:
    """Test suite for the ListApiKeysUseCase."""

    async def test_list_api_keys_empty(
        self,
        db_session: AsyncSession,
        password_hasher: PasswordHasher,
    ):
        """Test listing API keys for a tenant with no keys returns empty list."""

        # Arrange - Create a tenant and user first (no API keys)
        async with db_session.begin():
            tenant = Tenant(name="test-tenant", age_graph_name="test_graph_123")
            db_session.add(tenant)
            await db_session.flush()

            user_model = User(
                email="user@example.com",
                hashed_password=password_hasher.hash("userpass"),
                tenant_id=tenant.id,
                role=UserRole.TENANT_USER,
            )
            db_session.add(user_model)
            await db_session.flush()

        use_case = ListApiKeysUseCaseImpl(get_db_session=lambda: db_session)

        # Act
        response = await use_case.execute(tenant.id)

        # Assert
        assert response.api_keys == []

    async def test_list_api_keys_populated(
        self,
        db_session: AsyncSession,
        password_hasher: PasswordHasher,
    ):
        """Test listing API keys for a tenant with multiple keys."""

        # Arrange - Create a tenant and user first
        async with db_session.begin():
            tenant = Tenant(name="test-tenant", age_graph_name="test_graph_456")
            db_session.add(tenant)
            await db_session.flush()

            user_model = User(
                email="user@example.com",
                hashed_password=password_hasher.hash("userpass"),
                tenant_id=tenant.id,
                role=UserRole.TENANT_USER,
            )
            db_session.add(user_model)
            await db_session.flush()

        # Create multiple API keys using the create use case
        create_use_case = CreateApiKeyUseCaseImpl(get_db_session=lambda: db_session)

        api_key_names = ["key1", "key2", "key3"]
        created_keys = []

        for name in api_key_names:
            request = CreateApiKeyRequest(name=name)
            response = await create_use_case.execute(request, tenant.id)
            created_keys.append(response)

        list_use_case = ListApiKeysUseCaseImpl(get_db_session=lambda: db_session)

        # Act
        response = await list_use_case.execute(tenant.id)

        # Assert
        assert len(response.api_keys) == 3

        # Check that all created keys are in the response
        response_names = {key.name for key in response.api_keys}
        assert response_names == set(api_key_names)

        response_prefixes = {key.key_prefix for key in response.api_keys}
        created_prefixes = {key.key_prefix for key in created_keys}
        assert response_prefixes == created_prefixes

        # Check that all keys have required fields
        for api_key_info in response.api_keys:
            assert api_key_info.id is not None
            assert api_key_info.name is not None
            assert api_key_info.key_prefix is not None
            assert api_key_info.created_at is not None
            # expires_at and last_used_at can be None

    async def test_list_api_keys_tenant_isolation(
        self,
        db_session: AsyncSession,
        password_hasher: PasswordHasher,
    ):
        """Test that tenants only see their own API keys."""

        # Arrange - Create two tenants and users
        async with db_session.begin():
            tenant1 = Tenant(name="tenant1", age_graph_name="test_graph_789")
            tenant2 = Tenant(name="tenant2", age_graph_name="test_graph_101")
            db_session.add_all([tenant1, tenant2])
            await db_session.flush()

            user1_model = User(
                email="user1@example.com",
                hashed_password=password_hasher.hash("userpass"),
                tenant_id=tenant1.id,
                role=UserRole.TENANT_USER,
            )
            user2_model = User(
                email="user2@example.com",
                hashed_password=password_hasher.hash("userpass"),
                tenant_id=tenant2.id,
                role=UserRole.TENANT_USER,
            )
            db_session.add_all([user1_model, user2_model])
            await db_session.flush()

        # Create API keys for both tenants
        create_use_case = CreateApiKeyUseCaseImpl(get_db_session=lambda: db_session)

        # Create keys for tenant1
        request1 = CreateApiKeyRequest(name="tenant1-key")
        await create_use_case.execute(request1, tenant1.id)

        # Create keys for tenant2
        request2a = CreateApiKeyRequest(name="tenant2-key-a")
        request2b = CreateApiKeyRequest(name="tenant2-key-b")
        await create_use_case.execute(request2a, tenant2.id)
        await create_use_case.execute(request2b, tenant2.id)

        list_use_case = ListApiKeysUseCaseImpl(get_db_session=lambda: db_session)

        # Act - List keys for tenant1
        response1 = await list_use_case.execute(tenant1.id)

        # List keys for tenant2
        response2 = await list_use_case.execute(tenant2.id)

        # Assert - tenant1 should only see their 1 key
        assert len(response1.api_keys) == 1
        assert response1.api_keys[0].name == "tenant1-key"

        # tenant2 should only see their 2 keys
        assert len(response2.api_keys) == 2
        response2_names = {key.name for key in response2.api_keys}
        assert response2_names == {"tenant2-key-a", "tenant2-key-b"}
</file>

<file path="apps/api/tests/features/auth/usecases/test_login_usecase_integration.py">
"""Integration tests for the LoginUseCase."""

from contextlib import asynccontextmanager

import pytest
from fastapi import HTTPException
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.authentication import pwd_context
from app.features.auth.dtos.auth_dto import LoginRequest, LoginResponse
from app.features.auth.models import Tenant, User
from app.features.auth.usecases.login_usecase import LoginUseCaseImpl


class PasswordVerifierImpl:
    """Wrapper for password verification to match protocol."""

    def verify(self, plain_password: str, hashed_password: str) -> bool:
        """Verify a password against its hash."""
        return pwd_context.verify(plain_password, hashed_password)


class TokenCreatorImpl:
    """Wrapper for token creation to match protocol."""

    def __init__(self):
        self.created_tokens = []

    def __call__(self, data: dict[str, str | int], expires_delta=None) -> str:
        """Create an access token."""
        # Store the data for verification in tests
        self.created_tokens.append(data)
        # Return a mock token for testing
        return f"mock_token_{len(self.created_tokens)}"


def create_session_factory(session):
    """Create a session factory that returns the same session."""

    @asynccontextmanager
    async def get_session():
        yield session

    return get_session


@pytest.mark.asyncio
class TestLoginUseCase:
    """Test suite for the LoginUseCase."""

    async def test_login_successful(
        self,
        db_session: AsyncSession,
        password_hasher,
    ):
        """Test successful user login."""
        # Arrange - Create a tenant and user
        tenant = Tenant(name="test-tenant", age_graph_name="test_tenant_graph")
        db_session.add(tenant)
        await db_session.flush()

        hashed_password = password_hasher.hash("correctpassword")
        user = User(
            email="test@example.com",
            hashed_password=hashed_password,
            tenant_id=tenant.id,
            is_active=True,
            failed_login_attempts=0,
            locked_until=None,
        )
        db_session.add(user)
        await db_session.commit()

        # Create use case
        token_creator = TokenCreatorImpl()
        use_case = LoginUseCaseImpl(
            password_verifier=PasswordVerifierImpl(),
            token_creator=token_creator,
            get_db_session=create_session_factory(db_session),
        )

        request = LoginRequest(
            email="test@example.com",
            password="correctpassword",
        )

        # Act
        response = await use_case.execute(request.email, request.password)

        # Assert
        assert isinstance(response, LoginResponse)
        assert response.access_token.startswith("mock_token_")
        assert response.token_type == "bearer"
        assert response.expires_in == 1800  # 30 minutes in seconds

        # Verify token data
        assert len(token_creator.created_tokens) == 1
        token_data = token_creator.created_tokens[0]
        assert token_data["sub"] == str(user.id)
        assert token_data["tenant_id"] == str(tenant.id)
        assert token_data["role"] == user.role.value

        # Verify user state was updated
        await db_session.refresh(user)
        assert user.failed_login_attempts == 0
        assert user.locked_until is None

    async def test_login_user_not_found(
        self,
        db_session,
    ):
        """Test login with non-existent email."""
        # Arrange
        token_creator = TokenCreatorImpl()
        use_case = LoginUseCaseImpl(
            password_verifier=PasswordVerifierImpl(),
            token_creator=token_creator,
            get_db_session=create_session_factory(db_session),
        )

        # Act & Assert
        with pytest.raises(HTTPException) as excinfo:
            await use_case.execute("nonexistent@example.com", "password")

        assert excinfo.value.status_code == 401
        assert "Incorrect email or password" in excinfo.value.detail

    async def test_login_wrong_password(
        self,
        db_session,
        password_hasher,
    ):
        """Test login with wrong password."""
        # Arrange - Create a tenant and user
        tenant = Tenant(name="test-tenant", age_graph_name="test_tenant_graph")
        db_session.add(tenant)
        await db_session.flush()

        hashed_password = password_hasher.hash("correctpassword")
        user = User(
            email="test@example.com",
            hashed_password=hashed_password,
            tenant_id=tenant.id,
            is_active=True,
            failed_login_attempts=0,
            locked_until=None,
        )
        db_session.add(user)
        await db_session.commit()

        # Create use case
        token_creator = TokenCreatorImpl()
        use_case = LoginUseCaseImpl(
            password_verifier=PasswordVerifierImpl(),
            token_creator=token_creator,
            get_db_session=create_session_factory(db_session),
        )

        # Act & Assert
        with pytest.raises(HTTPException) as excinfo:
            await use_case.execute("test@example.com", "wrongpassword")

        assert excinfo.value.status_code == 401
        assert "Incorrect email or password" in excinfo.value.detail

        # Verify no token was created
        assert len(token_creator.created_tokens) == 0

    async def test_login_account_locked(
        self,
        db_session,
        password_hasher,
    ):
        """Test login when account is temporarily locked."""
        # Arrange - Create a tenant and user with locked account
        from datetime import UTC, datetime, timedelta

        tenant = Tenant(name="test-tenant", age_graph_name="test_tenant_graph")
        db_session.add(tenant)
        await db_session.flush()

        hashed_password = password_hasher.hash("correctpassword")
        locked_until = datetime.now(UTC) + timedelta(minutes=30)
        user = User(
            email="test@example.com",
            hashed_password=hashed_password,
            tenant_id=tenant.id,
            is_active=True,
            failed_login_attempts=3,
            locked_until=locked_until,
        )
        db_session.add(user)
        await db_session.commit()

        # Create use case
        token_creator = TokenCreatorImpl()
        use_case = LoginUseCaseImpl(
            password_verifier=PasswordVerifierImpl(),
            token_creator=token_creator,
            get_db_session=create_session_factory(db_session),
        )

        # Act & Assert
        with pytest.raises(HTTPException) as excinfo:
            await use_case.execute("test@example.com", "correctpassword")

        assert excinfo.value.status_code == 401
        assert "Account is temporarily locked" in excinfo.value.detail

        # Verify no token was created
        assert len(token_creator.created_tokens) == 0

    async def test_login_account_inactive(
        self,
        db_session,
        password_hasher,
    ):
        """Test login when account is disabled."""
        # Arrange - Create a tenant and inactive user
        tenant = Tenant(name="test-tenant", age_graph_name="test_tenant_graph")
        db_session.add(tenant)
        await db_session.flush()

        hashed_password = password_hasher.hash("correctpassword")
        user = User(
            email="test@example.com",
            hashed_password=hashed_password,
            tenant_id=tenant.id,
            is_active=False,  # Account is inactive
            failed_login_attempts=0,
            locked_until=None,
        )
        db_session.add(user)
        await db_session.commit()

        # Create use case
        token_creator = TokenCreatorImpl()
        use_case = LoginUseCaseImpl(
            password_verifier=PasswordVerifierImpl(),
            token_creator=token_creator,
            get_db_session=create_session_factory(db_session),
        )

        # Act & Assert
        with pytest.raises(HTTPException) as excinfo:
            await use_case.execute("test@example.com", "correctpassword")

        assert excinfo.value.status_code == 401
        assert "Account is disabled" in excinfo.value.detail

        # Verify no token was created
        assert len(token_creator.created_tokens) == 0

    async def test_login_resets_failed_attempts_on_success(
        self,
        db_session,
        password_hasher,
    ):
        """Test that successful login resets failed login attempts counter."""
        # Arrange - Create a tenant and user with some failed attempts
        tenant = Tenant(name="test-tenant", age_graph_name="test_tenant_graph")
        db_session.add(tenant)
        await db_session.flush()

        hashed_password = password_hasher.hash("correctpassword")
        user = User(
            email="test@example.com",
            hashed_password=hashed_password,
            tenant_id=tenant.id,
            is_active=True,
            failed_login_attempts=2,  # Had some failed attempts
            locked_until=None,
        )
        db_session.add(user)
        await db_session.commit()

        # Create use case
        token_creator = TokenCreatorImpl()
        use_case = LoginUseCaseImpl(
            password_verifier=PasswordVerifierImpl(),
            token_creator=token_creator,
            get_db_session=create_session_factory(db_session),
        )

        # Act
        await use_case.execute("test@example.com", "correctpassword")

        # Assert - Verify failed attempts were reset
        await db_session.refresh(user)
        assert user.failed_login_attempts == 0
        assert user.locked_until is None
</file>

<file path="apps/api/tests/features/graph/services/test_langchain_fact_extractor.py">
"""Integration tests for the LangChainFactExtractor service.

These tests actually call the Gemini LLM API, so they require:
- GOOGLE_API_KEY environment variable to be set
- Internet connection for API calls
"""

from unittest.mock import patch

import pytest

from app.features.graph.dtos.knowledge_dto import ExtractedFactDto, IdentifierDto
from app.features.graph.services.langchain_fact_extractor import LangChainFactExtractor


class TestLangChainFactExtractor:
    """Test suite for LangChainFactExtractor integration with Gemini API."""

    @pytest.fixture
    def extractor(self) -> LangChainFactExtractor:
        """Create a LangChainFactExtractor instance for testing."""
        return LangChainFactExtractor()

    def test_initialization_without_api_key(self):
        """Test that initialization fails when GOOGLE_API_KEY is not set."""
        from app.core.settings import Settings

        # Mock the Settings class to return a mock with no google_api_key
        mock_settings = Settings()
        mock_settings.google_api_key = None

        with patch(
            "app.features.graph.services.langchain_fact_extractor.Settings",
            return_value=mock_settings,
        ):
            # Should raise ValueError
            with pytest.raises(
                ValueError, match="GOOGLE_API_KEY environment variable not set"
            ):
                LangChainFactExtractor()  # pyright: ignore[reportUnusedCallResult]

    @pytest.mark.asyncio
    async def test_extract_facts_basic_person_info(
        self, extractor: LangChainFactExtractor
    ):
        """Test fact extraction with basic person information."""
        content = "John Doe lives in Paris and works as a Software Engineer at Google."
        entity_identifier = IdentifierDto(type="email", value="john.doe@example.com")

        facts = await extractor.extract_facts(content, entity_identifier)

        # Verify response structure
        assert isinstance(facts, list)
        assert len(facts) > 0  # Should extract at least some facts

        for fact in facts:
            assert isinstance(fact, ExtractedFactDto)

            # Verify data types
            assert isinstance(fact.name, str)
            assert isinstance(fact.type, str)
            assert isinstance(fact.verb, str)
            assert isinstance(fact.confidence_score, (int, float))

            # Verify confidence score range
            assert 0.0 <= fact.confidence_score <= 1.0

            # Verify non-empty strings
            assert fact.name.strip()
            assert fact.type.strip()
            assert fact.verb.strip()

    @pytest.mark.asyncio
    async def test_extract_facts_company_info(self, extractor: LangChainFactExtractor):
        """Test fact extraction with company information."""
        content = "Apple Inc. is headquartered in Cupertino, California and was founded in 1976."
        entity_identifier = IdentifierDto(type="username", value="AppleInc")

        facts = await extractor.extract_facts(content, entity_identifier)

        assert isinstance(facts, list)
        assert len(facts) > 0

        # Check that relevant facts are extracted
        fact_names = [fact.name for fact in facts]
        assert any(
            location in fact_name
            for fact_name in fact_names
            for location in ["Cupertino", "California"]
        )

        # Verify all facts have required structure
        for fact in facts:
            assert isinstance(fact, ExtractedFactDto)
            assert fact.name
            assert fact.type
            assert fact.verb
            assert 0.0 <= fact.confidence_score <= 1.0

    @pytest.mark.asyncio
    async def test_extract_facts_empty_content(self, extractor: LangChainFactExtractor):
        """Test fact extraction with minimal/empty content."""
        content = "This is a test entity with minimal information."
        entity_identifier = IdentifierDto(type="username", value="test-entity-123")

        facts = await extractor.extract_facts(content, entity_identifier)

        # Should still return a list (possibly empty)
        assert isinstance(facts, list)
        # With the new prompt, this should ideally return no facts.
        assert len(facts) == 0

    @pytest.mark.asyncio
    async def test_extract_facts_from_conversational_turn_hobby(
        self, extractor: LangChainFactExtractor
    ):
        """Test fact extraction from a conversational turn about a hobby."""
        content = "I really enjoy hiking on weekends."
        entity_identifier = IdentifierDto(type="email", value="john.doe@example.com")

        facts = await extractor.extract_facts(content, entity_identifier)

        assert isinstance(facts, list)
        assert len(facts) > 0

        fact_names = {str(fact.name).lower() for fact in facts}
        assert "hiking" in fact_names

        # Check for a hobby-related fact
        hobby_fact_found = False
        for fact in facts:
            if str(fact.name).lower() == "hiking":
                assert str(fact.type).lower() in ["hobby", "activity"]
                assert str(fact.verb).lower() in ["enjoys", "likes"]
                hobby_fact_found = True
        assert hobby_fact_found, "Hobby fact about hiking not found"

    @pytest.mark.asyncio
    async def test_extract_facts_from_conversational_turn_sentiment(
        self, extractor: LangChainFactExtractor
    ):
        """Test extracting sentiment as a fact."""
        content = "I don't like Mondays."
        entity_identifier = IdentifierDto(type="username", value="user123")

        facts = await extractor.extract_facts(content, entity_identifier)

        assert isinstance(facts, list)
        assert len(facts) > 0

        fact_names = {str(fact.name).lower() for fact in facts}
        assert "mondays" in fact_names

        # Check for a sentiment-related fact
        sentiment_fact_found = False
        for fact in facts:
            if str(fact.name).lower() == "mondays":
                assert str(fact.verb).lower() in ["dislikes", "does_not_like"]
                sentiment_fact_found = True
        assert sentiment_fact_found, "Sentiment fact about Mondays not found"

    @pytest.mark.asyncio
    async def test_extract_facts_with_conversational_history(
        self, extractor: LangChainFactExtractor
    ):
        """Test fact extraction from a conversation in Portuguese."""
        history = [
            "ai: Entendido, Mariele. Focar no trabalho para destravar as outras √°reas √© uma vis√£o estrat√©gica.\n\nQuem vai conduzir esse pilar √© o Fl√°vio Augusto, que tem uma experi√™ncia gigante em construir neg√≥cios e gerar riqueza.\n\nMe diga, o que exatamente no seu trabalho voc√™ sente que precisa de mais clareza ou dire√ß√£o nesse momento?"
        ]
        content = "De tomar a decis√£o correta em uma empresa nova que eu e meu marido vamos abrir. A forma certa de iniciar este novo neg√≥cio"
        entity_identifier = IdentifierDto(type="email", value="mariele@example.com")

        facts = await extractor.extract_facts(
            content, entity_identifier, history=history
        )

        assert isinstance(facts, list)
        assert len(facts) > 0

        # Check that the standardization is working
        for fact in facts:
            # 'name' can be in Portuguese
            assert isinstance(fact.name, str)

            # 'type' and 'verb' must be in English. A simple check is to see if they are ASCII.
            assert str(fact.type).isascii()
            assert str(fact.verb).isascii()

            # Check for core concepts in name
            name_lower = str(fact.name).lower()
            assert "empresa" in name_lower or "neg√≥cio" in name_lower

        # Verify all facts have required structure
        for fact in facts:
            assert isinstance(fact, ExtractedFactDto)
            assert fact.name
            assert fact.type
            assert fact.verb
            assert 0.0 <= fact.confidence_score <= 1.0
</file>

<file path="apps/api/tests/utils/__init__.py">
"""Test utilities package."""
</file>

<file path="apps/api/tests/conftest.py">
"""Pytest configuration and shared fixtures for all tests.

This module provides function-scoped fixtures for:
- Test database lifecycle management
- SQLAlchemy engine and session management
- PostgreSQL/AGE connection pools
- Password hashing utilities
"""

import asyncio
from collections.abc import AsyncGenerator

import asyncpg
import pytest
import pytest_asyncio
from sqlalchemy.ext.asyncio import AsyncEngine, AsyncSession, create_async_engine

from app.core.authentication import pwd_context
from app.core.settings import Settings, get_settings
from app.features.auth.usecases.signup_tenant_usecase import PasswordHasher
from tests.utils.database import (
    cleanup_age_graphs,
    create_all_tables,
    create_test_database,
    drop_all_tables,
    drop_test_database,
    setup_age_extension,
)

# Module-level state
_test_db_initialized = False
_tables_created = False


@pytest.fixture(scope="function")
def test_settings() -> Settings:
    """Provide test settings with testing mode enabled."""
    settings = get_settings()
    settings.testing = True
    return settings


@pytest_asyncio.fixture(scope="function")
async def async_engine(test_settings: Settings) -> AsyncGenerator[AsyncEngine, None]:
    """Provide SQLAlchemy async engine for tests.

    Creates a fresh engine for each test to avoid event loop issues.
    """
    global _test_db_initialized, _tables_created

    # Create test database once
    if not _test_db_initialized:
        await create_test_database(test_settings)

        # Create temporary pool to setup AGE
        temp_pool = await asyncpg.create_pool(
            user=test_settings.postgres_user,
            password=test_settings.postgres_password,
            host=test_settings.postgres_host,
            port=test_settings.postgres_port,
            database=test_settings.test_postgres_db,
            min_size=1,
            max_size=2,
        )
        await setup_age_extension(temp_pool)
        await temp_pool.close()

        _test_db_initialized = True

    # Create fresh engine for this test
    engine = create_async_engine(test_settings.database_url, echo=False)

    # Create tables once per session
    if not _tables_created:
        await create_all_tables(engine)
        _tables_created = True

    yield engine

    # Dispose engine after test
    await engine.dispose()


@pytest_asyncio.fixture
async def db_session(async_engine: AsyncEngine) -> AsyncGenerator[AsyncSession, None]:
    """Provide a database session for each test.

    Each test gets a fresh session. The session does not auto-commit,
    allowing tests or use cases to manage their own transactions.
    After the test, rolls back any uncommitted changes.
    """
    async with AsyncSession(async_engine, expire_on_commit=False) as session:
        yield session
        # Roll back any uncommitted changes
        await session.rollback()


@pytest_asyncio.fixture(autouse=True)
async def clean_tables(async_engine: AsyncEngine) -> AsyncGenerator[None, None]:
    """Clean all table data before each test.

    This ensures each test starts with a clean slate while keeping the schema intact.
    """
    yield

    # Clean all tables after test
    async with async_engine.begin() as conn:
        from sqlalchemy import text

        # Disable foreign key checks temporarily
        await conn.execute(text("SET session_replication_role = 'replica';"))

        try:
            # Get all table names
            result = await conn.execute(
                text(
                    """
                    SELECT tablename FROM pg_tables
                    WHERE schemaname = 'public'
                    AND tablename NOT LIKE 'pg_%'
                    AND tablename NOT LIKE 'sql_%'
                    AND tablename NOT LIKE 'alembic%'
                    """
                )
            )

            tables = [row[0] for row in result]

            # Truncate all tables
            for table in tables:
                await conn.execute(text(f'TRUNCATE TABLE "{table}" CASCADE;'))
        finally:
            # Re-enable foreign key checks
            await conn.execute(text("SET session_replication_role = 'origin';"))


@pytest_asyncio.fixture(scope="function")
async def postgres_pool(test_settings: Settings) -> AsyncGenerator[asyncpg.Pool, None]:
    """Provide PostgreSQL connection pool for AGE operations.

    Creates a fresh pool for each test to avoid event loop issues.
    """
    global _test_db_initialized

    # Ensure test database exists
    if not _test_db_initialized:
        await create_test_database(test_settings)

        # Create temporary pool to setup AGE
        temp_pool = await asyncpg.create_pool(
            user=test_settings.postgres_user,
            password=test_settings.postgres_password,
            host=test_settings.postgres_host,
            port=test_settings.postgres_port,
            database=test_settings.test_postgres_db,
            min_size=1,
            max_size=2,
        )
        await setup_age_extension(temp_pool)
        await temp_pool.close()

        _test_db_initialized = True

    # Create fresh pool for this test
    pool = await asyncpg.create_pool(
        user=test_settings.postgres_user,
        password=test_settings.postgres_password,
        host=test_settings.postgres_host,
        port=test_settings.postgres_port,
        database=test_settings.test_postgres_db,
        min_size=2,
        max_size=10,
    )

    yield pool

    # Close pool after test
    await pool.close()


@pytest.fixture
def password_hasher() -> PasswordHasher:
    """Provide password hasher instance for tests."""
    return pwd_context


@pytest_asyncio.fixture(autouse=True)
async def clean_graph_data(postgres_pool: asyncpg.Pool) -> AsyncGenerator[None, None]:
    """Clean AGE graph data before each test.

    This fixture automatically runs before each test to ensure a clean state.
    It creates a test_graph if needed and clears all data from it.
    """
    graph_name = "test_graph"

    # Skip if pool is closed/closing (for tests that manage pool lifecycle)
    if not postgres_pool.is_closing():
        # Acquire connection and set it up
        conn = await postgres_pool.acquire()
        try:
            await conn.execute("LOAD 'age';")
            await conn.execute("SET search_path = ag_catalog, '$user', public;")

            # Create test_graph if it doesn't exist
            graph_exists = await conn.fetchval(
                "SELECT 1 FROM ag_graph WHERE name = $1;", graph_name
            )
            if not graph_exists:
                await conn.execute(f"SELECT create_graph('{graph_name}');")

            # Clear all data from the graph
            try:
                await conn.execute(
                    f"SELECT * FROM ag_catalog.cypher('{graph_name}', $$ MATCH (n) DETACH DELETE n $$) as (v agtype);"
                )
            except Exception:
                # Ignore if graph is already empty
                pass
        finally:
            await postgres_pool.release(conn)

    yield

    # Clean after test as well (for safety) - skip if pool is closed
    if not postgres_pool.is_closing():
        conn = await postgres_pool.acquire()
        try:
            await conn.execute("LOAD 'age';")
            await conn.execute("SET search_path = ag_catalog, '$user', public;")
            try:
                await conn.execute(
                    f"SELECT * FROM ag_catalog.cypher('{graph_name}', $$ MATCH (n) DETACH DELETE n $$) as (v agtype);"
                )
            except Exception:
                pass
        finally:
            await postgres_pool.release(conn)


def pytest_sessionfinish(session, exitstatus):
    """Cleanup at the end of the test session."""
    global _test_db_initialized, _tables_created

    # Run cleanup in an async context
    async def cleanup():
        test_settings = get_settings()
        test_settings.testing = True

        # Drop tables if they were created
        if _tables_created:
            try:
                # Create engine just for dropping tables
                engine = create_async_engine(test_settings.database_url, echo=False)
                await drop_all_tables(engine)
                await engine.dispose()
            except Exception as e:
                print(f"Warning: Failed to drop tables: {e}")

        # Drop test database (cleanup graphs first with a temp pool)
        if _test_db_initialized:
            try:
                # Create temporary pool to cleanup graphs before dropping database
                temp_pool = await asyncpg.create_pool(
                    user=test_settings.postgres_user,
                    password=test_settings.postgres_password,
                    host=test_settings.postgres_host,
                    port=test_settings.postgres_port,
                    database=test_settings.test_postgres_db,
                    min_size=1,
                    max_size=2,
                )
                await cleanup_age_graphs(temp_pool)
                await temp_pool.close()
            except Exception as e:
                print(f"Warning: Failed to cleanup graphs: {e}")

            try:
                await drop_test_database(test_settings)
            except Exception as e:
                print(f"Warning: Failed to drop test database: {e}")

    # Run the cleanup
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # Create a new loop if current one is running
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        loop.run_until_complete(cleanup())
    except Exception as e:
        print(f"Warning: Session cleanup failed: {e}")
</file>

<file path="apps/api/tests/TEST_ISOLATION_SUMMARY.md">
# Test Database Isolation - Implementation Summary

## ‚úÖ Completed Implementation

Successfully implemented comprehensive test database isolation for the Nous API project. All integration tests now use a separate test database that is automatically managed, ensuring your development database is never touched during testing.

## üéØ What Was Accomplished

### 1. Enhanced Settings for Test Mode

**File: `apps/api/app/core/settings.py`**

Added test-specific configuration:

- `testing: bool` field - Enables test mode
- `test_postgres_db: str` field - Test database name (default: `multimodel_db_test`)
- Modified `database_url` property to automatically switch to test database when `testing=True`

### 2. Created Database Utilities

**File: `apps/api/tests/utils/database.py`**

Comprehensive helper functions for test database management:

- `create_test_database()` - Creates test database if needed
- `drop_test_database()` - Safely drops test database
- `setup_age_extension()` - Installs and configures AGE extension
- `cleanup_age_graphs()` - Removes all AGE graphs
- `create_all_tables()` - Creates tables from SQLAlchemy models
- `drop_all_tables()` - Drops all tables
- `clear_all_tables()` - Truncates tables without dropping schema

### 3. Created Shared Test Fixtures

**File: `apps/api/tests/conftest.py`**

Centralized pytest fixtures for all tests:

- **`test_settings`** - Settings with testing mode enabled
- **`async_engine`** - Fresh SQLAlchemy engine per test
- **`db_session`** - Clean database session per test
- **`postgres_pool`** - PostgreSQL connection pool for AGE operations
- **`password_hasher`** - Password hashing utility
- **`clean_tables`** (autouse) - Truncates all tables after each test
- **`clean_graph_data`** (autouse) - Clears AGE graph data before/after each test
- **`pytest_sessionfinish`** - Cleanup hook to drop test database after session

### 4. Updated All Integration Tests

Simplified test files by removing duplicate fixtures:

**a) `test_signup_tenant_usecase_integration.py`**

- ‚úÖ Removed custom async_engine, db_session, postgres_pool, password_hasher fixtures
- ‚úÖ Now uses shared fixtures from conftest.py

**b) `test_age_repository_integration.py`**

- ‚úÖ Removed custom postgres_pool and clean_graph_db fixtures
- ‚úÖ Now uses shared fixtures

**c) `test_assimilate_knowledge_usecase_integration.py`**

- ‚úÖ Removed custom postgres_pool and reset_db_connection fixtures
- ‚úÖ Now uses shared fixtures

**d) `test_get_entity_usecase_integration.py`**

- ‚úÖ Removed custom postgres_pool and reset_db_connection fixtures
- ‚úÖ Now uses shared fixtures

### 5. Updated Documentation

**File: `apps/api/README.md`**

Added comprehensive testing documentation:

- Explanation of test database isolation
- Configuration instructions
- How to run tests safely
- Examples of running specific tests

## üîí How It Works

1. **Automatic Setup**: When tests start, a dedicated test database (`multimodel_db_test`) is created automatically
2. **Schema Creation**: Tables are created once per session from SQLAlchemy models (no migrations needed in tests)
3. **AGE Extension**: PostgreSQL AGE extension is installed and configured automatically
4. **Per-Test Isolation**:
   - Each test gets fresh database connections (new engine and pool per test)
   - Tables are truncated after each test (keeping schema intact)
   - AGE graphs are cleared before and after each test
5. **Clean Teardown**: Test database is automatically dropped after all tests complete

## üéâ Key Benefits

‚úÖ **Complete Isolation**: Development database (`multimodel_db`) is never touched
‚úÖ **No Manual Setup**: Everything is automatic - just run pytest
‚úÖ **Clean Slate**: Each test starts with empty tables and graphs
‚úÖ **Shared Fixtures**: Reusable fixtures across all test files
‚úÖ **AGE Support**: Full AGE graph database support in tests
‚úÖ **Fast**: Tables created once, truncated between tests (not dropped/recreated)
‚úÖ **CI-Ready**: Works locally and in CI environments

## üìù Running Tests

```bash
# Run all tests
uv run pytest tests/

# Run specific test file
uv run pytest tests/features/auth/usecases/test_signup_tenant_usecase_integration.py -v

# Run specific test
uv run pytest tests/features/graph/repositories/test_age_repository_integration.py::TestCreateEntity::test_create_entity_basic -v

# Run with coverage
uv run pytest tests/ --cov=app --cov-report=html
```

## ‚úÖ Test Results

All integration tests passing:

- ‚úÖ 5/5 auth integration tests
- ‚úÖ Graph repository integration tests
- ‚úÖ Graph usecase integration tests
- ‚úÖ All tests use isolated test database
- ‚úÖ No interference between tests
- ‚úÖ Development database completely safe

## üîß Configuration

Tests automatically use the test database via the `testing=True` setting. You can customize via environment variables:

```bash
TESTING=true
POSTGRES_DB=multimodel_db_test
POSTGRES_USER=admin
POSTGRES_PASSWORD=supersecretpassword
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
```

## üìÇ Files Created/Modified

### Created:

- `apps/api/tests/conftest.py` - Shared test fixtures
- `apps/api/tests/utils/__init__.py` - Utils package
- `apps/api/tests/utils/database.py` - Database utility functions
- `apps/api/TEST_ISOLATION_SUMMARY.md` - This file

### Modified:

- `apps/api/app/core/settings.py` - Added test mode settings
- `apps/api/README.md` - Added testing documentation
- `apps/api/tests/features/auth/usecases/test_signup_tenant_usecase_integration.py` - Simplified
- `apps/api/tests/features/graph/repositories/test_age_repository_integration.py` - Simplified
- `apps/api/tests/features/graph/usecases/test_assimilate_knowledge_usecase_integration.py` - Simplified
- `apps/api/tests/features/graph/usecases/test_get_entity_usecase_integration.py` - Simplified

## üöÄ Next Steps

Your test suite is now fully isolated and ready to use! You can:

1. Run tests anytime without worrying about your development database
2. Add new integration tests using the shared fixtures
3. Run tests in CI/CD pipelines safely
4. Debug tests with confidence that they won't affect your dev data

---

**Implementation Date**: 2025-10-23
**Status**: ‚úÖ Complete and Tested
</file>

<file path="apps/api/.env.example">
# psql
POSTGRES_USER=
POSTGRES_PASSWORD=
POSTGRES_HOST=
POSTGRES_PORT=
POSTGRES_DB=
AGE_GRAPH_NAME=

# google
GOOGLE_API_KEY=
</file>

<file path="apps/api/alembic.ini">
# A generic, single database configuration.

[alembic]
# path to migration scripts.
# this is typically a path given in POSIX (e.g. forward slashes)
# format, relative to the token %(here)s which refers to the location of this
# ini file
script_location = %(here)s/migrations

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.  for multiple paths, the path separator
# is defined by "path_separator" below.
prepend_sys_path = .


# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the tzdata library which can be installed by adding
# `alembic[tz]` to the pip requirements.
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to <script_location>/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "path_separator"
# below.
# version_locations = %(here)s/bar:%(here)s/bat:%(here)s/alembic/versions

# path_separator; This indicates what character is used to split lists of file
# paths, including version_locations and prepend_sys_path within configparser
# files such as alembic.ini.
# The default rendered in new alembic.ini files is "os", which uses os.pathsep
# to provide os-dependent path splitting.
#
# Note that in order to support legacy alembic.ini files, this default does NOT
# take place if path_separator is not present in alembic.ini.  If this
# option is omitted entirely, fallback logic is as follows:
#
# 1. Parsing of the version_locations option falls back to using the legacy
#    "version_path_separator" key, which if absent then falls back to the legacy
#    behavior of splitting on spaces and/or commas.
# 2. Parsing of the prepend_sys_path option falls back to the legacy
#    behavior of splitting on spaces, commas, or colons.
#
# Valid values for path_separator are:
#
# path_separator = :
# path_separator = ;
# path_separator = space
# path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

# database URL.  This is consumed by the user-maintained env.py script only.
# other means of configuring database URLs may be customized within the env.py
# file.
sqlalchemy.url =


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the module runner, against the "ruff" module
# hooks = ruff
# ruff.type = module
# ruff.module = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Alternatively, use the exec runner to execute a binary found on your PATH
# hooks = ruff
# ruff.type = exec
# ruff.executable = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Logging configuration.  This is also consumed by the user-maintained
# env.py script only.
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
</file>

<file path="apps/api/package.json">
{
  "name": "api",
  "scripts": {
    "dev": "uv run uvicorn app.main:app --reload --host 0.0.0.0 --port 8000",
    "lint": "uv run ruff check . && uv run ruff format .",
    "test": "uv run pytest"
  }
}
</file>

<file path="apps/web/.vscode/extensions.json">
{
  "recommendations": ["Vue.volar"]
}
</file>

<file path="apps/web/docs/plans/api-layer.md">
# API Layer Implementation Plan

## 1. Objective

This document outlines the plan to implement the API communication layer for the Vue.js frontend, connecting it to our FastAPI backend. We will use the `@vueuse/core` library, specifically its `useFetch` composable, to handle all HTTP requests in a reactive and efficient manner.

The goal is to create a clean, reusable, and type-safe API layer.

## 2. Core Utility: `createFetch`

To avoid repeating configuration and to establish a consistent base for all API calls, we will use `createFetch`. This utility allows us to create a custom, pre-configured instance of `useFetch`.

We will create this instance in `apps/web/src/api/useApiFetch.ts`.

**Key Configurations:**

- **`baseUrl`**: All requests will be automatically prefixed with the API base URL (`http://localhost:8000/api/v1`).
- **`fetchOptions`**: We will set default headers, such as `Content-Type: application/json`.
- **Error Handling**: A default `onFetchError` handler will be configured to centrally manage and log API errors.

**`apps/web/src/api/useApiFetch.ts`**

```typescript
import { createFetch } from "@vueuse/core";

const BASE_URL = "http://localhost:8000/api/v1"; // Or from .env

export const useApiFetch = createFetch({
  baseUrl: BASE_URL,
  options: {
    // Standard hooks that run before every fetch call
    async beforeFetch({ options }) {
      // Here you could add authentication tokens to headers
      // const myAuthToken = '...'
      // options.headers.Authorization = `Bearer ${myAuthToken}`
      return { options };
    },
  },
  // Default options for the fetch request itself
  fetchOptions: {
    headers: {
      "Content-Type": "application/json",
    },
  },
});
```

## 3. API Function Implementations

All API functions related to the `graph` feature will be located in `apps/web/src/features/graph/api/graphApi.ts`. These functions will encapsulate the logic for each endpoint, providing a clean interface for our Vue components and Pinia stores.

They are designed as "composables" (and named with a `use` prefix) that return the entire reactive `UseFetchReturn` object from `@vueuse/core`. This gives you direct, reactive access to `data`, `isFetching`, `error`, and other properties.

### 3.1. Looking up an Entity (Reactive Composable)

This composable will fetch an entity and its related facts. It's designed to be fully reactive: if you pass it a `ref` for its parameters, it will automatically refetch when the parameters change.

- **Endpoint**: `GET /graph/entities/lookup`
- **Method**: `GET`
- **Query Parameters**: `type: string`, `value: string`

**`apps/web/src/features/graph/api/graphApi.ts`**

```typescript
import { useApiFetch } from "@/api";
import type { GetEntityResponse } from "@/types/api";
import { computed, toValue, type MaybeRefOrGetter } from "vue";

export interface FindEntityParams {
  type: string;
  value: string;
}

export const useFindEntityByIdentifier = (
  params: MaybeRefOrGetter<FindEntityParams>
) => {
  // The URL is a computed property, reacting to changes in `params`
  const url = computed(() => {
    const resolvedParams = toValue(params);
    // Return empty string to prevent invalid requests, let consumers handle empty state
    if (!resolvedParams || !resolvedParams.value) {
      return "";
    }
    return `/graph/entities/lookup?type=${resolvedParams.type}&value=${resolvedParams.value}`;
  });

  // The full reactive useFetch object is returned.
  return useApiFetch(url, {
    refetch: true,
    immediate: false, // Don't execute immediately, let consumers control execution
  })
    .get()
    .json<GetEntityResponse>();
};
```

### 3.2. Assimilating Knowledge (Action Trigger)

This function will send new content to the backend. Since this is a `POST` request (a mutation), it's designed to be called explicitly as an action, rather than reacting to changes. You can still access `isFetching` and `error` states after calling it.

- **Endpoint**: `POST /graph/entities/assimilate`
- **Method**: `POST`
- **Request Body**: `AssimilateKnowledgeRequest`

**`apps/web/src/features/graph/api/graphApi.ts` (continued)**

```typescript
// ... imports
import type {
  AssimilateKnowledgeRequest,
  AssimilateKnowledgeResponse,
} from "@/types/api";

// ... useFindEntityByIdentifier function

export const useAssimilateKnowledge = (payload: AssimilateKnowledgeRequest) => {
  const url = "/graph/entities/assimilate";

  // Use the .post() convenience method, passing the payload.
  // This returns the reactive useFetch object, which you can `await` or
  // use to track the state of the POST request.
  return useApiFetch(url).post(payload).json<AssimilateKnowledgeResponse>();
};
```

## 4. TypeScript Types

To ensure type safety across the application, we will define TypeScript interfaces that mirror the Pydantic DTOs from the backend. These will reside in `apps/web/src/types/api.ts`.

**`apps/web/src/types/api.ts`**

```typescript
// Base DTOs
export interface IdentifierDto {
  value: string;
  type: string;
}

export interface EntityDto {
  id: string; // UUID is a string in TS
  created_at: string; // ISO date string
  metadata?: Record<string, string> | null;
}

export interface FactDto {
  name: string;
  type: string;
  fact_id?: string | null;
}

export interface SourceDto {
  id: string; // UUID
  content: string;
  timestamp: string; // ISO date string
}

export interface HasFactDto {
  verb: string;
  confidence_score: number;
  created_at: string; // ISO date string
}

// Request Payloads
export interface AssimilateKnowledgeRequest {
  identifier: IdentifierDto;
  content: string;
  timestamp?: string | null; // ISO date string
  history?: string[] | null;
}

// API Responses
export interface AssimilatedFactDto {
  fact: FactDto;
  relationship: HasFactDto;
}

export interface AssimilateKnowledgeResponse {
  entity: EntityDto;
  source: SourceDto;
  assimilated_facts: AssimilatedFactDto[];
}

export interface HasIdentifierDto {
  is_primary: boolean;
  created_at: string; // ISO date string
}

export interface IdentifierWithRelationshipDto {
  identifier: IdentifierDto;
  relationship: HasIdentifierDto;
}

export interface FactWithSourceDto {
  fact: FactDto;
  relationship: HasFactDto;
  source?: SourceDto | null;
}

export interface GetEntityResponse {
  entity: EntityDto;
  identifier: IdentifierWithRelationshipDto;
  facts: FactWithSourceDto[];
}
```

## 5. Reactive Usage in a Pinia Store

With the reactive `useFindEntityByIdentifier` composable, our Pinia store becomes much simpler and more powerful. We no longer need to manage our own `isLoading` or `error` refs, as the composable provides them for us.

**Example: `apps/web/src/features/graph/store.ts` (Pinia Store)**

```typescript
import { defineStore } from "pinia";
import { ref } from "vue";
import { useFindEntityByIdentifier } from "@/api/graphApi";
import type { FindEntityParams } from "@/api/graphApi";

export const useGraphStore = defineStore("graph", () => {
  // A ref to hold the search parameters. Our API composable will react to this.
  const searchParams = ref<FindEntityParams>({ type: "email", value: "" });

  // Call the composable. It's that simple.
  // `data`, `isFetching`, and `error` are all reactive refs.
  const { data, isFetching, error } = useFindEntityByIdentifier(searchParams);

  // An action that simply updates the search parameters.
  // The `useFindEntityByIdentifier` composable will automatically
  // detect the change and trigger a new API call.
  function searchEntity(params: FindEntityParams) {
    searchParams.value = params;
  }

  // Expose the reactive state and the action to the components.
  return { data, isFetching, error, searchEntity };
});
```
</file>

<file path="apps/web/public/vite.svg">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="31.88" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 257"><defs><linearGradient id="IconifyId1813088fe1fbc01fb466" x1="-.828%" x2="57.636%" y1="7.652%" y2="78.411%"><stop offset="0%" stop-color="#41D1FF"></stop><stop offset="100%" stop-color="#BD34FE"></stop></linearGradient><linearGradient id="IconifyId1813088fe1fbc01fb467" x1="43.376%" x2="50.316%" y1="2.242%" y2="89.03%"><stop offset="0%" stop-color="#FFEA83"></stop><stop offset="8.333%" stop-color="#FFDD35"></stop><stop offset="100%" stop-color="#FFA800"></stop></linearGradient></defs><path fill="url(#IconifyId1813088fe1fbc01fb466)" d="M255.153 37.938L134.897 252.976c-2.483 4.44-8.862 4.466-11.382.048L.875 37.958c-2.746-4.814 1.371-10.646 6.827-9.67l120.385 21.517a6.537 6.537 0 0 0 2.322-.004l117.867-21.483c5.438-.991 9.574 4.796 6.877 9.62Z"></path><path fill="url(#IconifyId1813088fe1fbc01fb467)" d="M185.432.063L96.44 17.501a3.268 3.268 0 0 0-2.634 3.014l-5.474 92.456a3.268 3.268 0 0 0 3.997 3.378l24.777-5.718c2.318-.535 4.413 1.507 3.936 3.838l-7.361 36.047c-.495 2.426 1.782 4.5 4.151 3.78l15.304-4.649c2.372-.72 4.652 1.36 4.15 3.788l-11.698 56.621c-.732 3.542 3.979 5.473 5.943 2.437l1.313-2.028l72.516-144.72c1.215-2.423-.88-5.186-3.54-4.672l-25.505 4.922c-2.396.462-4.435-1.77-3.759-4.114l16.646-57.705c.677-2.35-1.37-4.583-3.769-4.113Z"></path></svg>
</file>

<file path="apps/web/src/api/index.ts">
export { useApiFetch } from "./useApiFetch";
</file>

<file path="apps/web/src/api/useApiFetch.ts">
import { createFetch } from "@vueuse/core";

const BASE_URL = "http://localhost:8000/api/v1"; // Or from .env

export const useApiFetch = createFetch({
  baseUrl: BASE_URL,
  options: {
    // Standard hooks that run before every fetch call
    async beforeFetch({ options }) {
      // Here you could add authentication tokens to headers
      // const myAuthToken = '...'
      // options.headers.Authorization = `Bearer ${myAuthToken}`
      return { options };
    },
  },
  // Default options for the fetch request itself
  fetchOptions: {
    headers: {
      "Content-Type": "application/json",
    },
  },
});
</file>

<file path="apps/web/src/assets/vue.svg">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="37.07" height="36" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 198"><path fill="#41B883" d="M204.8 0H256L128 220.8L0 0h97.92L128 51.2L157.44 0h47.36Z"></path><path fill="#41B883" d="m0 0l128 220.8L256 0h-51.2L128 132.48L50.56 0H0Z"></path><path fill="#35495E" d="M50.56 0L128 133.12L204.8 0h-47.36L128 51.2L97.92 0H50.56Z"></path></svg>
</file>

<file path="apps/web/src/components/layout/navigation/index.ts">
export { default as Navigation } from "./Navigation.vue";
</file>

<file path="apps/web/src/components/layout/navigation/Navigation.vue">
<script setup lang="ts">
import { useRouter } from "vue-router";

import { Button } from "@/components/ui/button";

const router = useRouter();

const handleLogout = () => {
  // Clear authentication state
  localStorage.removeItem("isLoggedIn");

  // Redirect to login page
  router.push("/login");
};
</script>

<template>
  <header class="bg-card shadow-sm border-b border-border">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
      <div class="flex justify-between items-center h-16">
        <!-- Logo/Brand -->
        <div class="flex items-center">
          <h1 class="text-xl font-semibold text-card-foreground">Nous</h1>
        </div>

        <!-- Logout Button -->
        <Button
          @click="handleLogout"
          variant="outline"
          size="sm"
          class="text-muted-foreground hover:text-foreground"
        >
          Logout
        </Button>
      </div>
    </div>
  </header>
</template>
</file>

<file path="apps/web/src/components/ui/alert/Alert.vue">
<script setup lang="ts">
import type { HTMLAttributes } from "vue"
import type { AlertVariants } from "."
import { cn } from "@/lib/utils"
import { alertVariants } from "."

const props = defineProps<{
  class?: HTMLAttributes["class"]
  variant?: AlertVariants["variant"]
}>()
</script>

<template>
  <div
    data-slot="alert"
    :class="cn(alertVariants({ variant }), props.class)"
    role="alert"
  >
    <slot />
  </div>
</template>
</file>

<file path="apps/web/src/components/ui/alert/AlertDescription.vue">
<script setup lang="ts">
import type { HTMLAttributes } from "vue"
import { cn } from "@/lib/utils"

const props = defineProps<{
  class?: HTMLAttributes["class"]
}>()
</script>

<template>
  <div
    data-slot="alert-description"
    :class="cn('text-muted-foreground col-start-2 grid justify-items-start gap-1 text-sm [&_p]:leading-relaxed', props.class)"
  >
    <slot />
  </div>
</template>
</file>

<file path="apps/web/src/components/ui/alert/AlertTitle.vue">
<script setup lang="ts">
import type { HTMLAttributes } from "vue"
import { cn } from "@/lib/utils"

const props = defineProps<{
  class?: HTMLAttributes["class"]
}>()
</script>

<template>
  <div
    data-slot="alert-title"
    :class="cn('col-start-2 line-clamp-1 min-h-4 font-medium tracking-tight', props.class)"
  >
    <slot />
  </div>
</template>
</file>

<file path="apps/web/src/components/ui/alert/index.ts">
import type { VariantProps } from "class-variance-authority"
import { cva } from "class-variance-authority"

export { default as Alert } from "./Alert.vue"
export { default as AlertDescription } from "./AlertDescription.vue"
export { default as AlertTitle } from "./AlertTitle.vue"

export const alertVariants = cva(
  "relative w-full rounded-lg border px-4 py-3 text-sm grid has-[>svg]:grid-cols-[calc(var(--spacing)*4)_1fr] grid-cols-[0_1fr] has-[>svg]:gap-x-3 gap-y-0.5 items-start [&>svg]:size-4 [&>svg]:translate-y-0.5 [&>svg]:text-current",
  {
    variants: {
      variant: {
        default: "bg-card text-card-foreground",
        destructive:
          "text-destructive bg-card [&>svg]:text-current *:data-[slot=alert-description]:text-destructive/90",
      },
    },
    defaultVariants: {
      variant: "default",
    },
  },
)

export type AlertVariants = VariantProps<typeof alertVariants>
</file>

<file path="apps/web/src/components/ui/button/Button.vue">
<script setup lang="ts">
import type { PrimitiveProps } from "reka-ui"
import type { HTMLAttributes } from "vue"
import type { ButtonVariants } from "."
import { Primitive } from "reka-ui"
import { cn } from "@/lib/utils"
import { buttonVariants } from "."

interface Props extends PrimitiveProps {
  variant?: ButtonVariants["variant"]
  size?: ButtonVariants["size"]
  class?: HTMLAttributes["class"]
}

const props = withDefaults(defineProps<Props>(), {
  as: "button",
})
</script>

<template>
  <Primitive
    data-slot="button"
    :as="as"
    :as-child="asChild"
    :class="cn(buttonVariants({ variant, size }), props.class)"
  >
    <slot />
  </Primitive>
</template>
</file>

<file path="apps/web/src/components/ui/button/index.ts">
import type { VariantProps } from "class-variance-authority"
import { cva } from "class-variance-authority"

export { default as Button } from "./Button.vue"

export const buttonVariants = cva(
  "inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-all disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg:not([class*='size-'])]:size-4 shrink-0 [&_svg]:shrink-0 outline-none focus-visible:border-ring focus-visible:ring-ring/50 focus-visible:ring-[3px] aria-invalid:ring-destructive/20 dark:aria-invalid:ring-destructive/40 aria-invalid:border-destructive",
  {
    variants: {
      variant: {
        default:
          "bg-primary text-primary-foreground shadow-xs hover:bg-primary/90",
        destructive:
          "bg-destructive text-white shadow-xs hover:bg-destructive/90 focus-visible:ring-destructive/20 dark:focus-visible:ring-destructive/40 dark:bg-destructive/60",
        outline:
          "border bg-background shadow-xs hover:bg-accent hover:text-accent-foreground dark:bg-input/30 dark:border-input dark:hover:bg-input/50",
        secondary:
          "bg-secondary text-secondary-foreground shadow-xs hover:bg-secondary/80",
        ghost:
          "hover:bg-accent hover:text-accent-foreground dark:hover:bg-accent/50",
        link: "text-primary underline-offset-4 hover:underline",
      },
      size: {
        "default": "h-9 px-4 py-2 has-[>svg]:px-3",
        "sm": "h-8 rounded-md gap-1.5 px-3 has-[>svg]:px-2.5",
        "lg": "h-10 rounded-md px-6 has-[>svg]:px-4",
        "icon": "size-9",
        "icon-sm": "size-8",
        "icon-lg": "size-10",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  },
)

export type ButtonVariants = VariantProps<typeof buttonVariants>
</file>

<file path="apps/web/src/components/ui/card/Card.vue">
<script setup lang="ts">
import type { HTMLAttributes } from "vue"
import { cn } from "@/lib/utils"

const props = defineProps<{
  class?: HTMLAttributes["class"]
}>()
</script>

<template>
  <div
    data-slot="card"
    :class="
      cn(
        'bg-card text-card-foreground flex flex-col gap-6 rounded-xl border py-6 shadow-sm',
        props.class,
      )
    "
  >
    <slot />
  </div>
</template>
</file>

<file path="apps/web/src/components/ui/card/CardAction.vue">
<script setup lang="ts">
import type { HTMLAttributes } from "vue"
import { cn } from "@/lib/utils"

const props = defineProps<{
  class?: HTMLAttributes["class"]
}>()
</script>

<template>
  <div
    data-slot="card-action"
    :class="cn('col-start-2 row-span-2 row-start-1 self-start justify-self-end', props.class)"
  >
    <slot />
  </div>
</template>
</file>

<file path="apps/web/src/components/ui/card/CardContent.vue">
<script setup lang="ts">
import type { HTMLAttributes } from "vue"
import { cn } from "@/lib/utils"

const props = defineProps<{
  class?: HTMLAttributes["class"]
}>()
</script>

<template>
  <div
    data-slot="card-content"
    :class="cn('px-6', props.class)"
  >
    <slot />
  </div>
</template>
</file>

<file path="apps/web/src/components/ui/card/CardDescription.vue">
<script setup lang="ts">
import type { HTMLAttributes } from "vue"
import { cn } from "@/lib/utils"

const props = defineProps<{
  class?: HTMLAttributes["class"]
}>()
</script>

<template>
  <p
    data-slot="card-description"
    :class="cn('text-muted-foreground text-sm', props.class)"
  >
    <slot />
  </p>
</template>
</file>

<file path="apps/web/src/components/ui/card/CardFooter.vue">
<script setup lang="ts">
import type { HTMLAttributes } from "vue"
import { cn } from "@/lib/utils"

const props = defineProps<{
  class?: HTMLAttributes["class"]
}>()
</script>

<template>
  <div
    data-slot="card-footer"
    :class="cn('flex items-center px-6 [.border-t]:pt-6', props.class)"
  >
    <slot />
  </div>
</template>
</file>

<file path="apps/web/src/components/ui/card/CardHeader.vue">
<script setup lang="ts">
import type { HTMLAttributes } from "vue"
import { cn } from "@/lib/utils"

const props = defineProps<{
  class?: HTMLAttributes["class"]
}>()
</script>

<template>
  <div
    data-slot="card-header"
    :class="cn('@container/card-header grid auto-rows-min grid-rows-[auto_auto] items-start gap-1.5 px-6 has-data-[slot=card-action]:grid-cols-[1fr_auto] [.border-b]:pb-6', props.class)"
  >
    <slot />
  </div>
</template>
</file>

<file path="apps/web/src/components/ui/card/CardTitle.vue">
<script setup lang="ts">
import type { HTMLAttributes } from "vue"
import { cn } from "@/lib/utils"

const props = defineProps<{
  class?: HTMLAttributes["class"]
}>()
</script>

<template>
  <h3
    data-slot="card-title"
    :class="cn('leading-none font-semibold', props.class)"
  >
    <slot />
  </h3>
</template>
</file>

<file path="apps/web/src/components/ui/card/index.ts">
export { default as Card } from "./Card.vue"
export { default as CardAction } from "./CardAction.vue"
export { default as CardContent } from "./CardContent.vue"
export { default as CardDescription } from "./CardDescription.vue"
export { default as CardFooter } from "./CardFooter.vue"
export { default as CardHeader } from "./CardHeader.vue"
export { default as CardTitle } from "./CardTitle.vue"
</file>

<file path="apps/web/src/components/ui/input/index.ts">
export { default as Input } from "./Input.vue"
</file>

<file path="apps/web/src/components/ui/menubar/index.ts">
export { default as Menubar } from "./Menubar.vue"
export { default as MenubarCheckboxItem } from "./MenubarCheckboxItem.vue"
export { default as MenubarContent } from "./MenubarContent.vue"
export { default as MenubarGroup } from "./MenubarGroup.vue"
export { default as MenubarItem } from "./MenubarItem.vue"
export { default as MenubarLabel } from "./MenubarLabel.vue"
export { default as MenubarMenu } from "./MenubarMenu.vue"
export { default as MenubarRadioGroup } from "./MenubarRadioGroup.vue"
export { default as MenubarRadioItem } from "./MenubarRadioItem.vue"
export { default as MenubarSeparator } from "./MenubarSeparator.vue"
export { default as MenubarShortcut } from "./MenubarShortcut.vue"
export { default as MenubarSub } from "./MenubarSub.vue"
export { default as MenubarSubContent } from "./MenubarSubContent.vue"
export { default as MenubarSubTrigger } from "./MenubarSubTrigger.vue"
export { default as MenubarTrigger } from "./MenubarTrigger.vue"
</file>

<file path="apps/web/src/components/ui/menubar/Menubar.vue">
<script setup lang="ts">
import type { MenubarRootEmits, MenubarRootProps } from "reka-ui"
import type { HTMLAttributes } from "vue"
import { reactiveOmit } from "@vueuse/core"
import {
  MenubarRoot,

  useForwardPropsEmits,
} from "reka-ui"
import { cn } from "@/lib/utils"

const props = defineProps<MenubarRootProps & { class?: HTMLAttributes["class"] }>()
const emits = defineEmits<MenubarRootEmits>()

const delegatedProps = reactiveOmit(props, "class")

const forwarded = useForwardPropsEmits(delegatedProps, emits)
</script>

<template>
  <MenubarRoot
    data-slot="menubar"
    v-bind="forwarded"
    :class="
      cn(
        'bg-background flex h-9 items-center gap-1 rounded-md border p-1 shadow-xs',
        props.class,
      )
    "
  >
    <slot />
  </MenubarRoot>
</template>
</file>

<file path="apps/web/src/components/ui/menubar/MenubarCheckboxItem.vue">
<script setup lang="ts">
import type { MenubarCheckboxItemEmits, MenubarCheckboxItemProps } from "reka-ui"
import type { HTMLAttributes } from "vue"
import { reactiveOmit } from "@vueuse/core"
import { Check } from "lucide-vue-next"
import {
  MenubarCheckboxItem,

  MenubarItemIndicator,
  useForwardPropsEmits,
} from "reka-ui"
import { cn } from "@/lib/utils"

const props = defineProps<MenubarCheckboxItemProps & { class?: HTMLAttributes["class"] }>()
const emits = defineEmits<MenubarCheckboxItemEmits>()

const delegatedProps = reactiveOmit(props, "class")

const forwarded = useForwardPropsEmits(delegatedProps, emits)
</script>

<template>
  <MenubarCheckboxItem
    data-slot="menubar-checkbox-item"
    v-bind="forwarded"
    :class="cn(
      'focus:bg-accent focus:text-accent-foreground relative flex cursor-default items-center gap-2 rounded-xs py-1.5 pr-2 pl-8 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*=\'size-\'])]:size-4',
      props.class,
    )"
  >
    <span class="pointer-events-none absolute left-2 flex size-3.5 items-center justify-center">
      <MenubarItemIndicator>
        <Check class="size-4" />
      </MenubarItemIndicator>
    </span>
    <slot />
  </MenubarCheckboxItem>
</template>
</file>

<file path="apps/web/src/components/ui/menubar/MenubarContent.vue">
<script setup lang="ts">
import type { MenubarContentProps } from "reka-ui"
import type { HTMLAttributes } from "vue"
import { reactiveOmit } from "@vueuse/core"
import {
  MenubarContent,

  MenubarPortal,
  useForwardProps,
} from "reka-ui"
import { cn } from "@/lib/utils"

const props = withDefaults(
  defineProps<MenubarContentProps & { class?: HTMLAttributes["class"] }>(),
  {
    align: "start",
    alignOffset: -4,
    sideOffset: 8,
  },
)

const delegatedProps = reactiveOmit(props, "class")

const forwardedProps = useForwardProps(delegatedProps)
</script>

<template>
  <MenubarPortal>
    <MenubarContent
      data-slot="menubar-content"
      v-bind="forwardedProps"
      :class="
        cn(
          'bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 z-50 min-w-[12rem] origin-(--reka-menubar-content-transform-origin) overflow-hidden rounded-md border p-1 shadow-md',
          props.class,
        )
      "
    >
      <slot />
    </MenubarContent>
  </MenubarPortal>
</template>
</file>

<file path="apps/web/src/components/ui/menubar/MenubarGroup.vue">
<script setup lang="ts">
import type { MenubarGroupProps } from "reka-ui"
import { MenubarGroup } from "reka-ui"

const props = defineProps<MenubarGroupProps>()
</script>

<template>
  <MenubarGroup
    data-slot="menubar-group"
    v-bind="props"
  >
    <slot />
  </MenubarGroup>
</template>
</file>

<file path="apps/web/src/components/ui/menubar/MenubarItem.vue">
<script setup lang="ts">
import type { MenubarItemEmits, MenubarItemProps } from "reka-ui"
import type { HTMLAttributes } from "vue"
import { reactiveOmit } from "@vueuse/core"
import {
  MenubarItem,

  useForwardPropsEmits,
} from "reka-ui"
import { cn } from "@/lib/utils"

const props = defineProps<MenubarItemProps & {
  class?: HTMLAttributes["class"]
  inset?: boolean
  variant?: "default" | "destructive"
}>()

const emits = defineEmits<MenubarItemEmits>()

const delegatedProps = reactiveOmit(props, "class", "inset", "variant")
const forwarded = useForwardPropsEmits(delegatedProps, emits)
</script>

<template>
  <MenubarItem
    data-slot="menubar-item"
    :data-inset="inset ? '' : undefined"
    :data-variant="variant"
    v-bind="forwarded"
    :class="cn(
      'focus:bg-accent focus:text-accent-foreground data-[variant=destructive]:text-destructive-foreground data-[variant=destructive]:focus:bg-destructive/10 dark:data-[variant=destructive]:focus:bg-destructive/40 data-[variant=destructive]:focus:text-destructive-foreground data-[variant=destructive]:*:[svg]:!text-destructive-foreground [&_svg:not([class*=\'text-\'])]:text-muted-foreground relative flex cursor-default items-center gap-2 rounded-sm px-2 py-1.5 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 data-[inset]:pl-8 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*=\'size-\'])]:size-4',
      props.class,
    )"
  >
    <slot />
  </MenubarItem>
</template>
</file>

<file path="apps/web/src/components/ui/menubar/MenubarLabel.vue">
<script setup lang="ts">
import type { MenubarLabelProps } from "reka-ui"
import type { HTMLAttributes } from "vue"
import { reactiveOmit } from "@vueuse/core"
import { MenubarLabel } from "reka-ui"
import { cn } from "@/lib/utils"

const props = defineProps<MenubarLabelProps & { class?: HTMLAttributes["class"], inset?: boolean }>()
const delegatedProps = reactiveOmit(props, "class", "inset")
</script>

<template>
  <MenubarLabel
    :data-inset="inset ? '' : undefined"
    v-bind="delegatedProps"
    :class="cn('px-2 py-1.5 text-sm font-medium data-[inset]:pl-8', props.class)"
  >
    <slot />
  </MenubarLabel>
</template>
</file>

<file path="apps/web/src/components/ui/menubar/MenubarMenu.vue">
<script setup lang="ts">
import type { MenubarMenuProps } from "reka-ui"
import { MenubarMenu } from "reka-ui"

const props = defineProps<MenubarMenuProps>()
</script>

<template>
  <MenubarMenu
    data-slot="menubar-menu"
    v-bind="props"
  >
    <slot />
  </MenubarMenu>
</template>
</file>

<file path="apps/web/src/components/ui/menubar/MenubarRadioGroup.vue">
<script setup lang="ts">
import type { MenubarRadioGroupEmits, MenubarRadioGroupProps } from "reka-ui"
import {
  MenubarRadioGroup,

  useForwardPropsEmits,
} from "reka-ui"

const props = defineProps<MenubarRadioGroupProps>()
const emits = defineEmits<MenubarRadioGroupEmits>()

const forwarded = useForwardPropsEmits(props, emits)
</script>

<template>
  <MenubarRadioGroup
    data-slot="menubar-radio-group"
    v-bind="forwarded"
  >
    <slot />
  </MenubarRadioGroup>
</template>
</file>

<file path="apps/web/src/components/ui/menubar/MenubarRadioItem.vue">
<script setup lang="ts">
import type { MenubarRadioItemEmits, MenubarRadioItemProps } from "reka-ui"
import type { HTMLAttributes } from "vue"
import { reactiveOmit } from "@vueuse/core"
import { Circle } from "lucide-vue-next"
import {
  MenubarItemIndicator,
  MenubarRadioItem,

  useForwardPropsEmits,
} from "reka-ui"
import { cn } from "@/lib/utils"

const props = defineProps<MenubarRadioItemProps & { class?: HTMLAttributes["class"] }>()
const emits = defineEmits<MenubarRadioItemEmits>()

const delegatedProps = reactiveOmit(props, "class")

const forwarded = useForwardPropsEmits(delegatedProps, emits)
</script>

<template>
  <MenubarRadioItem
    data-slot="menubar-radio-item"
    v-bind="forwarded"
    :class="cn(
      'focus:bg-accent focus:text-accent-foreground relative flex cursor-default items-center gap-2 rounded-xs py-1.5 pr-2 pl-8 text-sm outline-hidden select-none data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&_svg]:pointer-events-none [&_svg]:shrink-0 [&_svg:not([class*=\'size-\'])]:size-4',
      props.class,
    )"
  >
    <span class="pointer-events-none absolute left-2 flex size-3.5 items-center justify-center">
      <MenubarItemIndicator>
        <Circle class="size-2 fill-current" />
      </MenubarItemIndicator>
    </span>
    <slot />
  </MenubarRadioItem>
</template>
</file>

<file path="apps/web/src/components/ui/menubar/MenubarSeparator.vue">
<script setup lang="ts">
import type { MenubarSeparatorProps } from "reka-ui"
import type { HTMLAttributes } from "vue"
import { reactiveOmit } from "@vueuse/core"
import { MenubarSeparator, useForwardProps } from "reka-ui"
import { cn } from "@/lib/utils"

const props = defineProps<MenubarSeparatorProps & { class?: HTMLAttributes["class"] }>()

const delegatedProps = reactiveOmit(props, "class")

const forwardedProps = useForwardProps(delegatedProps)
</script>

<template>
  <MenubarSeparator
    data-slot="menubar-separator"
    :class=" cn('bg-border -mx-1 my-1 h-px', props.class)"
    v-bind="forwardedProps"
  />
</template>
</file>

<file path="apps/web/src/components/ui/menubar/MenubarShortcut.vue">
<script setup lang="ts">
import type { HTMLAttributes } from "vue"
import { cn } from "@/lib/utils"

const props = defineProps<{
  class?: HTMLAttributes["class"]
}>()
</script>

<template>
  <span
    data-slot="menubar-shortcut"
    :class="cn('text-muted-foreground ml-auto text-xs tracking-widest', props.class)"
  >
    <slot />
  </span>
</template>
</file>

<file path="apps/web/src/components/ui/menubar/MenubarSub.vue">
<script setup lang="ts">
import type { MenubarSubEmits } from "reka-ui"
import { MenubarSub, useForwardPropsEmits } from "reka-ui"

interface MenubarSubRootProps {
  defaultOpen?: boolean
  open?: boolean
}

const props = defineProps<MenubarSubRootProps>()
const emits = defineEmits<MenubarSubEmits>()

const forwarded = useForwardPropsEmits(props, emits)
</script>

<template>
  <MenubarSub
    data-slot="menubar-sub"
    v-bind="forwarded"
  >
    <slot />
  </MenubarSub>
</template>
</file>

<file path="apps/web/src/components/ui/menubar/MenubarSubContent.vue">
<script setup lang="ts">
import type { MenubarSubContentEmits, MenubarSubContentProps } from "reka-ui"
import type { HTMLAttributes } from "vue"
import { reactiveOmit } from "@vueuse/core"
import {
  MenubarPortal,
  MenubarSubContent,

  useForwardPropsEmits,
} from "reka-ui"
import { cn } from "@/lib/utils"

const props = defineProps<MenubarSubContentProps & { class?: HTMLAttributes["class"] }>()
const emits = defineEmits<MenubarSubContentEmits>()

const delegatedProps = reactiveOmit(props, "class")

const forwarded = useForwardPropsEmits(delegatedProps, emits)
</script>

<template>
  <MenubarPortal>
    <MenubarSubContent
      data-slot="menubar-sub-content"
      v-bind="forwarded"
      :class="
        cn(
          'bg-popover text-popover-foreground data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 z-50 min-w-[8rem] origin-(--reka-menubar-content-transform-origin) overflow-hidden rounded-md border p-1 shadow-lg',
          props.class,
        )
      "
    >
      <slot />
    </MenubarSubContent>
  </MenubarPortal>
</template>
</file>

<file path="apps/web/src/components/ui/menubar/MenubarSubTrigger.vue">
<script setup lang="ts">
import type { MenubarSubTriggerProps } from "reka-ui"
import type { HTMLAttributes } from "vue"
import { reactiveOmit } from "@vueuse/core"
import { ChevronRight } from "lucide-vue-next"
import { MenubarSubTrigger, useForwardProps } from "reka-ui"
import { cn } from "@/lib/utils"

const props = defineProps<MenubarSubTriggerProps & { class?: HTMLAttributes["class"], inset?: boolean }>()

const delegatedProps = reactiveOmit(props, "class", "inset")
const forwardedProps = useForwardProps(delegatedProps)
</script>

<template>
  <MenubarSubTrigger
    data-slot="menubar-sub-trigger"
    :data-inset="inset ? '' : undefined"
    v-bind="forwardedProps"
    :class="cn(
      'focus:bg-accent focus:text-accent-foreground data-[state=open]:bg-accent data-[state=open]:text-accent-foreground flex cursor-default items-center rounded-sm px-2 py-1.5 text-sm outline-none select-none data-[inset]:pl-8',
      props.class,
    )"
  >
    <slot />
    <ChevronRight class="ml-auto size-4" />
  </MenubarSubTrigger>
</template>
</file>

<file path="apps/web/src/components/ui/menubar/MenubarTrigger.vue">
<script setup lang="ts">
import type { MenubarTriggerProps } from "reka-ui"
import type { HTMLAttributes } from "vue"
import { reactiveOmit } from "@vueuse/core"
import { MenubarTrigger, useForwardProps } from "reka-ui"
import { cn } from "@/lib/utils"

const props = defineProps<MenubarTriggerProps & { class?: HTMLAttributes["class"] }>()

const delegatedProps = reactiveOmit(props, "class")

const forwardedProps = useForwardProps(delegatedProps)
</script>

<template>
  <MenubarTrigger
    data-slot="menubar-trigger"
    v-bind="forwardedProps"
    :class="
      cn(
        'focus:bg-accent focus:text-accent-foreground data-[state=open]:bg-accent data-[state=open]:text-accent-foreground flex items-center rounded-sm px-2 py-1 text-sm font-medium outline-hidden select-none',
        props.class,
      )
    "
  >
    <slot />
  </MenubarTrigger>
</template>
</file>

<file path="apps/web/src/features/login/routes.ts">
import type { RouteRecordRaw } from "vue-router";

// Lazy-load the view components for better performance
const LoginView = () => import("./views/LoginView.vue");

export const loginRoutes: RouteRecordRaw[] = [
  {
    path: "/login",
    name: "Login",
    component: LoginView,
  },
];
</file>

<file path="apps/web/src/lib/utils.ts">
import { type ClassValue, clsx } from "clsx";
import { twMerge } from "tailwind-merge";

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs));
}
</file>

<file path="apps/web/src/types/api.ts">
// Base DTOs
export interface IdentifierDto {
  value: string;
  type: string;
}

export interface EntityDto {
  id: string; // UUID is a string in TS
  created_at: string; // ISO date string
  metadata?: Record<string, string> | null;
}

export interface FactDto {
  name: string;
  type: string;
  fact_id?: string | null;
}

export interface SourceDto {
  id: string; // UUID
  content: string;
  timestamp: string; // ISO date string
}

export interface HasFactDto {
  verb: string;
  confidence_score: number;
  created_at: string; // ISO date string
}

// Request Payloads
export interface AssimilateKnowledgeRequest {
  identifier: IdentifierDto;
  content: string;
  timestamp?: string | null; // ISO date string
  history?: string[] | null;
}

// API Responses
export interface AssimilatedFactDto {
  fact: FactDto;
  relationship: HasFactDto;
}

export interface AssimilateKnowledgeResponse {
  entity: EntityDto;
  source: SourceDto;
  assimilated_facts: AssimilatedFactDto[];
}

export interface HasIdentifierDto {
  is_primary: boolean;
  created_at: string; // ISO date string
}

export interface IdentifierWithRelationshipDto {
  identifier: IdentifierDto;
  relationship: HasIdentifierDto;
}

export interface FactWithSourceDto {
  fact: FactDto;
  relationship: HasFactDto;
  source?: SourceDto | null;
}

export interface GetEntityResponse {
  entity: EntityDto;
  identifier: IdentifierWithRelationshipDto;
  facts: FactWithSourceDto[];
}
</file>

<file path="apps/web/.gitignore">
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*

node_modules
dist
dist-ssr
*.local

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?
</file>

<file path="apps/web/components.json">
{
  "$schema": "https://shadcn-vue.com/schema.json",
  "style": "new-york",
  "typescript": true,
  "tailwind": {
    "config": "",
    "css": "src/style.css",
    "baseColor": "neutral",
    "cssVariables": true,
    "prefix": ""
  },
  "iconLibrary": "lucide",
  "aliases": {
    "components": "@/components",
    "utils": "@/lib/utils",
    "ui": "@/components/ui",
    "lib": "@/lib",
    "composables": "@/composables"
  },
  "registries": {}
}
</file>

<file path="apps/web/README.md">
# Vue 3 + TypeScript + Vite

This template should help get you started developing with Vue 3 and TypeScript in Vite. The template uses Vue 3 `<script setup>` SFCs, check out the [script setup docs](https://v3.vuejs.org/api/sfc-script-setup.html#sfc-script-setup) to learn more.

Learn more about the recommended Project Setup and IDE Support in the [Vue Docs TypeScript Guide](https://vuejs.org/guide/typescript/overview.html#project-setup).
</file>

<file path="apps/web/tsconfig.node.json">
{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.node.tsbuildinfo",
    "target": "ES2023",
    "lib": ["ES2023"],
    "module": "ESNext",
    "types": ["node"],
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "moduleDetection": "force",
    "noEmit": true,

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["vite.config.ts"]
}
</file>

<file path="package.json">
{
  "name": "nous-monorepo",
  "private": true,
  "packageManager": "pnpm@9.6.0",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/jwandekoken/nous.git"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "bugs": {
    "url": "https://github.com/jwandekoken/nous/issues"
  },
  "homepage": "https://github.com/jwandekoken/nous#readme",
  "devDependencies": {
    "turbo": "^2.5.8"
  }
}
</file>

<file path="pnpm-workspace.yaml">
packages:
  # all packages in subdirs of apps/
  - "apps/*"
</file>

<file path="pyrightconfig.json">
{
  "include": ["apps/api"],
  "executionEnvironments": [
    {
      "root": "apps/api",
      "pythonVersion": "3.12"
    }
  ]
}
</file>

<file path="test_delete_api_key.py">
#!/usr/bin/env python3
"""Test script to verify API key deletion functionality."""

import asyncio
import uuid

from app.db.postgres.auth_session import get_auth_db_session
from app.features.auth.models import ApiKey
from app.features.auth.usecases.delete_api_key_usecase import DeleteApiKeyUseCaseImpl
from sqlalchemy import select


async def test_delete_api_key():
    """Test the delete API key functionality."""
    # Create a test API key first
    test_tenant_id = uuid.uuid4()

    async with get_auth_db_session() as session:
        # Create a test API key
        test_api_key = ApiKey(
            name="test_key",
            key_prefix="test123",
            hashed_key="hashed_test_key",
            tenant_id=test_tenant_id,
        )
        session.add(test_api_key)
        await session.commit()
        await session.refresh(test_api_key)

        api_key_id = str(test_api_key.id)
        print(f"Created test API key with ID: {api_key_id}")

        # Verify it exists
        result = await session.execute(
            select(ApiKey).where(ApiKey.id == test_api_key.id)
        )
        existing_key = result.scalar_one_or_none()
        print(f"API key exists before deletion: {existing_key is not None}")

    # Now try to delete it using the use case
    use_case = DeleteApiKeyUseCaseImpl(get_db_session=get_auth_db_session)

    try:
        result = await use_case.execute(api_key_id, test_tenant_id)
        print(f"Delete result: {result}")
    except Exception as e:
        print(f"Delete failed with error: {e}")

    # Verify it was actually deleted
    async with get_auth_db_session() as session:
        result = await session.execute(
            select(ApiKey).where(ApiKey.id == test_api_key.id)
        )
        deleted_key = result.scalar_one_or_none()
        print(f"API key exists after deletion: {deleted_key is not None}")

        # Clean up
        if deleted_key:
            session.delete(deleted_key)
            await session.commit()
            print("Cleaned up remaining test API key")


if __name__ == "__main__":
    asyncio.run(test_delete_api_key())
</file>

<file path="apps/api/app/core/authentication.py">
"""Authentication utilities for JWT token validation and user retrieval."""

from datetime import UTC, datetime, timedelta
from uuid import UUID

from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer
from jose import JWTError, jwt
from passlib.context import CryptContext

from app.core.schemas import AuthenticatedUser, UserRole
from app.core.settings import get_settings

# Password hashing - using argon2 as bcrypt has issues.
pwd_context = CryptContext(schemes=["argon2"], deprecated="auto")

security = HTTPBearer()


def verify_token(token: str) -> dict[str, str | int]:
    """Verify and decode a JWT token."""
    settings = get_settings()

    try:
        payload = jwt.decode(
            token, settings.secret_key, algorithms=[settings.algorithm]
        )
        return payload
    except JWTError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Could not validate credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )


def verify_password(plain_password: str, hashed_password: str) -> bool:
    """Verify a plain password against its hash."""
    return pwd_context.verify(plain_password, hashed_password)


def get_password_hash(password: str) -> str:
    """Hash a password."""
    return pwd_context.hash(password)


def create_access_token(
    data: dict[str, str | UUID | datetime], expires_delta: timedelta | None = None
) -> str:
    """Create a JWT access token."""
    settings = get_settings()
    to_encode = data.copy()

    if expires_delta:
        expire = datetime.now(UTC) + expires_delta
    else:
        expire = datetime.now(UTC) + timedelta(
            minutes=settings.access_token_expire_minutes
        )

    to_encode["exp"] = expire
    encoded_jwt = jwt.encode(
        to_encode, settings.secret_key, algorithm=settings.algorithm
    )
    return encoded_jwt


async def get_current_user(
    credentials: HTTPAuthorizationCredentials = Depends(security),
) -> AuthenticatedUser:
    """Get current user from JWT token with tenant information."""
    token = credentials.credentials
    payload = verify_token(token)

    user_id = payload.get("sub")
    tenant_id = payload.get("tenant_id")  # This can be None
    role_str = payload.get("role")

    if user_id is None or role_str is None:  # tenant_id can be None, but role cannot
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Token missing required user or role information",
            headers={"WWW-Authenticate": "Bearer"},
        )

    try:
        return AuthenticatedUser(
            user_id=UUID(str(user_id)),
            tenant_id=UUID(str(tenant_id)) if tenant_id else None,
            role=UserRole(role_str),  # <-- Validate and cast role
        )
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid user, tenant, or role format in token",
            headers={"WWW-Authenticate": "Bearer"},
        )
</file>

<file path="apps/api/app/core/authorization.py">
from collections.abc import Awaitable
from datetime import UTC, datetime
from typing import Callable
from uuid import UUID

from fastapi import Depends, HTTPException, status
from fastapi.security import APIKeyHeader
from pydantic import BaseModel
from sqlalchemy import and_, or_, select

from app.core.authentication import (
    AuthenticatedUser,
    get_current_user,
    pwd_context,
)
from app.db.postgres.auth_session import get_auth_db_session
from app.features.auth.models import ApiKey, Tenant, UserRole


# A dependency factory
def require_roles(
    allowed_roles: list[UserRole],
) -> Callable[..., Awaitable[AuthenticatedUser]]:
    """
    Factory for creating a dependency that checks if a user has one of
    the allowed roles.
    """

    async def role_checker(
        user: AuthenticatedUser = Depends(get_current_user),
    ) -> AuthenticatedUser:
        """
        The actual dependency that checks the user's role.
        """
        if user.role not in allowed_roles:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Insufficient permissions",
            )
        return user

    return role_checker


# --- Create your specific role dependencies ---
is_super_admin = require_roles([UserRole.SUPER_ADMIN])
is_tenant_admin = require_roles([UserRole.TENANT_ADMIN])
is_admin = require_roles([UserRole.SUPER_ADMIN, UserRole.TENANT_ADMIN])
is_any_tenant_user = require_roles([UserRole.TENANT_ADMIN, UserRole.TENANT_USER])


# --- Tenant Info and Authentication Dependencies ---
class TenantInfo(BaseModel):
    """Tenant information including graph name."""

    tenant_id: UUID
    graph_name: str


# API Key security scheme
api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)


async def get_tenant_from_api_key(
    key: str | None = Depends(api_key_header),
) -> TenantInfo | None:
    """Get tenant info from API key authentication."""
    if not key or "." not in key:
        return None

    prefix, _ = key.split(".", 1)

    async with get_auth_db_session() as session:
        result = await session.execute(
            select(ApiKey).where(
                and_(
                    ApiKey.key_prefix == prefix,
                    or_(
                        ApiKey.expires_at.is_(None),
                        ApiKey.expires_at > datetime.now(UTC),
                    ),
                )
            )
        )
        api_keys = result.scalars().all()

        found_key: ApiKey | None = None
        for api_key_record in api_keys:
            if pwd_context.verify(key, api_key_record.hashed_key):
                found_key = api_key_record
                break

        if not found_key:
            return None

        tenant = await session.get(Tenant, found_key.tenant_id)
        if not tenant:
            return None

        found_key.last_used_at = datetime.now(UTC)
        await session.commit()

        return TenantInfo(tenant_id=tenant.id, graph_name=tenant.age_graph_name)


async def get_tenant_from_jwt(
    user: AuthenticatedUser = Depends(get_current_user),
) -> TenantInfo | None:
    """Get tenant info from JWT authentication."""
    if not user or not user.tenant_id:
        return None

    async with get_auth_db_session() as session:
        tenant = await session.get(Tenant, user.tenant_id)
        if not tenant:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Tenant not found for authenticated user",
            )
        return TenantInfo(tenant_id=user.tenant_id, graph_name=tenant.age_graph_name)


async def get_tenant_info(
    jwt_tenant: TenantInfo | None = Depends(get_tenant_from_jwt),
    api_key_tenant: TenantInfo | None = Depends(get_tenant_from_api_key),
) -> TenantInfo:
    """Master dependency that resolves tenant info from either JWT or API key.

    Prioritizes JWT over API key authentication.
    """
    if jwt_tenant:
        return jwt_tenant

    if api_key_tenant:
        return api_key_tenant

    # Neither authentication method provided or valid
    raise HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Authentication required",
        headers={"WWW-Authenticate": "Bearer"},
    )
</file>

<file path="apps/api/app/db/postgres/auth_session.py">
"""SQLAlchemy database session management for auth operations.

This module provides SQLAlchemy ORM sessions for authentication and authorization operations.
Used primarily by the auth features for user management, tenant operations, and API key management.
"""

from collections.abc import AsyncGenerator
from contextlib import asynccontextmanager

from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine
from sqlalchemy.orm import DeclarativeBase

from app.core.settings import get_settings


class Base(DeclarativeBase):
    """Base class for all database models."""

    pass


# Global session factory
_async_session_maker: async_sessionmaker[AsyncSession] | None = None


def init_auth_db_session() -> None:
    """Initialize the database session factory."""
    global _async_session_maker

    if _async_session_maker is not None:
        return  # Already initialized

    settings = get_settings()

    # Create async engine
    engine = create_async_engine(
        str(settings.database_url),
        echo=settings.debug,
        pool_pre_ping=True,
    )

    # Create session factory
    _async_session_maker = async_sessionmaker(
        bind=engine,
        class_=AsyncSession,
        expire_on_commit=False,
    )


@asynccontextmanager
async def get_auth_db_session() -> AsyncGenerator[AsyncSession, None]:
    """Get a database session."""
    if _async_session_maker is None:
        init_auth_db_session()

    if _async_session_maker is None:
        raise RuntimeError("Failed to initialize database session")

    async with _async_session_maker() as session:
        try:
            yield session
        finally:
            await session.close()
</file>

<file path="apps/api/app/db/postgres/graph_connection.py">
"""PostgreSQL database connection management for graph operations.

This module provides direct asyncpg connections for AGE graph operations and raw SQL queries.
Used primarily by the graph features for Cypher queries and AGE-specific operations.
"""

import asyncpg

from app.core.settings import get_settings

_pool: asyncpg.Pool | None = None


async def get_graph_db_pool() -> asyncpg.Pool:
    """Get the database connection pool as a dependency."""
    global _pool
    if _pool is None:
        settings = get_settings()
        _pool = await asyncpg.create_pool(
            user=settings.postgres_user,
            password=settings.postgres_password,
            host=settings.postgres_host,
            port=settings.postgres_port,
            database=settings.postgres_db,
            min_size=5,
            max_size=20,
        )

    return _pool


async def close_graph_db_pool() -> None:
    """Close the database connection pool."""
    global _pool
    if _pool:
        await _pool.close()
        _pool = None


async def reset_db_pool() -> None:
    """Reset the database connection pool for testing purposes."""
    global _pool
    if _pool:
        try:
            await _pool.close()
        except Exception:
            pass  # Ignore errors during reset
        _pool = None
</file>

<file path="apps/api/app/features/graph/repositories/age_repository.py">
"""PostgreSQL AGE implementation of the graph repository protocol."""

import json
from datetime import datetime
from typing import Any, cast, override
from uuid import UUID

import asyncpg

from app.features.graph.models import (
    DerivedFrom,
    Entity,
    Fact,
    HasFact,
    HasIdentifier,
    Identifier,
    Source,
)
from app.features.graph.repositories.base import GraphRepository
from app.features.graph.repositories.types import (
    AddFactToEntityResult,
    CreateEntityResult,
    FactWithOptionalSource,
    FactWithSource,
    FindEntityByIdResult,
    FindEntityResult,
    IdentifierWithRelationship,
)


class AgeRepository(GraphRepository):
    """PostgreSQL AGE implementation of the graph repository."""

    pool: asyncpg.Pool
    graph_name: str

    def __init__(self, pool: asyncpg.Pool, graph_name: str):
        """Initialize the repository with a database connection pool and graph name."""
        self.pool = pool
        self.graph_name = graph_name
        if not graph_name:
            raise ValueError("graph_name must be provided")

    @staticmethod
    def _escape_cypher_string(value: str) -> str:
        """Escape single quotes for use in Cypher string literals."""
        return value.replace("'", "\\'")

    @staticmethod
    def _clean_agtype_string(agtype_str: str) -> str:
        """Clean AGE agtype string by removing type annotations like ::vertex and ::edge."""
        import re

        # Remove ::vertex and ::edge annotations
        return re.sub(r"::(vertex|edge)", "", agtype_str)

    async def _setup_age_connection(self, conn: asyncpg.Connection) -> None:
        """Setup AGE extension and search path for a connection."""
        _ = await conn.execute("LOAD 'age';")
        _ = await conn.execute("SET search_path = ag_catalog, '$user', public;")

    async def _execute_cypher(
        self,
        cypher_query: str,
        as_clause: str,
        fetch_mode: str = "row",
    ) -> asyncpg.Record | list[asyncpg.Record] | str | None:
        """
        Execute a Cypher query by wrapping it in the necessary SQL.

        Args:
            cypher_query: The raw Cypher query string.
            as_clause: The complete AS clause string, e.g., "as (result agtype)".
            fetch_mode: "row" for fetchrow, "all" for fetch, "none" for execute.

        Returns:
            Query result based on fetch_mode.
        """
        if not as_clause.strip().lower().startswith("as"):
            raise ValueError("The 'as_clause' must start with 'AS'.")

        # Build the complete AGE SQL query using the provided as_clause
        query = f"""
            SELECT * FROM cypher('{self.graph_name}', $${cypher_query}$$)
            {as_clause};
        """

        async with self.pool.acquire() as conn:
            conn = cast(asyncpg.Connection, conn)

            async with conn.transaction():
                await self._setup_age_connection(conn)

                if fetch_mode == "row":
                    return await conn.fetchrow(query)
                elif fetch_mode == "all":
                    return await conn.fetch(query)
                else:  # "none"
                    return await conn.execute(query)

    @override
    async def create_entity(
        self, entity: Entity, identifier: Identifier, relationship: HasIdentifier
    ) -> CreateEntityResult:
        """
        Creates a new entity with an identifier using an idempotent approach.

        This method first checks if an entity already exists for the given identifier.
        If it exists, returns the existing entity. If not, creates a new one.
        """

        # Check if entity already exists for this identifier
        existing_entity = await self.find_entity_by_identifier(
            identifier.value, identifier.type
        )

        if existing_entity is not None:
            # Return existing entity with its identifier and relationship
            return {
                "entity": existing_entity["entity"],
                "identifier": existing_entity["identifier"]["identifier"],
                "relationship": existing_entity["identifier"]["relationship"],
            }

        # Entity doesn't exist, create it
        # Convert Python boolean to Cypher boolean (lowercase)
        is_primary_str = str(relationship.is_primary).lower()

        # Prepare metadata JSON - use it directly as agtype without extra escaping
        metadata_json = json.dumps(entity.metadata or {}).replace(
            "'", "''"
        )  # Double single quotes for SQL

        # Build the Cypher query with embedded parameters
        # AGE cypher function expects the query as a dollar-quoted string
        # We use MERGE for idempotency - it will find or create
        # Note: AGE doesn't support ON CREATE SET, so we include all properties in MERGE
        cypher_query = f"""
        MERGE (i:Identifier {{value: '{self._escape_cypher_string(identifier.value)}', type: '{self._escape_cypher_string(identifier.type)}'}})
        MERGE (e:Entity {{id: '{entity.id}', created_at: '{entity.created_at.isoformat()}', metadata: '{metadata_json}'::agtype}})
        MERGE (e)-[r:HAS_IDENTIFIER {{is_primary: {is_primary_str}, created_at: '{relationship.created_at.isoformat()}'}}]->(i)
        RETURN {{
            entity: e,
            identifier: i,
            relationship: r
        }} AS result
        """

        # Execute the query using the new helper method
        record = await self._execute_cypher(
            cypher_query=cypher_query,
            as_clause="as (result agtype)",
            fetch_mode="row",
        )

        if not record:
            raise RuntimeError(
                "Failed to create entity, the query returned no results."
            )

        # Extract the result string from the agtype, clean it, and parse it as JSON
        record = cast(asyncpg.Record, record)
        result_str = cast(str, record["result"])

        cleaned_result_str = self._clean_agtype_string(result_str)
        result_map = cast(dict[str, Any], json.loads(cleaned_result_str))

        # Extract properties from the agtype objects
        entity_props = cast(dict[str, Any], result_map["entity"]["properties"])
        identifier_props = cast(dict[str, Any], result_map["identifier"]["properties"])
        relationship_props = cast(
            dict[str, Any], result_map["relationship"]["properties"]
        )

        created_entity = Entity(
            id=UUID(entity_props["id"]),
            created_at=datetime.fromisoformat(entity_props["created_at"]),
            metadata=json.loads(entity_props["metadata"])
            if entity_props["metadata"]
            else {},
        )

        created_identifier = Identifier(
            value=identifier_props["value"],
            type=identifier_props["type"],
        )

        created_relationship = HasIdentifier(
            from_entity_id=created_entity.id,
            to_identifier_value=created_identifier.value,
            is_primary=relationship_props["is_primary"],
            created_at=datetime.fromisoformat(relationship_props["created_at"]),
        )

        return {
            "entity": created_entity,
            "identifier": created_identifier,
            "relationship": created_relationship,
        }

    @override
    async def find_entity_by_identifier(
        self, identifier_value: str, identifier_type: str
    ) -> FindEntityResult | None:
        """Find an entity by its identifier."""
        cypher_query = f"""
        MATCH (e:Entity)-[r:HAS_IDENTIFIER]->(i:Identifier {{
            value: '{self._escape_cypher_string(identifier_value)}',
            type: '{self._escape_cypher_string(identifier_type)}'
        }})
        OPTIONAL MATCH (e)-[hf:HAS_FACT]->(f:Fact)
        OPTIONAL MATCH (f)-[df:DERIVED_FROM]->(s:Source)
        RETURN collect(DISTINCT {{
            entity: e,
            identifier: i,
            relationship: r,
            fact: f,
            source: s,
            fact_relationship: hf
        }}) AS result
        """

        record = await self._execute_cypher(
            cypher_query=cypher_query,
            as_clause="as (result agtype)",
            fetch_mode="row",
        )

        if not record:
            return None

        record = cast(asyncpg.Record, record)
        result_str = cast(str, record["result"])
        cleaned_result_str = self._clean_agtype_string(result_str)
        results_list = cast(list[dict[str, Any]], json.loads(cleaned_result_str))
        if not results_list:
            return None

        # The first result contains the entity and identifier info
        first_result = results_list[0]

        # Extract entity
        entity_props = cast(dict[str, Any], first_result["entity"]["properties"])
        entity = Entity(
            id=UUID(entity_props["id"]),
            created_at=datetime.fromisoformat(entity_props["created_at"]),
            metadata=json.loads(entity_props["metadata"])
            if entity_props["metadata"]
            else {},
        )

        # Extract identifier and relationship
        identifier_props = cast(
            dict[str, Any], first_result["identifier"]["properties"]
        )
        relationship_props = cast(
            dict[str, Any], first_result["relationship"]["properties"]
        )

        identifier = Identifier(
            value=identifier_props["value"],
            type=identifier_props["type"],
        )

        has_identifier_rel = HasIdentifier(
            from_entity_id=entity.id,
            to_identifier_value=identifier.value,
            is_primary=relationship_props["is_primary"],
            created_at=datetime.fromisoformat(relationship_props["created_at"]),
        )

        identifier_with_rel: IdentifierWithRelationship = {
            "identifier": identifier,
            "relationship": has_identifier_rel,
        }

        # Build facts with sources from all results
        facts_with_sources: list[FactWithSource] = []

        for result_item in results_list:
            fact_data = result_item.get("fact")
            if not fact_data:  # Skip if no fact
                continue

            fact_props = cast(dict[str, Any], fact_data["properties"])
            fact = Fact(
                name=fact_props["name"],
                type=fact_props["type"],
            )

            # Verify fact_id matches (should be computed by model validator)
            if fact.fact_id != fact_props["fact_id"]:
                continue

            # At this point we know fact.fact_id is not None
            assert fact.fact_id is not None

            source = None
            source_data = result_item.get("source")
            if source_data:
                source_props = cast(dict[str, Any], source_data["properties"])
                source = Source(
                    id=UUID(source_props["id"]),
                    content=source_props["content"],
                    timestamp=datetime.fromisoformat(source_props["timestamp"]),
                )

            fact_rel_props = cast(
                dict[str, Any], result_item["fact_relationship"]["properties"]
            )
            has_fact_rel = HasFact(
                from_entity_id=entity.id,
                to_fact_id=fact.fact_id,
                verb=fact_rel_props["verb"],
                confidence_score=fact_rel_props["confidence_score"],
                created_at=datetime.fromisoformat(fact_rel_props["created_at"]),
            )

            fact_with_source: FactWithSource = {
                "fact": fact,
                "source": source,
                "relationship": has_fact_rel,
            }
            facts_with_sources.append(fact_with_source)

        return {
            "entity": entity,
            "identifier": identifier_with_rel,
            "facts_with_sources": facts_with_sources,
        }

    @override
    async def find_entity_by_id(self, entity_id: str) -> FindEntityByIdResult | None:
        """Find an entity by its ID."""
        cypher_query = f"""
        MATCH (e:Entity {{id: '{entity_id}'}})
        OPTIONAL MATCH (e)-[r:HAS_IDENTIFIER]->(i:Identifier)
        OPTIONAL MATCH (e)-[hf:HAS_FACT]->(f:Fact)
        OPTIONAL MATCH (f)-[df:DERIVED_FROM]->(s:Source)
        RETURN collect(DISTINCT {{
            entity: e,
            identifier: i,
            relationship: r,
            fact: f,
            source: s,
            fact_relationship: hf
        }}) AS result
        """

        record = await self._execute_cypher(
            cypher_query=cypher_query,
            as_clause="as (result agtype)",
            fetch_mode="row",
        )

        if not record:
            return None

        record = cast(asyncpg.Record, record)
        result_str = cast(str, record["result"])
        cleaned_result_str = self._clean_agtype_string(result_str)
        results_list = cast(list[dict[str, Any]], json.loads(cleaned_result_str))

        if not results_list:
            return None

        # The first result contains the entity info
        first_result = results_list[0]

        # Extract entity
        entity_props = cast(dict[str, Any], first_result["entity"]["properties"])
        entity = Entity(
            id=UUID(entity_props["id"]),
            created_at=datetime.fromisoformat(entity_props["created_at"]),
            metadata=json.loads(entity_props["metadata"])
            if entity_props["metadata"]
            else {},
        )

        # Find the primary identifier (or first one if no primary exists)
        identifier_with_rel: IdentifierWithRelationship | None = None

        for result_item in results_list:
            identifier_data = result_item.get("identifier")
            relationship_data = result_item.get("relationship")

            if identifier_data and relationship_data:
                identifier_props = cast(dict[str, Any], identifier_data["properties"])
                relationship_props = cast(
                    dict[str, Any], relationship_data["properties"]
                )

                identifier = Identifier(
                    value=identifier_props["value"],
                    type=identifier_props["type"],
                )

                has_identifier_rel = HasIdentifier(
                    from_entity_id=entity.id,
                    to_identifier_value=identifier.value,
                    is_primary=relationship_props["is_primary"],
                    created_at=datetime.fromisoformat(relationship_props["created_at"]),
                )

                # Prefer primary identifier, but take the first one if none is primary
                if identifier_with_rel is None or relationship_props["is_primary"]:
                    identifier_with_rel = {
                        "identifier": identifier,
                        "relationship": has_identifier_rel,
                    }

                    # If this is primary, we can stop looking
                    if relationship_props["is_primary"]:
                        break

        # Build facts with sources from all results
        facts_with_sources: list[FactWithSource] = []

        for result_item in results_list:
            fact_data = result_item.get("fact")
            if not fact_data:  # Skip if no fact
                continue

            fact_props = cast(dict[str, Any], fact_data["properties"])
            fact = Fact(
                name=fact_props["name"],
                type=fact_props["type"],
            )

            # Verify fact_id matches (should be computed by model validator)
            if fact.fact_id != fact_props["fact_id"]:
                continue

            # At this point we know fact.fact_id is not None
            assert fact.fact_id is not None

            source = None
            source_data = result_item.get("source")
            if source_data:
                source_props = cast(dict[str, Any], source_data["properties"])
                source = Source(
                    id=UUID(source_props["id"]),
                    content=source_props["content"],
                    timestamp=datetime.fromisoformat(source_props["timestamp"]),
                )

            fact_rel_props = cast(
                dict[str, Any], result_item["fact_relationship"]["properties"]
            )
            has_fact_rel = HasFact(
                from_entity_id=entity.id,
                to_fact_id=fact.fact_id,
                verb=fact_rel_props["verb"],
                confidence_score=fact_rel_props["confidence_score"],
                created_at=datetime.fromisoformat(fact_rel_props["created_at"]),
            )

            fact_with_source: FactWithSource = {
                "fact": fact,
                "source": source,
                "relationship": has_fact_rel,
            }
            facts_with_sources.append(fact_with_source)

        return {
            "entity": entity,
            "identifier": identifier_with_rel,
            "facts_with_sources": facts_with_sources,
        }

    @override
    async def delete_entity_by_id(self, entity_id: str) -> bool:
        """Delete an entity by its ID."""
        # First check if entity exists
        entity_check = await self.find_entity_by_id(entity_id)
        if entity_check is None:
            return False

        # Get facts connected to this entity before deletion
        entity_data = entity_check

        # For each fact connected to this entity, check if it's used by other entities
        facts_to_delete = []
        for fact_data in entity_data["facts_with_sources"]:
            fact_id = fact_data["fact"].fact_id
            assert fact_id is not None

            # Check if this fact is used by other entities
            check_query = f"""
            MATCH (f:Fact {{fact_id: '{fact_id}'}})
            MATCH (e:Entity)-[:HAS_FACT]->(f)
            RETURN count(e) AS usage_count
            """

            record = await self._execute_cypher(
                cypher_query=check_query,
                as_clause="as (usage_count agtype)",
                fetch_mode="row",
            )

            if record:
                record = cast(asyncpg.Record, record)
                usage_count_str = cast(str, record["usage_count"])
                usage_count = int(usage_count_str)

                # If only used by this entity (usage_count == 1), mark for deletion
                if usage_count == 1:
                    facts_to_delete.append(fact_id)

        # Now perform the cascading delete
        # 1. Delete HAS_FACT relationships for this entity
        # 2. Delete HAS_IDENTIFIER relationships for this entity
        # 3. Delete facts that are only used by this entity
        # 4. Delete sources that are no longer referenced
        # 5. Delete identifiers that are no longer referenced
        # 6. Delete the entity itself

        # Delete HAS_FACT relationships for this entity
        delete_has_fact_query = f"""
        MATCH (e:Entity {{id: '{entity_id}'}})-[hf:HAS_FACT]->(f:Fact)
        DETACH DELETE hf
        RETURN count(hf) AS deleted_count
        """

        await self._execute_cypher(
            cypher_query=delete_has_fact_query,
            as_clause="as (deleted_count agtype)",
            fetch_mode="row",
        )

        # Delete HAS_IDENTIFIER relationships for this entity
        delete_has_identifier_query = f"""
        MATCH (e:Entity {{id: '{entity_id}'}})-[hi:HAS_IDENTIFIER]->(i:Identifier)
        DETACH DELETE hi
        RETURN count(hi) AS deleted_count
        """

        await self._execute_cypher(
            cypher_query=delete_has_identifier_query,
            as_clause="as (deleted_count agtype)",
            fetch_mode="row",
        )

        # Delete the entity itself
        delete_entity_query = f"""
        MATCH (e:Entity {{id: '{entity_id}'}})
        DETACH DELETE e
        RETURN true AS entity_deleted
        """

        record = await self._execute_cypher(
            cypher_query=delete_entity_query,
            as_clause="as (entity_deleted agtype)",
            fetch_mode="row",
        )

        if not record:
            raise RuntimeError(f"Failed to delete entity '{entity_id}'")

        # Now delete facts that were only used by this entity
        for fact_id in facts_to_delete:
            # Get the source ID before deleting the fact
            source_query = f"""
            MATCH (f:Fact {{fact_id: '{fact_id}'}})
            OPTIONAL MATCH (f)-[:DERIVED_FROM]->(s:Source)
            RETURN s.id AS source_id
            """

            source_record = await self._execute_cypher(
                cypher_query=source_query,
                as_clause="as (source_id agtype)",
                fetch_mode="row",
            )

            # Delete the fact
            delete_fact_query = f"""
            MATCH (f:Fact {{fact_id: '{fact_id}'}})
            DETACH DELETE f
            RETURN true AS fact_deleted
            """

            await self._execute_cypher(
                cypher_query=delete_fact_query,
                as_clause="as (fact_deleted agtype)",
                fetch_mode="row",
            )

            # Check if source should be deleted (no longer referenced by any facts)
            if source_record:
                source_record = cast(asyncpg.Record, source_record)
                source_id_str = cast(str, source_record["source_id"])

                if source_id_str != "null":
                    # Check if source is still used by other facts
                    check_source_usage = f"""
                    MATCH (s:Source {{id: '{source_id_str}'}})
                    OPTIONAL MATCH (f:Fact)-[:DERIVED_FROM]->(s)
                    RETURN count(f) AS usage_count
                    """

                    usage_record = await self._execute_cypher(
                        cypher_query=check_source_usage,
                        as_clause="as (usage_count agtype)",
                        fetch_mode="row",
                    )

                    if usage_record:
                        usage_record = cast(asyncpg.Record, usage_record)
                        usage_count_str = cast(str, usage_record["usage_count"])
                        usage_count = int(usage_count_str)

                        # Delete source if no longer used
                        if usage_count == 0:
                            delete_source_query = f"""
                            MATCH (s:Source {{id: '{source_id_str}'}})
                            DETACH DELETE s
                            RETURN true AS source_deleted
                            """

                            await self._execute_cypher(
                                cypher_query=delete_source_query,
                                as_clause="as (source_deleted agtype)",
                                fetch_mode="row",
                            )

        # Check and delete identifiers that are no longer used
        if entity_data["identifier"]:
            identifier_value = entity_data["identifier"]["identifier"].value
            identifier_type = entity_data["identifier"]["identifier"].type

            # Check if identifier is still used by other entities
            # Note: We need to check after all relationships are deleted
            check_identifier_query = f"""
            MATCH (i:Identifier {{value: '{self._escape_cypher_string(identifier_value)}', type: '{self._escape_cypher_string(identifier_type)}'}})
            OPTIONAL MATCH (e:Entity)-[:HAS_IDENTIFIER]->(i)
            RETURN count(e) AS identifier_usage_count
            """

            record = await self._execute_cypher(
                cypher_query=check_identifier_query,
                as_clause="as (identifier_usage_count agtype)",
                fetch_mode="row",
            )

            if record:
                record = cast(asyncpg.Record, record)
                usage_count_str = cast(str, record["identifier_usage_count"])
                usage_count = int(usage_count_str)

                # If no longer used, delete the identifier
                if usage_count == 0:
                    delete_identifier_query = f"""
                    MATCH (i:Identifier {{value: '{self._escape_cypher_string(identifier_value)}', type: '{self._escape_cypher_string(identifier_type)}'}})
                    DETACH DELETE i
                    RETURN true AS identifier_deleted
                    """

                    await self._execute_cypher(
                        cypher_query=delete_identifier_query,
                        as_clause="as (identifier_deleted agtype)",
                        fetch_mode="row",
                    )

        return True

    @override
    async def add_fact_to_entity(
        self,
        entity_id: str,
        fact: Fact,
        source: Source,
        verb: str,
        confidence_score: float = 1.0,
        create_source: bool = True,
    ) -> AddFactToEntityResult:
        """
        Add a fact to an entity with its source using an idempotent approach.

        This method creates or updates the fact, source, and relationships in the graph.
        If create_source is False, it will only create the relationship to an existing source.
        """
        # Ensure fact_id is set
        if fact.fact_id is None:
            raise ValueError("Fact must have a fact_id set")

        # First check if the entity exists
        entity_check = await self.find_entity_by_id(entity_id)
        if entity_check is None:
            raise ValueError(f"Entity with ID '{entity_id}' does not exist")

        # Check if the HAS_FACT relationship already exists
        check_query = f"""
        MATCH (e:Entity {{id: '{entity_id}'}})-[hf:HAS_FACT {{
            verb: '{self._escape_cypher_string(verb)}'
        }}]->(f:Fact {{fact_id: '{fact.fact_id}'}})
        RETURN hf AS relationship
        """

        existing_rel = await self._execute_cypher(
            cypher_query=check_query,
            as_clause="as (relationship agtype)",
            fetch_mode="row",
        )

        if existing_rel:
            # Relationship already exists, return the existing data
            # Get the full data by finding the entity
            found = await self.find_entity_by_id(entity_id)
            if found and found["facts_with_sources"]:
                # Find the matching fact
                for fact_with_source in found["facts_with_sources"]:
                    if (
                        fact_with_source["fact"].fact_id == fact.fact_id
                        and fact_with_source["relationship"].verb == verb
                    ):
                        if fact_with_source["source"] is None:
                            raise RuntimeError(
                                "Existing fact relationship found but source is missing"
                            )
                        derived_from_rel = DerivedFrom(
                            from_fact_id=fact.fact_id,
                            to_source_id=fact_with_source["source"].id,
                        )
                        return {
                            "fact": fact_with_source["fact"],
                            "source": fact_with_source["source"],
                            "has_fact_relationship": fact_with_source["relationship"],
                            "derived_from_relationship": derived_from_rel,
                        }
            raise RuntimeError("Relationship exists but could not retrieve data")

        # Relationship doesn't exist, create it
        cypher_query = f"""
        MATCH (e:Entity {{id: '{entity_id}'}})
        MERGE (f:Fact {{
            fact_id: '{fact.fact_id}',
            name: '{self._escape_cypher_string(fact.name)}',
            type: '{self._escape_cypher_string(fact.type)}'
        }})
        MERGE (s:Source {{
            id: '{source.id}',
            content: '{self._escape_cypher_string(source.content)}',
            timestamp: '{source.timestamp.isoformat()}'
        }})
        CREATE (e)-[hf:HAS_FACT {{
            verb: '{self._escape_cypher_string(verb)}',
            confidence_score: {confidence_score},
            created_at: '{datetime.now().isoformat()}'
        }}]->(f)
        MERGE (f)-[df:DERIVED_FROM]->(s)
        RETURN {{
            fact: f,
            source: s,
            has_fact_relationship: hf,
            derived_from_relationship: df
        }} AS result
        """

        # Execute the query using the helper method
        record = await self._execute_cypher(
            cypher_query=cypher_query,
            as_clause="as (result agtype)",
            fetch_mode="row",
        )

        if not record:
            raise RuntimeError(
                f"Failed to add fact '{fact.fact_id}' to entity '{entity_id}', the query returned no results."
            )

        # Extract the result string from the agtype, clean it, and parse it as JSON
        record = cast(asyncpg.Record, record)
        result_str = cast(str, record["result"])

        cleaned_result_str = self._clean_agtype_string(result_str)
        result_map = cast(dict[str, Any], json.loads(cleaned_result_str))

        # Extract properties from the agtype objects
        fact_props = cast(dict[str, Any], result_map["fact"]["properties"])
        source_props = cast(dict[str, Any], result_map["source"]["properties"])
        has_fact_props = cast(
            dict[str, Any], result_map["has_fact_relationship"]["properties"]
        )

        # Reconstruct the objects
        # Note: fact_id is automatically computed by the model validator from name and type
        created_fact = Fact(
            name=fact_props["name"],
            type=fact_props["type"],
        )
        # Verify the fact_id matches what we expect
        if created_fact.fact_id != fact_props["fact_id"]:
            raise RuntimeError(
                f"Fact ID mismatch: expected '{fact_props['fact_id']}', got '{created_fact.fact_id}'"
            )
        # At this point we know fact_id is not None
        assert created_fact.fact_id is not None

        created_source = Source(
            id=UUID(source_props["id"]),
            content=source_props["content"],
            timestamp=datetime.fromisoformat(source_props["timestamp"]),
        )

        created_has_fact = HasFact(
            from_entity_id=UUID(entity_id),
            to_fact_id=created_fact.fact_id,
            verb=has_fact_props["verb"],
            confidence_score=has_fact_props["confidence_score"],
            created_at=datetime.fromisoformat(has_fact_props["created_at"]),
        )

        # Note: DerivedFrom relationship doesn't have additional properties beyond the connection
        derived_from_rel = DerivedFrom(
            from_fact_id=created_fact.fact_id,
            to_source_id=created_source.id,
        )

        return {
            "fact": created_fact,
            "source": created_source,
            "has_fact_relationship": created_has_fact,
            "derived_from_relationship": derived_from_rel,
        }

    @override
    async def find_fact_by_id(self, fact_id: str) -> FactWithOptionalSource | None:
        """Find a fact by its ID."""
        cypher_query = f"""
        MATCH (f:Fact {{fact_id: '{fact_id}'}})
        OPTIONAL MATCH (f)-[df:DERIVED_FROM]->(s:Source)
        RETURN {{
            fact: f,
            source: s
        }} AS result
        """

        record = await self._execute_cypher(
            cypher_query=cypher_query,
            as_clause="as (result agtype)",
            fetch_mode="row",
        )

        if not record:
            return None

        record = cast(asyncpg.Record, record)
        result_str = cast(str, record["result"])
        cleaned_result_str = self._clean_agtype_string(result_str)
        result_map = cast(dict[str, Any], json.loads(cleaned_result_str))

        # Extract fact properties
        fact_props = cast(dict[str, Any], result_map["fact"]["properties"])
        fact = Fact(
            name=fact_props["name"],
            type=fact_props["type"],
        )

        # Verify fact_id matches (should be computed by model validator)
        if fact.fact_id != fact_props["fact_id"]:
            return None

        # Extract source if it exists
        source = None
        source_data = result_map.get("source")
        if source_data:
            source_props = cast(dict[str, Any], source_data["properties"])
            source = Source(
                id=UUID(source_props["id"]),
                content=source_props["content"],
                timestamp=datetime.fromisoformat(source_props["timestamp"]),
            )

        return {
            "fact": fact,
            "source": source,
        }

    async def clear_all_data(self) -> None:
        """Clear all data from the graph. Used for testing."""
        _ = await self._execute_cypher(
            cypher_query="MATCH (n) DETACH DELETE n",
            as_clause="as (result agtype)",
            fetch_mode="none",
        )
</file>

<file path="apps/api/migrations/env.py">
import asyncio
import os
import sys
from logging.config import fileConfig

from alembic import context
from sqlalchemy import pool
from sqlalchemy.ext.asyncio import async_engine_from_config

# add your model's MetaData object here
# for 'autogenerate' support
from app.core.settings import get_settings
from app.db.postgres.auth_session import Base

# Import all models to ensure they are registered with Base.metadata
from app.features.auth import models  # noqa: F401

# Add parent directory to path to allow for package imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)


target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = get_settings().database_url
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def do_run_migrations(connection):
    """Run migrations."""
    context.configure(connection=connection, target_metadata=target_metadata)
    with context.begin_transaction():
        context.run_migrations()


async def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    configuration = config.get_section(config.config_ini_section) or {}
    configuration["sqlalchemy.url"] = get_settings().database_url
    connectable = async_engine_from_config(
        configuration,
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()


if context.is_offline_mode():
    run_migrations_offline()
else:
    asyncio.run(run_migrations_online())
</file>

<file path="apps/api/scripts/setup_postgres_schema.py">
"""Script to set up the PostgreSQL database schema for the graph."""

import asyncio

from app.core.settings import get_settings
from app.db.postgres.graph_connection import close_db_pool, get_db_pool


async def setup_schema() -> None:
    """Create the graph and set up the schema."""
    settings = get_settings()
    pool = await get_db_pool()
    async with pool.acquire() as connection:
        print("Creating AGE extension...")
        await connection.execute("CREATE EXTENSION IF NOT EXISTS age;")

        print("Loading AGE extension...")
        await connection.execute("LOAD 'age';")

        print("Setting search path...")
        await connection.execute("SET search_path = ag_catalog, '$user', public;")

        graph_name = settings.age_graph_name

        # Delete the old knowledge_graph if it exists
        old_graph_exists = await connection.fetchval(
            "SELECT 1 FROM ag_graph WHERE name = $1;", "knowledge_graph"
        )
        if old_graph_exists:
            print("Deleting old 'knowledge_graph'...")
            await connection.execute("SELECT drop_graph('knowledge_graph', true);")
            print("Old graph deleted.")

        print(f"Creating graph '{graph_name}'...")

        # Check if graph exists
        graph_exists = await connection.fetchval(
            "SELECT 1 FROM ag_graph WHERE name = $1;", graph_name
        )
        if not graph_exists:
            await connection.execute(f"SELECT create_graph('{graph_name}');")
            print(f"Graph '{graph_name}' created.")
        else:
            print(f"Graph '{graph_name}' already exists.")

        print("Creating vertex labels...")
        # Create vertex labels if they don't exist
        await connection.execute(f"SELECT create_vlabel('{graph_name}', 'Entity');")
        await connection.execute(f"SELECT create_vlabel('{graph_name}', 'Identifier');")
        await connection.execute(f"SELECT create_vlabel('{graph_name}', 'Fact');")
        await connection.execute(f"SELECT create_vlabel('{graph_name}', 'Source');")

        print("Creating edge labels...")
        # Create edge labels if they don't exist
        await connection.execute(
            f"SELECT create_elabel('{graph_name}', 'HAS_IDENTIFIER');"
        )
        await connection.execute(f"SELECT create_elabel('{graph_name}', 'HAS_FACT');")
        await connection.execute(f"SELECT create_elabel('{graph_name}', 'HAS_SOURCE');")

        print("Schema setup complete.")


async def main() -> None:
    """Main function to run the schema setup."""
    try:
        await setup_schema()
    finally:
        await close_db_pool()


if __name__ == "__main__":
    print("Starting database schema setup...")
    asyncio.run(main())
</file>

<file path="apps/api/tests/features/auth/usecases/test_delete_api_key_usecase_integration.py">
"""Integration tests for the DeleteApiKeyUseCase."""

from datetime import UTC, datetime, timedelta
from uuid import uuid4

import pytest
from fastapi import HTTPException
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.authentication import get_password_hash
from app.core.schemas import UserRole
from app.features.auth.models import ApiKey, Tenant, User
from app.features.auth.usecases.delete_api_key_usecase import DeleteApiKeyUseCaseImpl
from app.features.auth.usecases.signup_tenant_usecase import PasswordHasher

# All fixtures are now provided by tests/conftest.py


class TestDeleteApiKeyUseCase:
    """Test suite for the DeleteApiKeyUseCase."""

    async def test_delete_api_key_successfully(
        self,
        db_session: AsyncSession,
        password_hasher: PasswordHasher,
    ):
        """Test the successful deletion of an API key."""

        # Arrange - Create a tenant, user, and API key first

        async with db_session.begin():
            tenant = Tenant(name="test-tenant", age_graph_name="test_graph_123")
            db_session.add(tenant)
            await db_session.flush()

            user_model = User(
                email="user@example.com",
                hashed_password=password_hasher.hash("userpass"),
                tenant_id=tenant.id,
                role=UserRole.TENANT_USER,
            )
            db_session.add(user_model)
            await db_session.flush()

            # Create API key directly (to avoid session conflicts)
            api_key = ApiKey(
                name="test-api-key",
                key_prefix="testpref",
                hashed_key=get_password_hash("testpref.testkey"),
                tenant_id=tenant.id,
                expires_at=datetime.now(UTC) + timedelta(days=365),
            )
            db_session.add(api_key)
            await db_session.flush()

        delete_use_case = DeleteApiKeyUseCaseImpl(get_db_session=lambda: db_session)

        # Act
        response = await delete_use_case.execute(str(api_key.id), tenant.id)

        # Assert
        assert response == {"message": "API key deleted successfully"}

        # Verify database state - API key should be gone
        # Use a fresh query to avoid session cache issues
        result = await db_session.execute(
            text("SELECT COUNT(*) FROM api_keys WHERE id = :id").bindparams(
                id=api_key.id
            )
        )
        count = result.scalar()
        assert count == 0

    async def test_delete_api_key_not_found(
        self,
        db_session: AsyncSession,
        password_hasher: PasswordHasher,
    ):
        """Test that deleting a non-existent API key fails with 404."""

        # Arrange - Create a tenant and user first
        async with db_session.begin():
            tenant = Tenant(name="test-tenant", age_graph_name="test_graph_456")
            db_session.add(tenant)
            await db_session.flush()

            user_model = User(
                email="user@example.com",
                hashed_password=password_hasher.hash("userpass"),
                tenant_id=tenant.id,
                role=UserRole.TENANT_USER,
            )
            db_session.add(user_model)
            await db_session.flush()

        delete_use_case = DeleteApiKeyUseCaseImpl(get_db_session=lambda: db_session)

        # Act & Assert - Try to delete a non-existent API key
        fake_api_key_id = str(uuid4())
        with pytest.raises(HTTPException) as excinfo:
            await delete_use_case.execute(fake_api_key_id, tenant.id)
        assert excinfo.value.status_code == 404
        assert "API key not found" in excinfo.value.detail

    async def test_delete_api_key_invalid_uuid(
        self,
        db_session: AsyncSession,
        password_hasher: PasswordHasher,
    ):
        """Test that deleting with an invalid UUID format fails with 400."""

        # Arrange - Create a tenant and user first
        async with db_session.begin():
            tenant = Tenant(name="test-tenant", age_graph_name="test_graph_789")
            db_session.add(tenant)
            await db_session.flush()

            user_model = User(
                email="user@example.com",
                hashed_password=password_hasher.hash("userpass"),
                tenant_id=tenant.id,
                role=UserRole.TENANT_USER,
            )
            db_session.add(user_model)
            await db_session.flush()

        delete_use_case = DeleteApiKeyUseCaseImpl(get_db_session=lambda: db_session)

        # Act & Assert - Try to delete with invalid UUID
        with pytest.raises(HTTPException) as excinfo:
            await delete_use_case.execute("invalid-uuid", tenant.id)
        assert excinfo.value.status_code == 400
        assert "Invalid API key ID format" in excinfo.value.detail

    async def test_delete_api_key_wrong_tenant(
        self,
        db_session: AsyncSession,
        password_hasher: PasswordHasher,
    ):
        """Test that deleting an API key from a different tenant fails with 403."""

        # Arrange - Create two tenants and users, and API key for tenant1

        async with db_session.begin():
            tenant1 = Tenant(name="tenant1", age_graph_name="test_graph_101")
            tenant2 = Tenant(name="tenant2", age_graph_name="test_graph_202")
            db_session.add_all([tenant1, tenant2])
            await db_session.flush()

            user1_model = User(
                email="user1@example.com",
                hashed_password=password_hasher.hash("userpass"),
                tenant_id=tenant1.id,
                role=UserRole.TENANT_USER,
            )
            user2_model = User(
                email="user2@example.com",
                hashed_password=password_hasher.hash("userpass"),
                tenant_id=tenant2.id,
                role=UserRole.TENANT_USER,
            )
            db_session.add_all([user1_model, user2_model])
            await db_session.flush()

            # Create API key directly for tenant1
            api_key = ApiKey(
                name="test-api-key",
                key_prefix="testpref2",
                hashed_key=get_password_hash("testpref2.testkey"),
                tenant_id=tenant1.id,
                expires_at=datetime.now(UTC) + timedelta(days=365),
            )
            db_session.add(api_key)
            await db_session.flush()

            # Store the ID before the session context ends
            api_key_id = api_key.id

        delete_use_case = DeleteApiKeyUseCaseImpl(get_db_session=lambda: db_session)

        # Act & Assert - Try to delete tenant1's API key as tenant2
        with pytest.raises(HTTPException) as excinfo:
            await delete_use_case.execute(str(api_key_id), tenant2.id)
        assert excinfo.value.status_code == 403
        assert "Access denied" in excinfo.value.detail

        # Verify API key still exists
        result = await db_session.execute(
            text("SELECT COUNT(*) FROM api_keys WHERE id = :id").bindparams(
                id=api_key_id
            )
        )
        count = result.scalar()
        assert count == 1
</file>

<file path="apps/api/tests/utils/database.py">
"""Database utilities for testing.

This module provides helper functions for managing test databases,
including creating/dropping databases, setting up AGE extension,
and cleaning up test data.
"""

import asyncpg
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncConnection, AsyncEngine

from app.core.settings import Settings
from app.db.postgres.auth_session import Base

# Import all models to ensure they are registered with Base.metadata
# This allows Base.metadata.create_all() and Base.metadata.drop_all() to work properly
from app.features.auth import models  # noqa: F401


async def create_test_database(settings: Settings) -> None:
    """Create test database if it doesn't exist.

    Args:
        settings: Application settings with test database configuration
    """
    # Connect to default 'postgres' database to create test database
    conn = await asyncpg.connect(
        user=settings.postgres_user,
        password=settings.postgres_password,
        host=settings.postgres_host,
        port=settings.postgres_port,
        database="postgres",
    )

    try:
        # Check if test database exists
        exists = await conn.fetchval(
            "SELECT 1 FROM pg_database WHERE datname = $1",
            settings.test_postgres_db,
        )

        if not exists:
            # Create test database
            await conn.execute(f'CREATE DATABASE "{settings.test_postgres_db}"')
    finally:
        await conn.close()


async def drop_test_database(settings: Settings) -> None:
    """Drop test database if it exists.

    Args:
        settings: Application settings with test database configuration
    """
    # Connect to default 'postgres' database to drop test database
    conn = await asyncpg.connect(
        user=settings.postgres_user,
        password=settings.postgres_password,
        host=settings.postgres_host,
        port=settings.postgres_port,
        database="postgres",
    )

    try:
        # Terminate all connections to test database
        await conn.execute(
            """
            SELECT pg_terminate_backend(pg_stat_activity.pid)
            FROM pg_stat_activity
            WHERE pg_stat_activity.datname = $1
            AND pid <> pg_backend_pid()
            """,
            settings.test_postgres_db,
        )

        # Drop test database
        await conn.execute(f'DROP DATABASE IF EXISTS "{settings.test_postgres_db}"')
    finally:
        await conn.close()


async def setup_age_extension(pool: asyncpg.Pool) -> None:
    """Install and configure AGE extension in the database.

    Args:
        pool: Database connection pool
    """
    async with pool.acquire() as conn:
        await conn.execute("CREATE EXTENSION IF NOT EXISTS age;")
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, '$user', public;")


async def cleanup_age_graphs(pool: asyncpg.Pool) -> None:
    """Drop all AGE graphs in the database.

    Args:
        pool: Database connection pool
    """
    async with pool.acquire() as conn:
        await conn.execute("LOAD 'age';")
        await conn.execute("SET search_path = ag_catalog, '$user', public;")

        # Get all graph names
        graph_names = await conn.fetch("SELECT name FROM ag_catalog.ag_graph;")

        # Drop each graph
        for row in graph_names:
            graph_name = row["name"]
            try:
                await conn.execute(
                    f"SELECT ag_catalog.drop_graph('{graph_name}', true);"
                )
            except Exception as e:
                # Continue even if one graph fails to drop
                print(f"Warning: Failed to drop graph {graph_name}: {e}")


async def create_all_tables(engine: AsyncEngine) -> None:
    """Create all tables from SQLAlchemy metadata.

    Args:
        engine: SQLAlchemy async engine
    """

    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)


async def drop_all_tables(engine: AsyncEngine) -> None:
    """Drop all tables from SQLAlchemy metadata.

    Args:
        engine: SQLAlchemy async engine
    """

    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.drop_all)


async def clear_all_tables(conn: AsyncConnection) -> None:
    """Clear all data from tables without dropping them.

    Args:
        conn: SQLAlchemy async connection
    """
    # Get all table names from auth schema
    result = await conn.execute(
        text(
            """
            SELECT tablename FROM pg_tables
            WHERE schemaname = 'public'
            AND tablename NOT LIKE 'pg_%'
            AND tablename NOT LIKE 'sql_%'
            """
        )
    )

    tables = [row[0] for row in result]

    # Disable foreign key checks temporarily
    await conn.execute(text("SET session_replication_role = 'replica';"))

    try:
        # Truncate all tables
        for table in tables:
            await conn.execute(text(f'TRUNCATE TABLE "{table}" CASCADE;'))
    finally:
        # Re-enable foreign key checks
        await conn.execute(text("SET session_replication_role = 'origin';"))
</file>

<file path="apps/web/docs/plans/architecture.md">
## Recommended Project Structure

I recommend refactoring your current `apps/web/src` directory from the default Vite starter into a **feature-based architecture**. This will keep your code organized as the application grows and align perfectly with your backend's structure.

Here is the proposed folder structure:

```plaintext
/apps/web/src/
|
|-- api/
|   |-- index.ts            # Axios/fetch client setup, interceptors, etc.
|   `-- graphApi.ts         # API functions related to the graph feature
|
|-- assets/                 # Static assets like images, fonts, global CSS
|   `-- styles/
|       |-- main.css
|       `-- _variables.css
|
|-- components/             # Global, shared, reusable components (e.g., Button, Modal, Layout)
|   |-- layout/
|   |   |-- AppHeader.vue
|   |   `-- MainLayout.vue
|   `-- ui/
|       |-- AppButton.vue
|       `-- LoadingSpinner.vue
|
|-- features/               # CORE: Each business feature gets its own module
|   `-- graph/
|       |-- components/     # Components specific to the graph feature
|       |   |-- EntityCard.vue
|       |   `-- FactList.vue
|       |-- views/          # Routable page components for this feature
|       |   |-- EntityDetailView.vue
|       |   `-- EntitySearchView.vue
|       |-- store.ts        # Pinia store for graph feature state
|       `-- routes.ts       # Routes specific to the graph feature
|
|-- router/
|   `-- index.ts            # Main router configuration, combines feature routes
|
|-- stores/
|   `-- index.ts            # Pinia setup and main store registration
|
|-- types/                  # Global TypeScript types and interfaces
|   `-- api.ts
|
|-- App.vue                 # Main application component (often contains the router-view)
`-- main.ts                 # App entry point (initializes Vue, router, Pinia)
```

---

## Explanation of Key Directories

- üìÅ **`features/`**: This is the heart of the architecture. Instead of grouping files by type (all components together, all views together), you group them by **business functionality**. A feature like `graph` contains everything it needs to function independently: its own components, routable views, state management (`store.ts`), and routes.

- üìÅ **`components/`**: This directory is for **truly global and reusable components** that are not tied to any specific feature. Think of UI kits (`AppButton.vue`) or application layout components (`MainLayout.vue`).

- üìÅ **`router/`**: This handles all routing logic. The main `index.ts` file is responsible for creating the router instance and, most importantly, **importing and consolidating the route definitions from each feature module** in `features/*/routes.ts`.

- üìÅ **`stores/`**: This is for your state management using **Pinia** (the official state management library for Vue). Each feature will have its own store file (e.g., `features/graph/store.ts`), making your state modular and easy to manage.

- üìÅ **`api/`**: This layer is responsible for all communication with your FastAPI backend. It abstracts away the HTTP logic so your components and stores remain clean.

---

## Bootstrapping Your SPA (`main.ts` & Routing)

You asked specifically about bootstrapping. This happens in two main places: `main.ts` and `router/index.ts`.

#### 1. Configure `main.ts` (The Entry Point)

Your `main.ts` should be lean. Its job is to create the Vue app instance and "plug in" your core modules like the router and state management.

**`apps/web/src/main.ts`**

```typescript
import { createApp } from "vue";
import { createPinia } from "pinia"; // Import Pinia
import router from "./router"; // Import the router

import App from "./App.vue";
import "./assets/styles/main.css";

const app = createApp(App);
const pinia = createPinia();

app.use(pinia); // Use Pinia for state management
app.use(router); // Use the router

app.mount("#app");
```

#### 2. Set Up Modular Routing

To keep your routing clean, each feature defines its own routes. The main router file then combines them.

**First, define routes for a feature:**
**`apps/web/src/features/graph/routes.ts`**

```typescript
import type { RouteRecordRaw } from "vue-router";

// Lazy-load the view components for better performance
const EntitySearchView = () => import("./views/EntitySearchView.vue");
const EntityDetailView = () => import("./views/EntityDetailView.vue");

export const graphRoutes: RouteRecordRaw[] = [
  {
    path: "/graph/search",
    name: "EntitySearch",
    component: EntitySearchView,
  },
  {
    // Example with dynamic segment for entity ID
    path: "/graph/entity/:id",
    name: "EntityDetail",
    component: EntityDetailView,
    props: true, // Pass route params as component props
  },
];
```

**Next, combine feature routes in the main router:**
**`apps/web/src/router/index.ts`**

```typescript
import { createRouter, createWebHistory } from "vue-router";
import { graphRoutes } from "@/features/graph/routes"; // Import feature routes

const router = createRouter({
  history: createWebHistory(),
  routes: [
    {
      path: "/",
      name: "Home",
      // Example of a simple home page component
      component: () => import("@/components/layout/MainLayout.vue"),
    },
    // Spread in the routes from your features
    ...graphRoutes,
    // ... add routes from other features here
  ],
});

export default router;
```

---

## State Management with Pinia

Pinia stores are perfect for this modular approach. You can create a store for each feature to manage its specific state.

**`apps/web/src/features/graph/store.ts`**

```typescript
import { defineStore } from "pinia";
import * as graphApi from "@/api/graphApi"; // Import your API functions
import type { Entity } from "@/types/api"; // Import your types

interface GraphState {
  entities: Entity[];
  isLoading: boolean;
  error: string | null;
}

// 'useGraphStore' is the hook you'll use in your components
export const useGraphStore = defineStore("graph", {
  state: (): GraphState => ({
    entities: [],
    isLoading: false,
    error: null,
  }),
  actions: {
    async searchEntities(query: string) {
      this.isLoading = true;
      this.error = null;
      try {
        // Call the API layer to fetch data
        const response = await graphApi.findEntityByIdentifier({
          type: "email",
          value: query,
        });
        this.entities = [response.entity]; // Adjust based on your API response
      } catch (err) {
        this.error = "Failed to fetch entities.";
      } finally {
        this.isLoading = false;
      }
    },
  },
});
```

---

## API Layer

Create a dedicated file for your API client setup and another for your feature-specific API calls. This separates concerns beautifully.

**`apps/web/src/api/index.ts`** (Example using `fetch`)

```typescript
const BASE_URL = "http://localhost:8000/api/v1"; // Or from .env

async function request<T>(
  endpoint: string,
  options: RequestInit = {}
): Promise<T> {
  const url = `${BASE_URL}${endpoint}`;

  const defaultOptions: RequestInit = {
    headers: {
      "Content-Type": "application/json",
      // Add Authorization header if needed
    },
    ...options,
  };

  const response = await fetch(url, defaultOptions);

  if (!response.ok) {
    throw new Error(`HTTP error! status: ${response.status}`);
  }

  return response.json();
}

export default request;
```

**`apps/web/src/api/graphApi.ts`**

```typescript
import request from "./index";
import type { GetEntityResponse } from "@/types/api"; // Define your response types

interface FindParams {
  type: string;
  value: string;
}

export const findEntityByIdentifier = (
  params: FindParams
): Promise<GetEntityResponse> => {
  return request(
    `/graph/entities/lookup?type=${params.type}&value=${params.value}`
  );
};

// Add other functions like assimilateKnowledge here
```

---

## Next Steps üöÄ

1.  **Install Dependencies**: Add `vue-router` and `pinia` to your `apps/web/package.json`.
    ```bash
    pnpm --filter web add vue-router pinia
    ```
2.  **Refactor**: Reorganize your `src` directory according to the structure above.
3.  **Implement**: Start by creating your router setup and your first feature module, `graph`. Create the `routes.ts` and a placeholder `EntitySearchView.vue`.
4.  **Connect**: Build out the API layer to start fetching data from your FastAPI backend.

This architecture will provide a solid, scalable foundation for your Vue.js application, making it a pleasure to work with as it evolves.
</file>

<file path="apps/web/src/components/ui/input/Input.vue">
<script setup lang="ts">
import type { HTMLAttributes } from "vue";
import { useVModel } from "@vueuse/core";
import { cn } from "@/lib/utils";

const props = defineProps<{
  defaultValue?: string | number;
  modelValue?: string | number;
  class?: HTMLAttributes["class"];
}>();

const emits = defineEmits<{
  (e: "update:modelValue", payload: string | number): void;
}>();

const modelValue = useVModel(props, "modelValue", emits, {
  passive: true,
  defaultValue: props.defaultValue,
});
</script>

<template>
  <input
    v-model="modelValue"
    :class="
      cn(
        'flex h-10 w-full rounded-md border border-input bg-background px-3 py-2 text-sm ring-offset-background file:border-0 file:bg-transparent file:text-foreground file:text-sm file:font-medium placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50',
        props.class
      )
    "
  />
</template>
</file>

<file path="apps/web/src/features/graph/api/graphApi.ts">
import { useApiFetch } from "@/api";
import type {
  GetEntityResponse,
  AssimilateKnowledgeRequest,
  AssimilateKnowledgeResponse,
} from "@/types/api";
import { computed, toValue, type MaybeRefOrGetter } from "vue";

export interface FindEntityParams {
  type: string;
  value: string;
}

export const useFindEntityByIdentifier = (
  params: MaybeRefOrGetter<FindEntityParams>
) => {
  // The URL is a computed property, reacting to changes in `params`
  const url = computed(() => {
    const resolvedParams = toValue(params);
    // Return empty string to prevent invalid requests, let consumers handle empty state
    if (!resolvedParams || !resolvedParams.value) {
      return "";
    }
    return `/graph/entities/lookup?type=${resolvedParams.type}&value=${resolvedParams.value}`;
  });

  // The full reactive useFetch object is returned.
  return useApiFetch(url, {
    refetch: false, // Disable automatic refetching when URL changes
    immediate: false, // Don't execute immediately, let consumers control execution
  })
    .get()
    .json<GetEntityResponse>();
};

export const useAssimilateKnowledge = (payload: AssimilateKnowledgeRequest) => {
  const url = "/graph/entities/assimilate";

  // Use the .post() convenience method, passing the payload.
  // This returns the reactive useFetch object, which you can `await` or
  // use to track the state of the POST request.
  return useApiFetch(url).post(payload).json<AssimilateKnowledgeResponse>();
};
</file>

<file path="apps/web/src/features/graph/routes.ts">
import type { RouteRecordRaw } from "vue-router";

// Mock authentication - replace with real auth logic later
const isAuthenticated = () => {
  // Mock: check localStorage or return false for demo
  return localStorage.getItem("isLoggedIn") === "true";
};

export const graphRoutes: RouteRecordRaw[] = [
  {
    path: "/",
    name: "Home",
    component: () => import("./views/HomeView.vue"),
    beforeEnter: (_to, _from, next) => {
      // Redirect to login if not authenticated
      if (!isAuthenticated()) {
        next("/login");
      } else {
        next();
      }
    },
  },
];
</file>

<file path="apps/web/src/App.vue">
<template>
  <div id="app">
    <router-view />
  </div>
</template>

<script setup lang="ts">
// Main application component - router-view renders the current route component
</script>

<style>
#app {
  font-family: Avenir, Helvetica, Arial, sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  color: #2c3e50;
}
</style>
</file>

<file path="apps/web/index.html">
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Nous Memory</title>
  </head>
  <body>
    <div id="app"></div>
    <script type="module" src="/src/main.ts"></script>
  </body>
</html>
</file>

<file path="apps/web/tsconfig.app.json">
{
  "extends": "@vue/tsconfig/tsconfig.dom.json",
  "compilerOptions": {
    "baseUrl": ".",
    "paths": {
      "@/*": ["src/*"]
    },
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.app.tsbuildinfo",
    "types": ["vite/client"],

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["src/**/*.ts", "src/**/*.tsx", "src/**/*.vue"]
}
</file>

<file path="apps/web/tsconfig.json">
{
  "files": [],
  "compilerOptions": {
    "baseUrl": ".",
    "paths": {
      "@/*": ["./src/*"]
    }
  },
  "references": [
    { "path": "./tsconfig.app.json" },
    { "path": "./tsconfig.node.json" }
  ]
}
</file>

<file path="turbo.json">
{
  "$schema": "https://turbo.build/schema.json",
  "tasks": {
    "test": {
      "outputs": ["coverage/**"]
    },
    "lint": {},
    "dev": {
      "cache": false,
      "persistent": true
    }
  }
}
</file>

<file path="apps/api/app/features/auth/routes/tenants.py">
"""Signup route handler."""

from typing import Protocol

from fastapi import APIRouter, Depends

from app.core.authentication import AuthenticatedUser, pwd_context
from app.core.authorization import is_super_admin
from app.db.postgres.auth_session import get_auth_db_session
from app.db.postgres.graph_connection import get_graph_db_pool
from app.features.auth.dtos import CreateTenantRequest, CreateTenantResponse
from app.features.auth.usecases.signup_tenant_usecase import SignupTenantUseCaseImpl


class PasswordHasherImpl:
    """Wrapper for password hashing to match protocol."""

    def hash(self, secret: str | bytes, **kwargs) -> str:
        """Hash a password or secret."""
        return pwd_context.hash(secret, **kwargs)


async def get_signup_tenant_use_case():
    """Dependency injection for the signup tenant use case."""
    return SignupTenantUseCaseImpl(
        password_hasher=PasswordHasherImpl(),
        get_db_session=get_auth_db_session,
        get_db_pool=get_graph_db_pool,
    )


class SignupTenantUseCase(Protocol):
    """Protocol for the signup tenant use case."""

    async def execute(self, request: CreateTenantRequest) -> CreateTenantResponse:
        """Create a new tenant with user and graph."""
        ...


router = APIRouter()


@router.post("/create_tenant", response_model=CreateTenantResponse)
async def create_tenant(
    request: CreateTenantRequest,
    use_case: SignupTenantUseCase = Depends(get_signup_tenant_use_case),
    _: AuthenticatedUser = Depends(is_super_admin),
) -> CreateTenantResponse:
    """Create a new tenant with an initial user and AGE graph.

    This endpoint creates:
    1. A new tenant record
    2. An initial user for the tenant
    3. A dedicated Apache AGE graph for the tenant
    """
    return await use_case.execute(request)
</file>

<file path="apps/api/app/features/auth/routes/users.py">
from typing import Protocol

from fastapi import APIRouter, Depends, status

from app.core.authentication import pwd_context
from app.core.authorization import is_tenant_admin
from app.core.schemas import AuthenticatedUser
from app.db.postgres.auth_session import get_auth_db_session
from app.features.auth.dtos import CreateUserRequest, CreateUserResponse
from app.features.auth.usecases.create_user_usecase import CreateUserUseCaseImpl

router = APIRouter()


class PasswordHasherImpl:
    """Wrapper for password hashing to match protocol."""

    def hash(self, secret: str | bytes, **kwargs) -> str:
        """Hash a password or secret."""
        return pwd_context.hash(secret, **kwargs)


async def get_create_user_use_case():
    """Dependency injection for the create user use case."""
    return CreateUserUseCaseImpl(
        password_hasher=PasswordHasherImpl(),
        get_db_session=get_auth_db_session,
    )


class CreateUserUseCase(Protocol):
    """Protocol for the create user use case."""

    async def execute(
        self, request: CreateUserRequest, admin_user: AuthenticatedUser
    ) -> CreateUserResponse:
        """Create a new user within a tenant."""
        ...


@router.post(
    "/users", response_model=CreateUserResponse, status_code=status.HTTP_201_CREATED
)
async def create_tenant_user(
    request: CreateUserRequest,
    admin_user: AuthenticatedUser = Depends(is_tenant_admin),
    use_case: CreateUserUseCase = Depends(get_create_user_use_case),
) -> CreateUserResponse:
    """
    Allows a TENANT_ADMIN to create a new user within their own tenant.
    """
    return await use_case.execute(request, admin_user)
</file>

<file path="apps/api/app/features/auth/usecases/__init__.py">
"""Authentication use cases."""

from .create_api_key_usecase import CreateApiKeyUseCaseImpl
from .delete_api_key_usecase import DeleteApiKeyUseCaseImpl
from .list_api_keys_usecase import ListApiKeysUseCaseImpl
from .login_usecase import LoginUseCaseImpl
from .signup_tenant_usecase import SignupTenantUseCaseImpl

__all__ = [
    "SignupTenantUseCaseImpl",
    "LoginUseCaseImpl",
    "CreateApiKeyUseCaseImpl",
    "ListApiKeysUseCaseImpl",
    "DeleteApiKeyUseCaseImpl",
]
</file>

<file path="apps/api/scripts/create_super_admin.py">
import argparse
import asyncio
import uuid

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine

from app.core.authentication import get_password_hash
from app.core.schemas import UserRole
from app.core.settings import get_settings
from app.db.postgres.auth_session import Base
from app.features.auth.models import User


async def create_super_admin(email, password):
    """Creates the first super admin user."""
    if not email or not password:
        print("Email and password cannot be empty.")
        return

    settings = get_settings()
    engine = create_async_engine(settings.database_url, echo=True)

    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    async with engine.connect() as conn:
        async with conn.begin():
            session = AsyncSession(engine)

            result = await session.execute(select(User).where(User.email == email))
            if result.scalar_one_or_none():
                print(f"User with email {email} already exists.")
                return

            hashed_password = get_password_hash(password)
            super_admin = User(
                id=uuid.uuid4(),
                email=email,
                hashed_password=hashed_password,
                role=UserRole.SUPER_ADMIN,
                is_active=True,
                tenant_id=None,
            )
            session.add(super_admin)
            await session.commit()
            print(f"Super admin {email} created successfully.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Create a super admin user.")
    parser.add_argument("--email", required=True, help="Email for the super admin.")
    parser.add_argument(
        "--password", required=True, help="Password for the super admin."
    )
    args = parser.parse_args()

    asyncio.run(create_super_admin(args.email, args.password))
</file>

<file path="apps/api/tests/db/postgres/test_postgres_integration.py">
"""Integration tests for PostgreSQL connection management."""

from collections.abc import AsyncGenerator

import asyncpg
import pytest

from app.core.settings import get_settings
from app.db.postgres.graph_connection import (
    close_graph_db_pool,
    get_graph_db_pool,
)


@pytest.fixture
async def postgres_pool() -> AsyncGenerator[asyncpg.Pool, None]:
    """Provide a connection pool and ensure it's closed after the test."""
    pool = await get_graph_db_pool()
    try:
        yield pool
    finally:
        # Ensure the pool is closed after each test
        await close_graph_db_pool()


class TestPostgresIntegration:
    """Integration tests for PostgreSQL using a real database connection."""

    @pytest.mark.asyncio
    async def test_settings_configuration(self):
        """Test that PostgreSQL settings are properly configured."""
        settings = get_settings()

        # Check that required settings exist and are of the correct type
        assert hasattr(settings, "postgres_user")
        assert isinstance(settings.postgres_user, str)
        assert len(settings.postgres_user.strip()) > 0

        assert hasattr(settings, "postgres_password")
        assert isinstance(settings.postgres_password, str)

        assert hasattr(settings, "postgres_host")
        assert isinstance(settings.postgres_host, str)
        assert len(settings.postgres_host.strip()) > 0

        assert hasattr(settings, "postgres_port")
        assert isinstance(settings.postgres_port, int)

        assert hasattr(settings, "postgres_db")
        assert isinstance(settings.postgres_db, str)
        assert len(settings.postgres_db.strip()) > 0

    @pytest.mark.asyncio
    async def test_connection_pooling_lifecycle(self):
        """Test the singleton behavior of the connection pool."""
        try:
            # First call should create a new pool
            pool1 = await get_graph_db_pool()
            assert isinstance(pool1, asyncpg.Pool)
            assert not pool1.is_closing()

            # Second call should return the same pool instance
            pool2 = await get_graph_db_pool()
            assert pool1 is pool2

            # Close the pool
            await close_graph_db_pool()
            assert pool1.is_closing()

            # The global _pool variable should be None now
            # Next call should create a new pool
            new_pool = await get_graph_db_pool()
            assert isinstance(new_pool, asyncpg.Pool)
            assert new_pool is not pool1

        except Exception as e:
            pytest.skip(f"PostgreSQL server not available: {e}")
        finally:
            # Final cleanup
            await close_graph_db_pool()

    @pytest.mark.asyncio
    async def test_successful_connection_and_query(self, postgres_pool: asyncpg.Pool):
        """Test that we can connect and execute a simple query."""
        try:
            async with postgres_pool.acquire() as connection:
                async with connection.transaction():
                    result = await connection.fetchval("SELECT 1")
            assert result == 1
        except Exception as e:
            pytest.skip(f"PostgreSQL server not available: {e}")
</file>

<file path="apps/api/tests/features/auth/usecases/test_signup_tenant_usecase_integration.py">
"""Integration tests for the SignupTenantUseCase."""

import asyncpg
import pytest
from sqlalchemy.ext.asyncio import AsyncSession

from app.features.auth.usecases.signup_tenant_usecase import (
    PasswordHasher,
    SignupTenantUseCaseImpl,
)

# All fixtures are now provided by tests/conftest.py


@pytest.mark.asyncio
class TestSignupTenantUseCase:
    """Test suite for the SignupTenantUseCase."""

    async def test_signup_tenant_successfully(
        self,
        db_session: AsyncSession,
        postgres_pool: asyncpg.Pool,
        password_hasher: PasswordHasher,
    ):
        """Test the successful creation of a tenant, user, and graph."""
        # Arrange
        use_case = SignupTenantUseCaseImpl(
            password_hasher=password_hasher,
            get_db_session=lambda: db_session,  # type: ignore
            get_db_pool=lambda: postgres_pool,  # type: ignore
        )
        from app.features.auth.dtos import CreateTenantRequest

        request = CreateTenantRequest(
            name="test-tenant",
            email="test@example.com",
            password="testpassword",
        )

        # Act
        response = await use_case.execute(request)

        # Assert
        assert response.message == "Tenant created successfully"
        assert response.tenant_id is not None
        assert response.user_id is not None

        # Verify database state
        from sqlalchemy.future import select

        from app.features.auth.models import Tenant, User

        tenant = (
            await db_session.execute(select(Tenant).filter_by(id=response.tenant_id))
        ).scalar_one_or_none()
        assert tenant is not None
        assert tenant.name == "test-tenant"

        user = (
            await db_session.execute(select(User).filter_by(id=response.user_id))
        ).scalar_one_or_none()
        assert user is not None
        assert user.email == "test@example.com"

        # Verify graph creation
        async with postgres_pool.acquire() as conn:
            await conn.execute("SET search_path = ag_catalog, '$user', public;")
            graph_exists = await conn.fetchval(
                "SELECT 1 FROM ag_graph WHERE name = $1;", tenant.age_graph_name
            )
            assert graph_exists == 1

    async def test_signup_tenant_duplicate(
        self,
        db_session: AsyncSession,
        postgres_pool: asyncpg.Pool,
        password_hasher: PasswordHasher,
    ):
        """Test that creating a tenant with a duplicate name or email fails."""
        # Arrange
        use_case = SignupTenantUseCaseImpl(
            password_hasher=password_hasher,
            get_db_session=lambda: db_session,
            get_db_pool=lambda: postgres_pool,
        )
        from app.features.auth.dtos import CreateTenantRequest

        request = CreateTenantRequest(
            name="test-tenant",
            email="test@example.com",
            password="testpassword",
        )
        await use_case.execute(request)

        # Act & Assert
        from fastapi import HTTPException

        with pytest.raises(HTTPException) as excinfo:
            await use_case.execute(request)
        assert excinfo.value.status_code == 400

    @pytest.mark.parametrize(
        "name, email, password, error_detail",
        [
            (
                "a",
                "test@example.com",
                "password",
                "Tenant name must be between 3 and 50 characters",
            ),
            (
                "test-tenant",
                "test@example.com",
                "short",
                "Password must be at least 8 characters long",
            ),
            (
                "invalid name!",
                "test@example.com",
                "password",
                "Tenant name can only contain alphanumeric characters, hyphens, and underscores",
            ),
        ],
    )
    async def test_signup_tenant_invalid_input(
        self,
        db_session: AsyncSession,
        postgres_pool: asyncpg.Pool,
        password_hasher: PasswordHasher,
        name: str,
        email: str,
        password: str,
        error_detail: str,
    ):
        """Test that signup fails with invalid input."""
        # Arrange
        use_case = SignupTenantUseCaseImpl(
            password_hasher=password_hasher,
            get_db_session=lambda: db_session,  # type: ignore
            get_db_pool=lambda: postgres_pool,  # type: ignore
        )
        from app.features.auth.dtos import CreateTenantRequest

        request = CreateTenantRequest(
            name=name,
            email=email,
            password=password,
        )
        from fastapi import HTTPException

        # Act & Assert
        with pytest.raises(HTTPException) as excinfo:
            await use_case.execute(request)
        assert excinfo.value.status_code == 400
        assert excinfo.value.detail == error_detail
</file>

<file path="apps/api/README.md">
# Nous - FastAPI Modular Architecture

A FastAPI application built with a modular, feature-based architecture supporting PostgreSQL AGE Graph Database.

## Architecture

This project follows the modular architecture defined in [Project-architecture](docs/project_architecture.md), with tests properly separated from application code in a dedicated `tests/` directory.

## Project Structure

```plaintext
/nous-api/
‚îú‚îÄ‚îÄ app/                          # Application source code
‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # FastAPI app instance
‚îÇ   ‚îú‚îÄ‚îÄ core/                      # App-wide concerns
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ security.py           # Authentication & security
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py           # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ db/                       # Database connections
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgres/             # PostgreSQL AGE connection
‚îÇ   ‚îî‚îÄ‚îÄ features/                 # Feature modules
‚îÇ       ‚îî‚îÄ‚îÄ graph/                # Graph feature
‚îÇ           ‚îú‚îÄ‚îÄ models/           # Domain models
‚îÇ           ‚îú‚îÄ‚îÄ repositories/     # Data access layer
‚îÇ           ‚îú‚îÄ‚îÄ routes/           # API endpoints
‚îÇ           ‚îî‚îÄ‚îÄ usecases/         # Business logic
‚îú‚îÄ‚îÄ tests/                        # Test suite (mirrors app structure)
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_security.py      # Security tests
‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ graph/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ test_graph.py     # Graph database tests
‚îÇ   ‚îî‚îÄ‚îÄ features/
‚îÇ       ‚îî‚îÄ‚îÄ graph/
‚îÇ           ‚îî‚îÄ‚îÄ repositories/
‚îÇ               ‚îî‚îÄ‚îÄ test_entity_repository_integration.py
‚îî‚îÄ‚îÄ pyproject.toml                # Dependencies & config
```

## Features

- ‚úÖ **Modular Architecture**: Feature-based organization
- ‚úÖ **Graph Database Support**: PostgreSQL AGE Graph Database
- ‚úÖ **Database Integration**: PostgreSQL AGE accessed via native SQL queries
- ‚úÖ **Authentication**: JWT-based auth with password hashing
- ‚úÖ **Test Suite**: Comprehensive test coverage with separated test directory
- ‚úÖ **Modern Python**: Type hints, async/await, Pydantic v2, SQLModel
- ‚úÖ **Development Tools**: Ruff linting/formatting, basedpyright type checking, pytest

## Getting Started

### Prerequisites

- Python 3.12+
- PostgreSQL with AGE extension (required, for graph database features)
- `uv` package manager

### Installation

1. **Clone and setup**:

   ```bash
   git clone <repository>
   cd nous-api
   ```

2. **Install dependencies using uv**:

   ```bash
   uv sync
   ```

3. **Set up environment variables**:
   ```bash
   cp .env.example .env
   # Edit .env with your database credentials
   ```

### Running the Application

**Development server (recommended)**:

```bash
uv run fastapi dev app/main.py
```

**Alternative methods**:

```bash
# Using uvicorn directly
uv run uvicorn app.main:app --reload

# Using the main module
uv run python -m app.main
```

The API will be available at:

- **API**: http://localhost:8000
- **Docs**: http://localhost:8000/docs
- **Health**: http://localhost:8000/health

### Testing

The test suite uses a dedicated test database to ensure complete isolation from your development database. Tests automatically create and tear down the test database, making them safe to run at any time.

#### Test Database Isolation

Tests use a separate PostgreSQL database (`multimodel_db_test` by default) with the following features:

- **Automatic setup**: Test database is created automatically when tests run
- **Schema sync**: Tables are created from SQLAlchemy models (no migrations needed)
- **AGE support**: AGE extension is installed and configured automatically
- **Clean slate**: All data is cleaned between tests
- **Safe teardown**: Test database is dropped after tests complete

Your development database (`multimodel_db`) is never touched by tests.

#### Configuration

The test database is configured via environment variables or settings:

```bash
# Optional: Create .env.test for custom test configuration
TESTING=true
POSTGRES_DB=multimodel_db_test
POSTGRES_USER=admin
POSTGRES_PASSWORD=supersecretpassword
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
```

The `TESTING=true` flag automatically switches to the test database.

#### Running Tests

Run all tests:

```bash
uv run pytest tests/
```

Run tests with coverage:

```bash
uv run pytest tests/ --cov=app --cov-report=html
```

Run specific test file:

```bash
uv run pytest tests/features/auth/usecases/test_signup_tenant_usecase_integration.py -v
```

Run only integration tests:

```bash
uv run pytest tests/ -m asyncio -v
```

#### Integration Tests

Integration tests use real database connections and test the full stack:

```bash
# Run all graph repository integration tests
uv run pytest tests/features/graph/repositories/test_age_repository_integration.py -v

# Run auth integration tests
uv run pytest tests/features/auth/usecases/test_signup_tenant_usecase_integration.py -v

# Run specific test with detailed output
uv run pytest tests/features/graph/usecases/test_assimilate_knowledge_usecase_integration.py::TestAssimilateKnowledgeUseCaseIntegration::test_assimilate_knowledge_basic -v -s
```

**How It Works**:

- Shared fixtures in `tests/conftest.py` manage the test database lifecycle
- Each test gets a clean graph (AGE) state via autouse fixtures
- Tables are created once per test session from models
- Test database is automatically dropped after all tests complete

### Development Tools

**Format and lint code**:

```bash
uv run ruff format app/
uv run ruff check app/ tests/
```

**Fix linting issues automatically**:

```bash
uv run ruff check app/ tests/ --fix
```

**Type checking**:

```bash
uv run basedpyright app/ tests/
```

## API Endpoints

### Authentication

- `POST /api/v1/auth/token` - Login and get access token

### Users

- `POST /api/v1/users/` - Create new user
- `GET /api/v1/users/me` - Get current user info
- `GET /api/v1/users/` - List users (authenticated)
- `GET /api/v1/users/{user_id}` - Get user by ID
- `PUT /api/v1/users/{user_id}` - Update user
- `DELETE /api/v1/users/{user_id}` - Delete user
- `POST /api/v1/users/{user_id}/friends/{friend_id}` - Add friend
- `GET /api/v1/users/{user_id}/friends` - Get user's friends

### Health

- `GET /health` - Health check

## Database Setup

### PostgreSQL AGE Graph Database

1. **Setup PostgreSQL with AGE extension** (required):

   The application uses PostgreSQL with the AGE extension for graph database functionality.

   ```bash
   # Install PostgreSQL and AGE extension (see compose/postgres/ for Docker setup)
   # Or use a PostgreSQL service with AGE extension installed
   ```

2. **Database Migrations**:

   This project uses Alembic for database schema management. See the [migration documentation](migrations/README.md) for detailed instructions on creating, applying, and managing database migrations.

3. **Update connection settings in `.env`**:
   ```env
   POSTGRES_USER=admin
   POSTGRES_PASSWORD=supersecretpassword
   POSTGRES_HOST=localhost
   POSTGRES_PORT=5432
   POSTGRES_DB=multimodel_db
   AGE_GRAPH_NAME=nous
   ```

## Development

### Adding New Features

1. **Create feature directory**:

   ```bash
   mkdir -p app/features/your_feature
   ```

2. **Add the standard files**:

   - `router.py` - API endpoints
   - `service.py` - Business logic
   - `schemas.py` - Pydantic models
   - `models.py` - SQLModel database models
   - `graph_models.py` - PostgreSQL AGE models (if needed)

3. **Add corresponding tests in `tests/` directory**:

   Create the test directory structure:

   ```bash
   mkdir -p tests/features/your_feature
   ```

   Add test files:

   - `tests/features/your_feature/test_router.py`
   - `tests/features/your_feature/test_service.py`

4. **Include router in main app**:
   ```python
   from app.features.your_feature.router import router
   app.include_router(router, prefix="/api/v1")
   ```

### Environment Variables

All configuration is handled through environment variables. See `.env.example` for available options.

## Contributing

1. Follow the naming conventions in [docs/naming_convention.md](docs/naming_convention.md)
2. Write tests for new features in the `tests/` directory (mirroring the `app/` structure)
3. Use type hints for all functions
4. Follow the modular architecture patterns
5. Use `uv` for dependency management
</file>

<file path="apps/web/src/router/index.ts">
import { createRouter, createWebHistory } from "vue-router";
import { graphRoutes } from "@/features/graph/routes";
import { loginRoutes } from "@/features/login/routes";

const router = createRouter({
  history: createWebHistory(),
  routes: [...graphRoutes, ...loginRoutes],
});

export default router;
</file>

<file path="apps/web/src/style.css">
@import "tailwindcss";
@import "tw-animate-css";

@custom-variant dark (&:is(.dark *));

@theme inline {
  --radius-sm: calc(var(--radius) - 4px);
  --radius-md: calc(var(--radius) - 2px);
  --radius-lg: var(--radius);
  --radius-xl: calc(var(--radius) + 4px);
  --color-background: var(--background);
  --color-foreground: var(--foreground);
  --color-card: var(--card);
  --color-card-foreground: var(--card-foreground);
  --color-popover: var(--popover);
  --color-popover-foreground: var(--popover-foreground);
  --color-primary: var(--primary);
  --color-primary-foreground: var(--primary-foreground);
  --color-secondary: var(--secondary);
  --color-secondary-foreground: var(--secondary-foreground);
  --color-muted: var(--muted);
  --color-muted-foreground: var(--muted-foreground);
  --color-accent: var(--accent);
  --color-accent-foreground: var(--accent-foreground);
  --color-destructive: var(--destructive);
  --color-destructive-foreground: var(--destructive-foreground);
  --color-border: var(--border);
  --color-input: var(--input);
  --color-ring: var(--ring);
  --color-chart-1: var(--chart-1);
  --color-chart-2: var(--chart-2);
  --color-chart-3: var(--chart-3);
  --color-chart-4: var(--chart-4);
  --color-chart-5: var(--chart-5);
  --color-sidebar: var(--sidebar);
  --color-sidebar-foreground: var(--sidebar-foreground);
  --color-sidebar-primary: var(--sidebar-primary);
  --color-sidebar-primary-foreground: var(--sidebar-primary-foreground);
  --color-sidebar-accent: var(--sidebar-accent);
  --color-sidebar-accent-foreground: var(--sidebar-accent-foreground);
  --color-sidebar-border: var(--sidebar-border);
  --color-sidebar-ring: var(--sidebar-ring);
}

:root {
  --radius: 0.625rem;
  --background: oklch(1 0 0);
  --foreground: oklch(0.145 0 0);
  --card: oklch(1 0 0);
  --card-foreground: oklch(0.145 0 0);
  --popover: oklch(1 0 0);
  --popover-foreground: oklch(0.145 0 0);
  --primary: oklch(0.205 0 0);
  --primary-foreground: oklch(0.985 0 0);
  --secondary: oklch(0.97 0 0);
  --secondary-foreground: oklch(0.205 0 0);
  --muted: oklch(0.97 0 0);
  --muted-foreground: oklch(0.556 0 0);
  --accent: oklch(0.97 0 0);
  --accent-foreground: oklch(0.205 0 0);
  --destructive: oklch(0.577 0.245 27.325);
  --destructive-foreground: oklch(0.577 0.245 27.325);
  --border: oklch(0.922 0 0);
  --input: oklch(0.922 0 0);
  --ring: oklch(0.708 0 0);
  --chart-1: oklch(0.646 0.222 41.116);
  --chart-2: oklch(0.6 0.118 184.704);
  --chart-3: oklch(0.398 0.07 227.392);
  --chart-4: oklch(0.828 0.189 84.429);
  --chart-5: oklch(0.769 0.188 70.08);
  --sidebar: oklch(0.985 0 0);
  --sidebar-foreground: oklch(0.145 0 0);
  --sidebar-primary: oklch(0.205 0 0);
  --sidebar-primary-foreground: oklch(0.985 0 0);
  --sidebar-accent: oklch(0.97 0 0);
  --sidebar-accent-foreground: oklch(0.205 0 0);
  --sidebar-border: oklch(0.922 0 0);
  --sidebar-ring: oklch(0.708 0 0);
}

.dark {
  --background: oklch(0.145 0 0);
  --foreground: oklch(0.985 0 0);
  --card: oklch(0.145 0 0);
  --card-foreground: oklch(0.985 0 0);
  --popover: oklch(0.145 0 0);
  --popover-foreground: oklch(0.985 0 0);
  --primary: oklch(0.985 0 0);
  --primary-foreground: oklch(0.205 0 0);
  --secondary: oklch(0.269 0 0);
  --secondary-foreground: oklch(0.985 0 0);
  --muted: oklch(0.269 0 0);
  --muted-foreground: oklch(0.708 0 0);
  --accent: oklch(0.269 0 0);
  --accent-foreground: oklch(0.985 0 0);
  --destructive: oklch(0.396 0.141 25.723);
  --destructive-foreground: oklch(0.637 0.237 25.331);
  --border: oklch(0.269 0 0);
  --input: oklch(0.269 0 0);
  --ring: oklch(0.439 0 0);
  --chart-1: oklch(0.488 0.243 264.376);
  --chart-2: oklch(0.696 0.17 162.48);
  --chart-3: oklch(0.769 0.188 70.08);
  --chart-4: oklch(0.627 0.265 303.9);
  --chart-5: oklch(0.645 0.246 16.439);
  --sidebar: oklch(0.205 0 0);
  --sidebar-foreground: oklch(0.985 0 0);
  --sidebar-primary: oklch(0.488 0.243 264.376);
  --sidebar-primary-foreground: oklch(0.985 0 0);
  --sidebar-accent: oklch(0.269 0 0);
  --sidebar-accent-foreground: oklch(0.985 0 0);
  --sidebar-border: oklch(0.269 0 0);
  --sidebar-ring: oklch(0.439 0 0);
}

@layer base {
  * {
    @apply border-border outline-ring/50;
  }
  body {
    @apply bg-background text-foreground;
  }
}
</file>

<file path="apps/web/vite.config.ts">
import { defineConfig } from "vite";
import vue from "@vitejs/plugin-vue";
import tailwindcss from "@tailwindcss/vite";
import path from "path";

// https://vite.dev/config/
export default defineConfig({
  plugins: [vue(), tailwindcss()],
  resolve: {
    alias: {
      "@": path.resolve(__dirname, "./src"),
    },
  },
});
</file>

<file path="apps/api/app/core/settings.py">
"""Application settings and configuration."""

from functools import lru_cache

from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    """Application settings loaded from environment variables."""

    model_config = SettingsConfigDict(  # pyright: ignore[reportUnannotatedClassAttribute]
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="ignore",
    )

    # Application
    app_name: str = Field(default="Nous API", description="Application name")
    app_version: str = Field(default="0.1.0", description="Application version")
    debug: bool = Field(default=False, description="Debug mode")
    testing: bool = Field(default=False, description="Testing mode")
    host: str = Field(default="0.0.0.0", description="Host to bind to")
    port: int = Field(default=8000, description="Port to bind to")

    # CORS
    allowed_origins: list[str] = Field(
        default=[
            "http://localhost:3000",
            "http://localhost:8080",
            "http://localhost:5173",
        ],
        description="Allowed CORS origins",
    )

    # Security
    secret_key: str = Field(
        default="your-secret-key-change-in-production",
        description="Secret key for JWT tokens",
    )
    algorithm: str = Field(default="HS256", description="JWT algorithm")
    access_token_expire_minutes: int = Field(
        default=30, description="Access token expiration time in minutes"
    )

    # PostgreSQL Database
    postgres_user: str = Field(default="admin", description="PostgreSQL user")
    postgres_password: str = Field(
        default="supersecretpassword", description="PostgreSQL password"
    )
    postgres_host: str = Field(default="localhost", description="PostgreSQL host")
    postgres_port: int = Field(default=5432, description="PostgreSQL port")
    postgres_db: str = Field(
        default="multimodel_db", description="PostgreSQL database name"
    )
    test_postgres_db: str = Field(
        default="multimodel_db_test", description="PostgreSQL test database name"
    )
    age_graph_name: str = Field(default="nous", description="AGE graph name")

    # Database URL (computed property)
    @property
    def database_url(self) -> str:
        """Construct database URL from individual components."""
        db_name = self.test_postgres_db if self.testing else self.postgres_db
        return (
            f"postgresql+asyncpg://{self.postgres_user}:{self.postgres_password}"
            f"@{self.postgres_host}:{self.postgres_port}/{db_name}"
        )

    # Google AI
    google_api_key: str | None = Field(
        default=None, description="Google AI API key for Gemini model"
    )


@lru_cache
def get_settings() -> Settings:
    """Get cached application settings."""
    return Settings()
</file>

<file path="apps/api/app/features/auth/dtos/__init__.py">
"""Authentication and authorization DTOs package.

This package contains all Data Transfer Objects for API responses
and requests in the authentication feature.
"""

from .auth_dto import (
    ApiKeyInfo,
    CreateApiKeyRequest,
    CreateApiKeyResponse,
    CreateTenantRequest,
    CreateTenantResponse,
    CreateUserRequest,
    CreateUserResponse,
    ListApiKeysResponse,
    LoginRequest,
    LoginResponse,
)

__all__ = [
    "CreateTenantRequest",
    "CreateTenantResponse",
    "CreateUserRequest",
    "CreateUserResponse",
    "LoginRequest",
    "LoginResponse",
    "CreateApiKeyRequest",
    "CreateApiKeyResponse",
    "ApiKeyInfo",
    "ListApiKeysResponse",
]
</file>

<file path="apps/api/app/features/auth/dtos/auth_dto.py">
"""Authentication and authorization data transfer objects."""

from datetime import datetime

from pydantic import BaseModel

from app.core.schemas import UserRole


class CreateTenantRequest(BaseModel):
    """Request model for tenant creation."""

    name: str
    email: str
    password: str


class CreateTenantResponse(BaseModel):
    """Response model for successful tenant creation."""

    message: str
    tenant_id: str
    user_id: str


class LoginRequest(BaseModel):
    """Request model for user login."""

    email: str
    password: str


class LoginResponse(BaseModel):
    """Response model for successful login."""

    access_token: str
    token_type: str = "bearer"
    expires_in: int


class CreateApiKeyRequest(BaseModel):
    """Request model for creating an API key."""

    name: str


class CreateApiKeyResponse(BaseModel):
    """Response model for successful API key creation."""

    message: str
    api_key: str
    key_prefix: str
    expires_at: str | None


class ApiKeyInfo(BaseModel):
    """Information about an API key."""

    id: str
    name: str
    key_prefix: str
    created_at: datetime
    expires_at: datetime | None
    last_used_at: datetime | None


class ListApiKeysResponse(BaseModel):
    """Response model for listing API keys."""

    api_keys: list[ApiKeyInfo]


class CreateUserRequest(BaseModel):
    """Request model for creating a new user."""

    email: str
    password: str
    role: UserRole


class CreateUserResponse(BaseModel):
    """Response model for successful user creation."""

    message: str
    user_id: str
    email: str
    role: UserRole
</file>

<file path="apps/api/app/features/auth/usecases/create_api_key_usecase.py">
"""Use case for creating API keys."""

import secrets
from contextlib import AbstractAsyncContextManager
from datetime import UTC, datetime, timedelta
from typing import Callable
from uuid import UUID

from fastapi import HTTPException, status
from sqlalchemy.exc import IntegrityError
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.authentication import get_password_hash
from app.features.auth.dtos import CreateApiKeyRequest, CreateApiKeyResponse
from app.features.auth.models import ApiKey


class CreateApiKeyUseCaseImpl:
    """Implementation of the create API key use case."""

    def __init__(
        self,
        get_db_session: Callable[[], AbstractAsyncContextManager[AsyncSession]],
    ):
        """Initialize the use case with dependencies.

        Args:
            get_db_session: Function to get database session
        """
        self.get_auth_db_session = get_db_session

    async def execute(
        self, request: CreateApiKeyRequest, tenant_id: UUID
    ) -> CreateApiKeyResponse:
        """Create a new API key for programmatic access.

        Args:
            request: The API key creation request
            tenant_id: The tenant ID creating the key

        Returns:
            Response with the created API key details

        Raises:
            HTTPException: With appropriate status codes for validation and creation errors
        """
        # Validate input
        if len(request.name) < 3 or len(request.name) > 50:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="API key name must be between 3 and 50 characters",
            )

        # Generate a secure key
        full_key, key_prefix = self._generate_api_key()

        # Hash the key for storage
        hashed_key = get_password_hash(full_key)

        async with self.get_auth_db_session() as session:
            async with session.begin():
                try:
                    # Create API key record
                    api_key_obj = ApiKey(
                        name=request.name,
                        key_prefix=key_prefix,
                        hashed_key=hashed_key,
                        tenant_id=tenant_id,
                        expires_at=datetime.now(UTC)
                        + timedelta(days=365),  # 1 year expiry
                    )
                    session.add(api_key_obj)
                    await session.flush()

                    return CreateApiKeyResponse(
                        message="API key created successfully",
                        api_key=full_key,  # Return the plaintext key once
                        key_prefix=key_prefix,
                        expires_at=api_key_obj.expires_at.isoformat()
                        if api_key_obj.expires_at
                        else None,
                    )

                except IntegrityError:
                    await session.rollback()
                    raise HTTPException(
                        status_code=status.HTTP_400_BAD_REQUEST,
                        detail="API key name already exists for this tenant",
                    )
                except Exception:
                    await session.rollback()
                    raise HTTPException(
                        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                        detail="Failed to create API key",
                    )

    def _generate_api_key(self) -> tuple[str, str]:
        """Generate a secure API key with prefix for identification.

        Returns:
            Tuple of (full_key, prefix)
        """
        # Generate a short prefix for easy identification (10 chars)
        prefix = secrets.token_hex(5)

        # Generate the main key (43 chars when urlsafe base64 encoded)
        key = secrets.token_urlsafe(32)

        # Combine: e.g., "a1b2c3d4e5.f8jK9mNp2qRs5tUv7wX..."
        full_key = f"{prefix}.{key}"

        return full_key, prefix
</file>

<file path="apps/api/app/features/auth/usecases/list_api_keys_usecase.py">
"""Use case for listing API keys."""

from contextlib import AbstractAsyncContextManager
from typing import Callable
from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.features.auth.dtos import ApiKeyInfo, ListApiKeysResponse
from app.features.auth.models import ApiKey


class ListApiKeysUseCaseImpl:
    """Implementation of the list API keys use case."""

    def __init__(
        self, get_db_session: Callable[[], AbstractAsyncContextManager[AsyncSession]]
    ):
        """Initialize the use case with dependencies.

        Args:
            get_db_session: Function to get database session
        """
        self.get_auth_db_session = get_db_session

    async def execute(self, tenant_id: UUID) -> ListApiKeysResponse:
        """List all API keys for a tenant.

        Args:
            tenant_id: The tenant ID to list keys for

        Returns:
            Response with list of API keys
        """
        async with self.get_auth_db_session() as session:
            result = await session.execute(
                select(ApiKey).where(ApiKey.tenant_id == tenant_id)
            )
            api_keys = result.scalars().all()

            api_key_infos = [
                ApiKeyInfo(
                    id=str(api_key.id),
                    name=api_key.name,
                    key_prefix=api_key.key_prefix,
                    created_at=api_key.created_at,
                    expires_at=api_key.expires_at,
                    last_used_at=api_key.last_used_at,
                )
                for api_key in api_keys
            ]

            return ListApiKeysResponse(api_keys=api_key_infos)
</file>

<file path="apps/web/src/features/login/views/LoginView.vue">
<script setup lang="ts">
import { ref } from "vue";
import { useRouter } from "vue-router";
import {
  Card,
  CardContent,
  CardDescription,
  CardHeader,
  CardTitle,
} from "@/components/ui/card";
import { Input } from "@/components/ui/input";
import { Button } from "@/components/ui/button";
import { Alert, AlertDescription } from "@/components/ui/alert";

const router = useRouter();

// Reactive data
const email = ref("");
const password = ref("");
const loading = ref(false);
const errorMessage = ref("");

// Methods
const handleLogin = async () => {
  if (!email.value || !password.value) {
    errorMessage.value = "Please fill in all fields";
    return;
  }

  loading.value = true;
  errorMessage.value = "";

  try {
    // TODO: Implement actual login logic
    console.log("Login attempt:", {
      email: email.value,
      password: password.value,
    });

    // Simulate API call
    await new Promise((resolve) => setTimeout(resolve, 2000));

    // Mock successful login - set authentication state
    localStorage.setItem("isLoggedIn", "true");

    // Redirect to home after successful login
    router.push("/");
  } catch (error) {
    errorMessage.value = "Login failed. Please try again.";
  } finally {
    loading.value = false;
  }
};
</script>

<template>
  <div class="flex justify-center items-center min-h-screen bg-background p-8">
    <Card class="w-full max-w-sm">
      <CardHeader>
        <CardTitle>Welcome Back</CardTitle>
        <CardDescription>Enter your credentials to sign in.</CardDescription>
      </CardHeader>
      <CardContent>
        <form @submit.prevent="handleLogin" class="flex flex-col gap-6">
          <div class="flex flex-col gap-2">
            <label for="email">Email</label>
            <Input
              id="email"
              v-model="email"
              type="email"
              placeholder="Enter your email"
              class="w-full"
              required
            />
          </div>

          <div class="flex flex-col gap-2">
            <label for="password">Password</label>
            <Input
              id="password"
              v-model="password"
              type="password"
              placeholder="Enter your password"
              class="w-full"
              required
            />
          </div>

          <Alert v-if="errorMessage" variant="destructive">
            <AlertDescription>
              {{ errorMessage }}
            </AlertDescription>
          </Alert>

          <Button type="submit" :disabled="loading" size="lg">
            <span v-if="loading">Signing in...</span>
            <span v-else>Sign In</span>
          </Button>
        </form>
      </CardContent>
    </Card>
  </div>
</template>
</file>

<file path="README.md">
# Nous Monorepo

Welcome to the `nous` project monorepo. This repository contains all the code for the Nous AI Agent Memory System, including the FastAPI backend (`api`) and the Vue.js frontend (`web`).

This project is managed as a **polyglot (Python + TypeScript) monorepo** using **pnpm Workspaces** and **Turborepo**.

- **`pnpm`** manages the JavaScript dependencies and workspaces.
- **`Turborepo`** orchestrates tasks (like `dev`, `test`, `lint`) across all projects.
- **`uv`** manages the Python dependencies and virtual environment for the `api`.

## Prerequisites

Before you begin, ensure you have the following tools installed:

1.  **Node.js**: Version `22.12.0` or higher is required. We recommend using a version manager like [nvm](https://github.com/nvm-sh/nvm) to easily switch between Node.js versions.
2.  **pnpm**: The recommended way to install is via `corepack` (which comes with Node.js):
    ```bash
    corepack enable
    ```
3.  **uv**: The Python package manager used by the `api`.
    ```bash
    # (macOS/Linux)
    curl -LsSf https://astral.sh/uv/install.sh | sh
    # (Windows)
    powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
    ```

## 1. Initial Setup

To get your environment ready, run these commands from the root (`nous/`) directory:

1.  **Install Node.js Dependencies:**
    This command installs all Node.js dependencies for the entire monorepo, including `turbo` at the root and all dependencies for each application inside `apps/` (like `vite` for `web`). It also links the workspace packages together.

    ```bash
    pnpm install
    ```

2.  **Set up Python Environment:**
    You only need to do this once for the `api` project.

    ```bash
    # 1. Navigate to the api directory
    cd apps/api

    # 2. Create a virtual environment
    uv venv

    # 3. Install all Python dependencies
    uv sync

    # 4. Go back to the root
    cd ../..
    ```

3.  **Set up Environment Variables:**
    The API requires a `.env` file for its configuration.

    ```bash
    # From the root directory:
    cp apps/api/.env.example apps/api/.env

    # Now, edit apps/api/.env with your database credentials.
    ```

## 2. Development Workflow

All commands should be run from the **root of the monorepo**.

### How to Start the API

This command uses Turborepo to find the `api` project and run its `dev` script.

```bash
pnpm turbo dev --filter=api
```

Your FastAPI server will be running on `http://localhost:8000`.

---

### How to Start the Web-App

This command uses Turborepo to find the `web` project and run its `dev` script.

```bash
pnpm turbo dev --filter=web
```

Your Vue.js app will be running on `http://localhost:5173` (or the next available port).

---

### How to Start Both (API + Web)

This is the most common command you'll use. Turborepo finds _all_ projects with a `dev` script and runs them in parallel.

```bash
pnpm turbo dev
```

---

### How to Run Tests

This command runs the `test` script in all projects.

```bash
pnpm turbo test
```

> **Note:** The `apps/web` project currently has a placeholder test script. This command will primarily run the **Python tests** for the `api`.

---

### How to Run Linting

This command runs the `lint` script in all projects (`ruff` for the API, `eslint` for the web app).

```bash
pnpm turbo lint
```

## 3. What Else Can I Do with Turborepo?

Turborepo is more than just a task runner; it's a build system that makes your monorepo fast and efficient.

### ‚ö°Ô∏è Caching (The "Turbo")

This is Turborepo's killer feature.

- **What it does:** Turborepo caches the output and logs of your tasks (like `test`, `lint`, and `build`).
- **Why it's great:** If you run `pnpm turbo test`, and then run it again without changing any files in `apps/api`, Turborepo will **skip** running the tests and show you the cached result instantly. This saves a massive amount of time, especially in CI/CD pipelines.

### üéØ Filtering

You've already used this\! The `--filter` flag lets you run tasks on a single project or a subset of projects.

```bash
# Run `build` only on the `web` app
pnpm turbo build --filter=web
```

### üèéÔ∏è Parallel Execution

When you run a command like `pnpm turbo dev` or `pnpm turbo lint`, Turborepo reads your `turbo.json` and understands that these tasks can be run in parallel, maximizing your CPU usage and finishing faster.

## 4. High-Level Directory Structure

```plaintext
nous/
‚îú‚îÄ‚îÄ apps/
‚îÇ   ‚îú‚îÄ‚îÄ api/          # FastAPI (Python) backend
‚îÇ   ‚îî‚îÄ‚îÄ web/          # Vue.js (TypeScript) frontend
‚îú‚îÄ‚îÄ package.json      # Root Node.js dependencies (contains `turbo`)
‚îú‚îÄ‚îÄ pnpm-workspace.yaml # Defines the `apps/*` as pnpm workspaces
‚îî‚îÄ‚îÄ turbo.json        # Defines the monorepo task pipeline
```
</file>

<file path="apps/api/app/features/auth/usecases/delete_api_key_usecase.py">
"""Use case for deleting API keys."""

from contextlib import AbstractAsyncContextManager
from typing import Callable
from uuid import UUID

from fastapi import HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession

from app.features.auth.models import ApiKey


class DeleteApiKeyUseCaseImpl:
    """Implementation of the delete API key use case."""

    def __init__(
        self, get_db_session: Callable[[], AbstractAsyncContextManager[AsyncSession]]
    ):
        """Initialize the use case with dependencies.

        Args:
            get_db_session: Function to get database session
        """
        self.get_auth_db_session = get_db_session

    async def execute(self, api_key_id: str, tenant_id: UUID) -> dict[str, str]:
        """Delete an API key.

        Args:
            api_key_id: The ID of the API key to delete
            tenant_id: The tenant ID for authorization

        Returns:
            Success message

        Raises:
            HTTPException: With appropriate status codes for validation and access errors
        """
        try:
            uuid_obj = UUID(api_key_id)
        except ValueError:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid API key ID format",
            )

        async with self.get_auth_db_session() as session:
            async with session.begin():
                api_key = await session.get(ApiKey, uuid_obj)

                if not api_key:
                    raise HTTPException(
                        status_code=status.HTTP_404_NOT_FOUND,
                        detail="API key not found",
                    )

                # Ensure the API key belongs to the current tenant
                if api_key.tenant_id != tenant_id:
                    raise HTTPException(
                        status_code=status.HTTP_403_FORBIDDEN, detail="Access denied"
                    )

                await session.delete(api_key)

        return {"message": "API key deleted successfully"}
</file>

<file path="apps/api/app/features/auth/usecases/login_usecase.py">
"""Use case for user login and token generation."""

from contextlib import AbstractAsyncContextManager
from datetime import UTC, datetime, timedelta
from typing import Any, Callable, Protocol

from fastapi import HTTPException, status
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.features.auth.dtos import LoginResponse
from app.features.auth.models import User


class PasswordVerifier(Protocol):
    """Protocol for password verification operations."""

    def verify(self, plain_password: str, hashed_password: str) -> bool:
        """Verify a password against its hash."""
        ...


class TokenCreator(Protocol):
    """Protocol for JWT token creation operations."""

    def __call__(
        self, data: dict[str, Any], expires_delta: timedelta | None = None
    ) -> str:
        """Create an access token."""
        ...


class LoginUseCaseImpl:
    """Implementation of the login use case."""

    def __init__(
        self,
        password_verifier: PasswordVerifier,
        token_creator: TokenCreator,
        get_db_session: Callable[[], AbstractAsyncContextManager[AsyncSession]],
    ):
        """Initialize the use case with dependencies.

        Args:
            password_verifier: Service for verifying passwords
            token_creator: Service for creating access tokens
            get_db_session: Function to get database session
        """
        self.password_verifier = password_verifier
        self.token_creator = token_creator
        self.get_auth_db_session = get_db_session

    async def execute(self, email: str, password: str) -> LoginResponse:
        """Authenticate user and return JWT access token.

        Args:
            email: User's email address
            password: User's password

        Returns:
            Response with access token

        Raises:
            HTTPException: With appropriate status codes for authentication errors
        """
        async with self.get_auth_db_session() as session:
            # Find user by email
            result = await session.execute(select(User).where(User.email == email))
            user: User | None = result.scalar_one_or_none()

            if not user or not self.password_verifier.verify(
                password, user.hashed_password
            ):
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Incorrect email or password",
                    headers={"WWW-Authenticate": "Bearer"},
                )

            # Check if account is locked
            if user.locked_until and user.locked_until > datetime.now(UTC):
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Account is temporarily locked due to failed login attempts",
                    headers={"WWW-Authenticate": "Bearer"},
                )

            # Check if user is active
            if not user.is_active:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Account is disabled",
                    headers={"WWW-Authenticate": "Bearer"},
                )

            # Reset failed login attempts and update user
            user.failed_login_attempts = 0
            user.locked_until = None
            await session.commit()

            # Create access token with tenant info
            access_token_expires = timedelta(minutes=30)  # 30 minutes

            # Convert tenant_id to string only if it exists
            tenant_id_str = str(user.tenant_id) if user.tenant_id else None

            access_token = self.token_creator(
                data={
                    "sub": str(user.id),
                    "tenant_id": tenant_id_str,  # <-- This can be None
                    "role": user.role.value,  # <-- Add this
                },
                expires_delta=access_token_expires,
            )

            return LoginResponse(
                access_token=access_token,
                token_type="bearer",
                expires_in=int(access_token_expires.total_seconds()),
            )
</file>

<file path="apps/api/app/features/auth/router.py">
"""Authentication and authorization API routes."""

from fastapi import APIRouter

from app.features.auth.routes.api_keys import router as api_keys_router
from app.features.auth.routes.login import router as login_router
from app.features.auth.routes.tenants import router as tenants_router
from app.features.auth.routes.users import router as users_router

router = APIRouter(prefix="/auth", tags=["auth"])

# Include all route handlers
router.include_router(tenants_router)
router.include_router(login_router)
router.include_router(api_keys_router)
router.include_router(users_router)
</file>

<file path="apps/api/app/features/graph/router.py">
"""Graph database API routes - main router that includes all entity-specific route modules."""

from fastapi import APIRouter, Depends

from app.core.authorization import get_tenant_info
from app.features.graph.routes.assimilate import router as assimilate_router
from app.features.graph.routes.lookup import router as lookup_router

router = APIRouter(
    prefix="/graph",
    tags=["graph"],
    dependencies=[Depends(get_tenant_info)],
)

# Include all route handlers
router.include_router(assimilate_router)
router.include_router(lookup_router)
</file>

<file path="apps/api/app/main.py">
"""Main FastAPI application entry point."""

from collections.abc import AsyncGenerator
from contextlib import asynccontextmanager

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from app.core.settings import get_settings
from app.db.postgres.auth_session import init_auth_db_session
from app.db.postgres.graph_connection import close_graph_db_pool, get_graph_db_pool
from app.features.auth.router import router as auth_router
from app.features.graph.router import router as graph_router


@asynccontextmanager
async def lifespan(_app: FastAPI) -> AsyncGenerator[None, None]:
    """Application lifespan context manager.
    Handles startup and shutdown events.
    """
    # Startup
    settings = get_settings()
    print(f"Starting {settings.app_name} v{settings.app_version}")

    # Initialize database connections
    init_auth_db_session()
    print("Auth database session initialized.")
    _ = await get_graph_db_pool()
    print("Graph database connection pool created.")

    yield

    # Shutdown
    print("Shutting down application")
    await close_graph_db_pool()
    print("Database connection pool closed.")


def create_app() -> FastAPI:
    """Create and configure the FastAPI application."""
    settings = get_settings()

    app = FastAPI(
        title=settings.app_name,
        version=settings.app_version,
        description="FastAPI application with modular architecture",
        lifespan=lifespan,
    )

    # Add CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=settings.allowed_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Include routers
    app.include_router(auth_router, prefix="/api/v1")
    app.include_router(graph_router, prefix="/api/v1")

    @app.get("/health")
    async def health_check() -> dict[str, str]:  # pyright: ignore[reportUnusedFunction]
        """Health check endpoint."""
        return {"status": "healthy"}

    return app


# Create the app instance
app = create_app()


if __name__ == "__main__":
    import uvicorn

    settings = get_settings()
    uvicorn.run(
        "app.main:app",
        host=settings.host,
        port=settings.port,
        reload=settings.debug,
        log_level="info" if not settings.debug else "debug",
    )
</file>

<file path="apps/api/tests/features/graph/repositories/test_age_repository_integration.py">
"""Integration tests for AgeRepository using a real PostgreSQL/AGE connection."""

import uuid
from datetime import datetime

import asyncpg
import pytest

from app.features.graph.models import Entity, Fact, HasIdentifier, Identifier, Source
from app.features.graph.repositories.age_repository import AgeRepository


@pytest.fixture
async def age_repository(postgres_pool: asyncpg.Pool) -> AgeRepository:
    """Fixture to get an AgeRepository instance."""
    return AgeRepository(postgres_pool, graph_name="test_graph")


@pytest.fixture
def test_entity() -> Entity:
    """Test entity with integration test metadata."""
    return Entity(
        metadata={
            "test_type": "integration",
            "test_run_id": str(uuid.uuid4()),
            "created_by": "test_age_repository_integration.py",
        }
    )


@pytest.fixture
def test_identifier() -> Identifier:
    """Test identifier for integration testing."""
    return Identifier(
        value=f"test.integration.{uuid.uuid4()}@example.com", type="email"
    )


@pytest.fixture
def test_has_identifier_relationship(
    test_entity: Entity, test_identifier: Identifier
) -> HasIdentifier:
    """Test HasIdentifier relationship between entity and identifier."""
    return HasIdentifier(
        from_entity_id=test_entity.id,
        to_identifier_value=test_identifier.value,
        is_primary=True,
    )


@pytest.fixture
def test_fact() -> Fact:
    """Test fact for integration testing."""
    return Fact(
        name="Paris",
        type="Location",
    )


@pytest.fixture
def test_source() -> Source:
    """Test source for integration testing."""
    return Source(
        content="User mentioned they live in Paris during onboarding",
        timestamp=datetime.now(),
    )


class TestCreateEntity:
    """Integration tests for AgeRepository.create_entity method."""

    @pytest.mark.asyncio
    async def test_create_entity_basic(
        self,
        age_repository: AgeRepository,
        test_entity: Entity,
        test_identifier: Identifier,
        test_has_identifier_relationship: HasIdentifier,
    ) -> None:
        """Test basic entity creation with minimal data."""
        # Act
        result = await age_repository.create_entity(
            test_entity, test_identifier, test_has_identifier_relationship
        )

        # Assert
        assert isinstance(result, dict)
        assert "entity" in result
        assert "identifier" in result
        assert "relationship" in result

        returned_entity = result["entity"]
        returned_identifier = result["identifier"]
        returned_relationship = result["relationship"]

        assert isinstance(returned_entity, Entity)
        assert isinstance(returned_identifier, Identifier)
        assert isinstance(returned_relationship, HasIdentifier)

        assert returned_entity.id == test_entity.id
        assert returned_entity.metadata == test_entity.metadata
        assert returned_identifier.value == test_identifier.value
        assert returned_identifier.type == test_identifier.type
        assert (
            returned_relationship.is_primary
            == test_has_identifier_relationship.is_primary
        )

        # Verify by finding it
        found = await age_repository.find_entity_by_identifier(
            test_identifier.value, test_identifier.type
        )
        assert found is not None
        assert found["entity"].id == test_entity.id

    @pytest.mark.asyncio
    async def test_create_entity_is_idempotent(
        self,
        age_repository: AgeRepository,
        test_entity: Entity,
        test_identifier: Identifier,
        test_has_identifier_relationship: HasIdentifier,
    ) -> None:
        """Test that create_entity is idempotent."""
        # Create it once
        first_result = await age_repository.create_entity(
            test_entity, test_identifier, test_has_identifier_relationship
        )
        assert first_result["entity"].id == test_entity.id

        # Try to create it again with a different entity object but same identifier
        second_entity = Entity()
        second_relationship = HasIdentifier(
            from_entity_id=second_entity.id,
            to_identifier_value=test_identifier.value,
        )
        second_result = await age_repository.create_entity(
            second_entity, test_identifier, second_relationship
        )

        # Assert it returned the first entity
        assert second_result["entity"].id == first_result["entity"].id
        assert second_result["entity"].id == test_entity.id


class TestFindEntityByIdentifier:
    """Integration tests for AgeRepository.find_entity_by_identifier method."""

    @pytest.mark.asyncio
    async def test_find_entity_by_identifier(
        self,
        age_repository: AgeRepository,
        test_entity: Entity,
        test_identifier: Identifier,
        test_has_identifier_relationship: HasIdentifier,
    ) -> None:
        """Test finding an entity by its identifier value and type."""
        # Arrange: Create an entity first
        await age_repository.create_entity(
            test_entity, test_identifier, test_has_identifier_relationship
        )

        # Act
        found_result = await age_repository.find_entity_by_identifier(
            test_identifier.value, test_identifier.type
        )

        # Assert
        assert found_result is not None
        found_entity = found_result["entity"]
        found_identifier = found_result["identifier"]["identifier"]
        found_rel = found_result["identifier"]["relationship"]

        assert found_entity.id == test_entity.id
        assert found_identifier.value == test_identifier.value
        assert found_rel.from_entity_id == test_entity.id

    @pytest.mark.asyncio
    async def test_find_entity_by_identifier_not_found(
        self,
        age_repository: AgeRepository,
    ) -> None:
        """Test finding a non-existent entity."""
        found_result = await age_repository.find_entity_by_identifier(
            "nonexistent@example.com", "email"
        )
        assert found_result is None


class TestFindEntityById:
    """Integration tests for AgeRepository.find_entity_by_id method."""

    @pytest.mark.asyncio
    async def test_find_entity_by_id(
        self,
        age_repository: AgeRepository,
        test_entity: Entity,
        test_identifier: Identifier,
        test_has_identifier_relationship: HasIdentifier,
    ) -> None:
        """Test finding an entity by its ID."""
        # Arrange
        await age_repository.create_entity(
            test_entity, test_identifier, test_has_identifier_relationship
        )

        # Act
        found_result = await age_repository.find_entity_by_id(str(test_entity.id))

        # Assert
        assert found_result is not None
        assert found_result["entity"].id == test_entity.id
        assert found_result["identifier"] is not None
        assert found_result["identifier"]["identifier"].value == test_identifier.value

    @pytest.mark.asyncio
    async def test_find_entity_by_id_not_found(
        self,
        age_repository: AgeRepository,
    ) -> None:
        """Test finding a non-existent entity by ID."""
        found_result = await age_repository.find_entity_by_id(str(uuid.uuid4()))
        assert found_result is None


class TestDeleteEntityById:
    """Integration tests for AgeRepository.delete_entity_by_id method."""

    @pytest.mark.asyncio
    async def test_delete_entity_by_id(
        self,
        age_repository: AgeRepository,
        test_entity: Entity,
        test_identifier: Identifier,
        test_has_identifier_relationship: HasIdentifier,
    ) -> None:
        """Test deleting an entity by its ID."""
        # Arrange
        _ = await age_repository.create_entity(
            test_entity, test_identifier, test_has_identifier_relationship
        )
        # Act
        delete_result = await age_repository.delete_entity_by_id(str(test_entity.id))

        # Assert
        assert delete_result is True
        found_after = await age_repository.find_entity_by_id(str(test_entity.id))
        assert found_after is None

    @pytest.mark.asyncio
    async def test_delete_entity_by_id_not_found(
        self,
        age_repository: AgeRepository,
    ) -> None:
        """Test deleting a non-existent entity."""
        delete_result = await age_repository.delete_entity_by_id(str(uuid.uuid4()))
        assert delete_result is False

    @pytest.mark.asyncio
    async def test_delete_entity_cascades_to_unique_identifier(
        self,
        age_repository: AgeRepository,
        test_entity: Entity,
        test_identifier: Identifier,
        test_has_identifier_relationship: HasIdentifier,
    ) -> None:
        """Test that deleting an entity also deletes its unique identifier."""
        # Arrange: Create entity with identifier
        _ = await age_repository.create_entity(
            test_entity, test_identifier, test_has_identifier_relationship
        )

        # Verify identifier exists
        found_before = await age_repository.find_entity_by_identifier(
            test_identifier.value, test_identifier.type
        )
        assert found_before is not None

        # Act: Delete the entity
        delete_result = await age_repository.delete_entity_by_id(str(test_entity.id))
        assert delete_result is True

        # Assert: Identifier should also be deleted since it was only used by this entity
        found_after = await age_repository.find_entity_by_identifier(
            test_identifier.value, test_identifier.type
        )
        assert found_after is None

    @pytest.mark.asyncio
    async def test_delete_entity_does_not_affect_other_entities(
        self,
        age_repository: AgeRepository,
        test_entity: Entity,
        test_identifier: Identifier,
        test_has_identifier_relationship: HasIdentifier,
    ) -> None:
        """Test that deleting one entity doesn't affect other entities with different identifiers."""
        # Arrange: Create first entity
        _ = await age_repository.create_entity(
            test_entity, test_identifier, test_has_identifier_relationship
        )

        # Create second entity with different identifier
        second_entity = Entity()
        second_identifier = Identifier(
            value=f"second.{uuid.uuid4()}@example.com", type="email"
        )
        second_relationship = HasIdentifier(
            from_entity_id=second_entity.id,
            to_identifier_value=second_identifier.value,
        )
        _ = await age_repository.create_entity(
            second_entity, second_identifier, second_relationship
        )

        # Act: Delete the first entity
        delete_result = await age_repository.delete_entity_by_id(str(test_entity.id))
        assert delete_result is True

        # Assert: Second entity should still exist
        found_second = await age_repository.find_entity_by_identifier(
            second_identifier.value, second_identifier.type
        )
        assert found_second is not None
        assert found_second["entity"].id == second_entity.id

        # And first entity should be gone
        found_first = await age_repository.find_entity_by_identifier(
            test_identifier.value, test_identifier.type
        )
        assert found_first is None

    @pytest.mark.asyncio
    async def test_delete_entity_cascades_to_unique_facts(
        self,
        age_repository: AgeRepository,
        test_entity: Entity,
        test_identifier: Identifier,
        test_has_identifier_relationship: HasIdentifier,
        test_fact: Fact,
        test_source: Source,
    ) -> None:
        """Test that deleting an entity also deletes its unique facts and sources."""
        # Arrange: Create entity with fact and source
        _ = await age_repository.create_entity(
            test_entity, test_identifier, test_has_identifier_relationship
        )
        _ = await age_repository.add_fact_to_entity(
            entity_id=str(test_entity.id),
            fact=test_fact,
            source=test_source,
            verb="lives_in",
        )

        # Verify fact exists
        assert test_fact.fact_id is not None
        fact_before = await age_repository.find_fact_by_id(test_fact.fact_id)
        assert fact_before is not None

        # Act: Delete the entity
        delete_result = await age_repository.delete_entity_by_id(str(test_entity.id))
        assert delete_result is True

        # Assert: Fact should also be deleted since it was only used by this entity
        fact_after = await age_repository.find_fact_by_id(test_fact.fact_id)
        assert fact_after is None

    @pytest.mark.asyncio
    async def test_delete_entity_preserves_shared_facts(
        self,
        age_repository: AgeRepository,
        test_entity: Entity,
        test_identifier: Identifier,
        test_has_identifier_relationship: HasIdentifier,
        test_fact: Fact,
        test_source: Source,
    ) -> None:
        """Test that deleting an entity preserves facts shared with other entities."""
        # Arrange: Create first entity with fact
        _ = await age_repository.create_entity(
            test_entity, test_identifier, test_has_identifier_relationship
        )
        _ = await age_repository.add_fact_to_entity(
            entity_id=str(test_entity.id),
            fact=test_fact,
            source=test_source,
            verb="lives_in",
        )

        # Create second entity and add the same fact to it
        second_entity = Entity()
        second_identifier = Identifier(
            value=f"second.{uuid.uuid4()}@example.com", type="email"
        )
        second_relationship = HasIdentifier(
            from_entity_id=second_entity.id,
            to_identifier_value=second_identifier.value,
        )
        _ = await age_repository.create_entity(
            second_entity, second_identifier, second_relationship
        )
        _ = await age_repository.add_fact_to_entity(
            entity_id=str(second_entity.id),
            fact=test_fact,  # Same fact
            source=test_source,  # Same source
            verb="works_in",  # Different verb
        )

        # Act: Delete the first entity
        delete_result = await age_repository.delete_entity_by_id(str(test_entity.id))
        assert delete_result is True

        # Assert: Fact should still exist because it's used by the second entity
        assert test_fact.fact_id is not None
        fact_after = await age_repository.find_fact_by_id(test_fact.fact_id)
        assert fact_after is not None

        # And the second entity should still have the fact
        second_entity_found = await age_repository.find_entity_by_id(
            str(second_entity.id)
        )
        assert second_entity_found is not None
        assert len(second_entity_found["facts_with_sources"]) == 1
        assert (
            second_entity_found["facts_with_sources"][0]["fact"].fact_id
            == test_fact.fact_id
        )


class TestAddFactToEntity:
    """Integration tests for AgeRepository.add_fact_to_entity method."""

    @pytest.mark.asyncio
    async def test_add_fact_to_entity_basic(
        self,
        age_repository: AgeRepository,
        test_entity: Entity,
        test_identifier: Identifier,
        test_has_identifier_relationship: HasIdentifier,
        test_fact: Fact,
        test_source: Source,
    ) -> None:
        """Test basic fact addition to an entity."""
        # Arrange
        _ = await age_repository.create_entity(
            test_entity, test_identifier, test_has_identifier_relationship
        )

        # Act
        result = await age_repository.add_fact_to_entity(
            entity_id=str(test_entity.id),
            fact=test_fact,
            source=test_source,
            verb="lives_in",
            confidence_score=0.9,
        )

        # Assert
        assert result["fact"].name == test_fact.name
        assert result["source"].content == test_source.content
        assert result["has_fact_relationship"].verb == "lives_in"
        assert result["has_fact_relationship"].confidence_score == 0.9

        # Verify
        found = await age_repository.find_entity_by_id(str(test_entity.id))
        assert found is not None
        assert len(found["facts_with_sources"]) == 1
        assert found["facts_with_sources"][0]["fact"].name == test_fact.name

    @pytest.mark.asyncio
    async def test_add_fact_to_entity_is_idempotent(
        self,
        age_repository: AgeRepository,
        test_entity: Entity,
        test_identifier: Identifier,
        test_has_identifier_relationship: HasIdentifier,
        test_fact: Fact,
        test_source: Source,
    ) -> None:
        """Test that adding the same fact is idempotent."""
        # Arrange
        _ = await age_repository.create_entity(
            test_entity, test_identifier, test_has_identifier_relationship
        )
        # Act
        _ = await age_repository.add_fact_to_entity(
            entity_id=str(test_entity.id),
            fact=test_fact,
            source=test_source,
            verb="lives_in",
        )
        _ = await age_repository.add_fact_to_entity(
            entity_id=str(test_entity.id),
            fact=test_fact,
            source=test_source,
            verb="lives_in",
        )
        # Assert
        found = await age_repository.find_entity_by_id(str(test_entity.id))
        assert found is not None
        assert len(found["facts_with_sources"]) == 1


class TestFindFactById:
    """Integration tests for AgeRepository.find_fact_by_id method."""

    @pytest.mark.asyncio
    async def test_find_fact_by_id(
        self,
        age_repository: AgeRepository,
        test_entity: Entity,
        test_identifier: Identifier,
        test_has_identifier_relationship: HasIdentifier,
        test_fact: Fact,
        test_source: Source,
    ) -> None:
        """Test finding a fact by its fact_id."""
        # Arrange
        await age_repository.create_entity(
            test_entity, test_identifier, test_has_identifier_relationship
        )
        await age_repository.add_fact_to_entity(
            entity_id=str(test_entity.id),
            fact=test_fact,
            source=test_source,
            verb="lives_in",
        )

        # Act
        assert test_fact.fact_id is not None
        found = await age_repository.find_fact_by_id(test_fact.fact_id)

        # Assert
        assert found is not None
        assert found["fact"].fact_id == test_fact.fact_id
        assert found["source"] is not None
        assert found["source"].id == test_source.id

    @pytest.mark.asyncio
    async def test_find_fact_by_id_not_found(
        self,
        age_repository: AgeRepository,
    ) -> None:
        """Test finding a non-existent fact."""
        found = await age_repository.find_fact_by_id("non:existent")
        assert found is None
</file>

<file path="apps/api/tests/features/graph/usecases/test_assimilate_knowledge_usecase_integration.py">
"""Integration tests for AssimilateKnowledgeUseCaseImpl using real dependencies.

This module provides integration tests for the AssimilateKnowledgeUseCaseImpl
using the actual production implementations of AgeRepository and LangChainFactExtractor.
"""

import uuid

import asyncpg
import pytest

from app.features.graph.dtos.knowledge_dto import (
    AssimilateKnowledgeRequest,
    AssimilateKnowledgeResponse,
    IdentifierDto,
)
from app.features.graph.repositories.age_repository import AgeRepository
from app.features.graph.services.langchain_fact_extractor import LangChainFactExtractor
from app.features.graph.usecases.assimilate_knowledge_usecase import (
    AssimilateKnowledgeUseCaseImpl,
)


@pytest.fixture
async def age_repository(postgres_pool: asyncpg.Pool) -> AgeRepository:
    """Fixture to get an AgeRepository instance."""
    return AgeRepository(postgres_pool, graph_name="test_graph")


@pytest.fixture
def langchain_fact_extractor() -> LangChainFactExtractor:
    """LangChainFactExtractor instance for testing."""
    return LangChainFactExtractor()


@pytest.fixture
async def assimilate_knowledge_usecase(
    age_repository: AgeRepository,
    langchain_fact_extractor: LangChainFactExtractor,
) -> AssimilateKnowledgeUseCaseImpl:
    """AssimilateKnowledgeUseCaseImpl instance for testing."""
    return AssimilateKnowledgeUseCaseImpl(
        repository=age_repository, fact_extractor=langchain_fact_extractor
    )


@pytest.fixture
def test_identifier() -> IdentifierDto:
    """Test identifier payload for integration testing."""
    return IdentifierDto(
        value=f"test.integration.{uuid.uuid4()}@example.com", type="email"
    )


@pytest.fixture
def test_content() -> str:
    """Test content for fact extraction."""
    return "I live in Paris and work as a Software Engineer. I really enjoy hiking on weekends."


class TestAssimilateKnowledgeUseCaseIntegration:
    """Integration tests for AssimilateKnowledgeUseCaseImpl.execute method."""

    @pytest.mark.asyncio
    async def test_assimilate_knowledge_basic(
        self,
        assimilate_knowledge_usecase: AssimilateKnowledgeUseCaseImpl,
        test_identifier: IdentifierDto,
        test_content: str,
    ) -> None:
        """Test basic knowledge assimilation flow with fact extraction."""

        # Act
        request = AssimilateKnowledgeRequest(
            identifier=test_identifier,
            content=test_content,
        )

        result: AssimilateKnowledgeResponse = (
            await assimilate_knowledge_usecase.execute(request)
        )
        print(f"{result=}")

        # Assert
        assert isinstance(result, AssimilateKnowledgeResponse)
        assert result.entity is not None
        assert result.source is not None
        assert result.assimilated_facts is not None

        # Verify entity was created with correct ID
        assert result.entity.id is not None
        assert isinstance(result.entity.metadata, dict)

        # Verify source was created correctly
        assert result.source.id is not None
        assert result.source.content == test_content
        assert result.source.timestamp is not None  # Should be auto-generated

        # Verify facts were extracted and assimilated
        # The exact facts depend on the LLM output, but we expect some facts
        assert len(result.assimilated_facts) > 0

        # Check structure of assimilated facts
        for assimilated_fact in result.assimilated_facts:
            assert assimilated_fact.fact is not None
            assert assimilated_fact.fact.name is not None
            assert assimilated_fact.fact.type is not None
            assert assimilated_fact.fact.fact_id is not None
            assert assimilated_fact.relationship is not None
            assert assimilated_fact.relationship.verb is not None
            assert 0.0 <= assimilated_fact.relationship.confidence_score <= 1.0
            assert assimilated_fact.relationship.created_at is not None

    @pytest.mark.asyncio
    async def test_assimilate_knowledge_creates_new_entity(
        self,
        assimilate_knowledge_usecase: AssimilateKnowledgeUseCaseImpl,
        test_identifier: IdentifierDto,
        test_content: str,
    ) -> None:
        """Test that assimilate knowledge creates a new entity when identifier doesn't exist."""

        # Act
        request = AssimilateKnowledgeRequest(
            identifier=test_identifier,
            content=test_content,
        )

        result = await assimilate_knowledge_usecase.execute(request)

        # Assert
        assert result.entity is not None
        assert result.entity.id is not None

        # Verify the entity can be found in the database
        found_entity = (
            await assimilate_knowledge_usecase.repository.find_entity_by_identifier(
                test_identifier.value, test_identifier.type
            )
        )
        assert found_entity is not None
        assert found_entity["entity"].id == result.entity.id

    @pytest.mark.asyncio
    async def test_assimilate_knowledge_reuses_existing_entity(
        self,
        assimilate_knowledge_usecase: AssimilateKnowledgeUseCaseImpl,
        test_identifier: IdentifierDto,
        test_content: str,
    ) -> None:
        """Test that assimilate knowledge reuses existing entity when identifier already exists."""

        # First, create an entity with the identifier
        first_request = AssimilateKnowledgeRequest(
            identifier=test_identifier,
            content="Initial content about the entity.",
        )

        first_result = await assimilate_knowledge_usecase.execute(first_request)
        first_entity_id = first_result.entity.id

        # Now assimilate more knowledge with the same identifier
        second_request = AssimilateKnowledgeRequest(
            identifier=test_identifier,
            content=test_content,
        )

        second_result = await assimilate_knowledge_usecase.execute(second_request)

        # Assert that the same entity was reused
        assert second_result.entity.id == first_entity_id

        # Verify both sets of facts are associated with the same entity
        found_entity = await assimilate_knowledge_usecase.repository.find_entity_by_id(
            str(first_entity_id)
        )
        assert found_entity is not None

        # Should have facts from both assimilation calls
        facts_with_sources = found_entity["facts_with_sources"]
        assert len(facts_with_sources) > 0

    @pytest.mark.asyncio
    async def test_assimilate_knowledge_with_history(
        self,
        assimilate_knowledge_usecase: AssimilateKnowledgeUseCaseImpl,
        test_identifier: IdentifierDto,
    ) -> None:
        """Test knowledge assimilation with conversation history for context."""

        history = [
            "Hello, I'm John and I'm 25 years old.",
            "I moved to Paris last year.",
        ]

        current_content = "I work as a software engineer now."

        # Act
        request = AssimilateKnowledgeRequest(
            identifier=test_identifier,
            content=current_content,
            history=history,
        )

        result = await assimilate_knowledge_usecase.execute(request)

        # Assert
        assert result.entity is not None
        assert result.source is not None
        assert len(result.assimilated_facts) > 0

        # The fact extractor should use the history for better context
        # We can't predict exact facts, but ensure the process completes successfully

    @pytest.mark.asyncio
    async def test_assimilate_knowledge_no_facts_extracted(
        self,
        assimilate_knowledge_usecase: AssimilateKnowledgeUseCaseImpl,
        test_identifier: IdentifierDto,
    ) -> None:
        """Test knowledge assimilation when no facts can be extracted from content."""

        # Content that should not yield any facts
        content = (
            "This is just a generic message with no specific information about me."
        )

        # Act
        request = AssimilateKnowledgeRequest(
            identifier=test_identifier,
            content=content,
        )

        result = await assimilate_knowledge_usecase.execute(request)

        # Assert
        assert result.entity is not None
        assert result.source is not None
        # The fact extractor might still extract some facts, or it might return empty
        # We just verify the process completes without error
        assert isinstance(result.assimilated_facts, list)

    @pytest.mark.asyncio
    async def test_assimilate_knowledge_multiple_facts(
        self,
        assimilate_knowledge_usecase: AssimilateKnowledgeUseCaseImpl,
        test_identifier: IdentifierDto,
    ) -> None:
        """Test assimilation of content that should yield multiple facts."""

        # Content designed to yield multiple distinct facts about the entity
        content = """I live in San Francisco, California.
        I work as a Senior Product Manager at a tech company. I graduated from Stanford University
        with a degree in Computer Science. I speak English, Spanish, and French fluently.
        My hobbies include photography, hiking, and playing the piano."""

        # Act
        request = AssimilateKnowledgeRequest(
            identifier=test_identifier,
            content=content,
        )

        result = await assimilate_knowledge_usecase.execute(request)
        print(f"---> result: {result}")

        # Assert
        assert result.entity is not None
        assert result.source is not None

        # Should extract multiple facts
        assert len(result.assimilated_facts) > 1

        # Verify all facts have proper structure
        fact_types: set[str] = set()
        for assimilated_fact in result.assimilated_facts:
            assert assimilated_fact.fact.name
            assert assimilated_fact.fact.type
            assert assimilated_fact.fact.fact_id
            assert assimilated_fact.relationship.verb
            assert 0.0 <= assimilated_fact.relationship.confidence_score <= 1.0
            fact_types.add(assimilated_fact.fact.type)

        # Should have variety in fact types
        assert len(fact_types) > 1

    @pytest.mark.asyncio
    async def test_assimilate_knowledge_different_languages(
        self,
        assimilate_knowledge_usecase: AssimilateKnowledgeUseCaseImpl,
        test_identifier: IdentifierDto,
    ) -> None:
        """Test knowledge assimilation with content in different languages."""

        # Test with Spanish content
        spanish_content = "Me llamo Mar√≠a Garc√≠a. Vivo en Barcelona y trabajo como profesora de matem√°ticas."

        request = AssimilateKnowledgeRequest(
            identifier=test_identifier,
            content=spanish_content,
        )

        result = await assimilate_knowledge_usecase.execute(request)
        print(f"---> result: {result}")

        # Assert
        assert result.entity is not None
        assert result.source is not None
        assert len(result.assimilated_facts) > 0

        # Fact names should be in original language, types and verbs in English
        for assimilated_fact in result.assimilated_facts:
            assert assimilated_fact.fact.name  # Could be in Spanish
            assert assimilated_fact.fact.type  # Should be in English
            assert assimilated_fact.relationship.verb  # Should be in English
</file>

<file path="apps/api/tests/features/graph/usecases/test_get_entity_usecase_integration.py">
"""Integration tests for GetEntityUseCaseImpl using real dependencies.

This module provides integration tests for the GetEntityUseCaseImpl
using the actual production implementation of AgeRepository.
"""

import uuid
from datetime import datetime

import asyncpg
import pytest

from app.features.graph.dtos.knowledge_dto import (
    AssimilateKnowledgeRequest,
    GetEntityResponse,
    IdentifierDto,
)
from app.features.graph.repositories.age_repository import AgeRepository
from app.features.graph.services.langchain_fact_extractor import LangChainFactExtractor
from app.features.graph.usecases.assimilate_knowledge_usecase import (
    AssimilateKnowledgeUseCaseImpl,
)
from app.features.graph.usecases.get_entity_usecase import GetEntityUseCaseImpl


@pytest.fixture
async def age_repository(postgres_pool: asyncpg.Pool) -> AgeRepository:
    """Fixture to get an AgeRepository instance."""
    return AgeRepository(postgres_pool, graph_name="test_graph")


@pytest.fixture
def langchain_fact_extractor() -> LangChainFactExtractor:
    """LangChainFactExtractor instance for testing."""
    return LangChainFactExtractor()


@pytest.fixture
async def assimilate_knowledge_usecase(
    age_repository: AgeRepository,
    langchain_fact_extractor: LangChainFactExtractor,
) -> AssimilateKnowledgeUseCaseImpl:
    """AssimilateKnowledgeUseCaseImpl instance for setting up test data."""
    return AssimilateKnowledgeUseCaseImpl(
        repository=age_repository, fact_extractor=langchain_fact_extractor
    )


@pytest.fixture
async def get_entity_usecase(
    age_repository: AgeRepository,
) -> GetEntityUseCaseImpl:
    """GetEntityUseCaseImpl instance for testing."""
    return GetEntityUseCaseImpl(repository=age_repository)


@pytest.fixture
def test_identifier() -> IdentifierDto:
    """Test identifier payload for integration testing."""
    return IdentifierDto(
        value=f"test.integration.{uuid.uuid4()}@example.com", type="email"
    )


@pytest.fixture
def test_content() -> str:
    """Test content for fact extraction."""
    return "I live in Paris and work as a Software Engineer. I really enjoy hiking on weekends."


class TestGetEntityUseCaseIntegration:
    """Integration tests for GetEntityUseCaseImpl.execute method."""

    @pytest.mark.asyncio
    async def test_get_entity_existing_entity(
        self,
        get_entity_usecase: GetEntityUseCaseImpl,
        assimilate_knowledge_usecase: AssimilateKnowledgeUseCaseImpl,
        test_identifier: IdentifierDto,
        test_content: str,
    ) -> None:
        """Test retrieving an existing entity by identifier."""

        # First, create an entity with facts using assimilate knowledge
        assimilate_request = AssimilateKnowledgeRequest(
            identifier=test_identifier,
            content=test_content,
        )
        _ = await assimilate_knowledge_usecase.execute(assimilate_request)

        # Now retrieve the entity using get entity use case
        result: GetEntityResponse = await get_entity_usecase.execute(
            identifier_value=test_identifier.value, identifier_type=test_identifier.type
        )
        print(f"{result=}")

        # Assert
        assert isinstance(result, GetEntityResponse)
        assert result.entity is not None
        assert result.identifier is not None
        assert result.facts is not None

        # Verify entity details
        assert result.entity.id is not None
        assert isinstance(result.entity.metadata, dict)

        # Verify identifier details
        assert result.identifier.identifier.value == test_identifier.value
        assert result.identifier.identifier.type == test_identifier.type
        assert result.identifier.relationship.is_primary is True
        assert result.identifier.relationship.created_at is not None

        # Verify facts were included (should have facts from assimilation)
        assert len(result.facts) > 0

        # Check structure of facts with sources
        for fact_with_source in result.facts:
            assert fact_with_source.fact is not None
            assert fact_with_source.fact.name is not None
            assert fact_with_source.fact.type is not None
            assert fact_with_source.fact.fact_id is not None
            assert fact_with_source.relationship is not None
            assert fact_with_source.relationship.verb is not None
            assert 0.0 <= fact_with_source.relationship.confidence_score <= 1.0
            assert fact_with_source.relationship.created_at is not None
            # Source should always be present for facts created through assimilation
            assert fact_with_source.source is not None
            assert fact_with_source.source.id is not None
            assert fact_with_source.source.content is not None
            assert fact_with_source.source.timestamp is not None
            assert isinstance(fact_with_source.source.timestamp, datetime)

    @pytest.mark.asyncio
    async def test_get_entity_with_multiple_facts(
        self,
        get_entity_usecase: GetEntityUseCaseImpl,
        assimilate_knowledge_usecase: AssimilateKnowledgeUseCaseImpl,
        test_identifier: IdentifierDto,
    ) -> None:
        """Test retrieving an entity with multiple facts from multiple assimilations."""

        # First assimilation
        first_content = "I live in Paris and work as a Software Engineer."
        first_request = AssimilateKnowledgeRequest(
            identifier=test_identifier,
            content=first_content,
        )
        _ = await assimilate_knowledge_usecase.execute(first_request)

        # Second assimilation with same entity
        second_content = "I enjoy hiking and photography as hobbies."
        second_request = AssimilateKnowledgeRequest(
            identifier=test_identifier,
            content=second_content,
        )
        _ = await assimilate_knowledge_usecase.execute(second_request)

        # Retrieve the entity
        result: GetEntityResponse = await get_entity_usecase.execute(
            identifier_value=test_identifier.value, identifier_type=test_identifier.type
        )

        # Assert
        assert result.entity is not None
        assert len(result.facts) > 1  # Should have facts from both assimilations

        # Verify all facts have proper structure
        fact_names: set[str] = set()
        for fact_with_source in result.facts:
            assert fact_with_source.fact.name
            assert fact_with_source.fact.type
            assert fact_with_source.fact.fact_id
            assert fact_with_source.relationship.verb
            # Source should always be present for facts created through assimilation
            assert fact_with_source.source is not None
            assert fact_with_source.source.id is not None
            assert fact_with_source.source.content is not None
            assert fact_with_source.source.timestamp is not None
            assert isinstance(fact_with_source.source.timestamp, datetime)
            fact_names.add(fact_with_source.fact.name)

        # Should have multiple distinct facts
        assert len(fact_names) > 1

    @pytest.mark.asyncio
    async def test_get_entity_not_found(
        self,
        get_entity_usecase: GetEntityUseCaseImpl,
    ) -> None:
        """Test that retrieving a non-existent entity raises HTTPException."""

        from fastapi import HTTPException

        # Try to get an entity that doesn't exist
        with pytest.raises(HTTPException) as exc_info:
            _ = await get_entity_usecase.execute(
                identifier_value="nonexistent@example.com", identifier_type="email"
            )

        # Assert
        assert exc_info.value.status_code == 404
        assert "not found" in str(exc_info.value.detail).lower()

    @pytest.mark.asyncio
    async def test_get_entity_different_identifier_types(
        self,
        get_entity_usecase: GetEntityUseCaseImpl,
        assimilate_knowledge_usecase: AssimilateKnowledgeUseCaseImpl,
        test_content: str,
    ) -> None:
        """Test retrieving entities with different identifier types."""

        # Create entity with phone identifier
        phone_identifier = IdentifierDto(
            value=f"+1234567890{uuid.uuid4().hex[:6]}", type="phone"
        )
        phone_request = AssimilateKnowledgeRequest(
            identifier=phone_identifier,
            content=test_content,
        )
        _ = await assimilate_knowledge_usecase.execute(phone_request)

        # Retrieve by phone identifier
        result: GetEntityResponse = await get_entity_usecase.execute(
            identifier_value=phone_identifier.value,
            identifier_type=phone_identifier.type,
        )

        # Assert
        assert result.entity is not None
        assert result.identifier.identifier.value == phone_identifier.value
        assert result.identifier.identifier.type == phone_identifier.type
        assert len(result.facts) > 0

    @pytest.mark.asyncio
    async def test_get_entity_entity_with_no_facts(
        self,
        get_entity_usecase: GetEntityUseCaseImpl,
        age_repository: AgeRepository,
        test_identifier: IdentifierDto,
    ) -> None:
        """Test retrieving an entity that exists but has no associated facts."""

        # Create entity manually without facts
        from app.features.graph.models import Entity, HasIdentifier, Identifier

        entity = Entity(id=uuid.uuid4())  # Will use default timestamp
        identifier = Identifier(value=test_identifier.value, type=test_identifier.type)
        relationship = HasIdentifier(
            from_entity_id=entity.id,
            to_identifier_value=identifier.value,
            is_primary=True,
        )  # Will use default timestamp

        _ = await age_repository.create_entity(entity, identifier, relationship)

        # Retrieve the entity
        result: GetEntityResponse = await get_entity_usecase.execute(
            identifier_value=test_identifier.value, identifier_type=test_identifier.type
        )

        # Assert
        assert result.entity is not None
        assert result.identifier is not None
        assert result.facts == []  # Should be empty list, not None

    @pytest.mark.asyncio
    async def test_get_entity_with_primary_identifier_selection(
        self,
        get_entity_usecase: GetEntityUseCaseImpl,
        age_repository: AgeRepository,
        test_identifier: IdentifierDto,
    ) -> None:
        """Test that when an entity has multiple identifiers, the primary one is returned."""

        # Create entity with multiple identifiers
        from app.features.graph.models import Entity, HasIdentifier, Identifier

        entity = Entity(id=uuid.uuid4())
        primary_identifier = Identifier(
            value=test_identifier.value, type=test_identifier.type
        )
        primary_relationship = HasIdentifier(
            from_entity_id=entity.id,
            to_identifier_value=primary_identifier.value,
            is_primary=True,
        )

        # Create the entity with primary identifier
        _ = await age_repository.create_entity(
            entity, primary_identifier, primary_relationship
        )

        # Add a secondary identifier manually
        secondary_identifier = Identifier(
            value=f"secondary.{uuid.uuid4().hex[:8]}@example.com", type="email"
        )
        secondary_relationship = HasIdentifier(
            from_entity_id=entity.id,
            to_identifier_value=secondary_identifier.value,
            is_primary=False,
        )

        # Add secondary identifier using the repository method
        # We need to create a new entity instance for the secondary identifier
        secondary_entity = Entity(id=entity.id)  # Same entity
        _ = await age_repository.create_entity(
            secondary_entity, secondary_identifier, secondary_relationship
        )

        # Retrieve the entity by primary identifier
        result: GetEntityResponse = await get_entity_usecase.execute(
            identifier_value=test_identifier.value, identifier_type=test_identifier.type
        )

        # Assert that the primary identifier is returned
        assert result.identifier.identifier.value == test_identifier.value
        assert result.identifier.identifier.type == test_identifier.type
        assert result.identifier.relationship.is_primary is True
</file>

<file path="apps/api/pyproject.toml">
[project]
name = "nous"
version = "0.1.0"
description = "FastAPI application with modular architecture"
authors = [
    {name = "Your Name", email = "your.email@example.com"}
]
readme = "README.md"
requires-python = ">=3.12"

dependencies = [
    "fastapi>=0.115.0",
    "uvicorn[standard]>=0.32.0",
    "python-multipart>=0.0.12",
    "pydantic>=2.10.0",
    "pydantic-settings>=2.6.0",
    "email-validator>=2.2.0",
    "python-jose[cryptography]>=3.3.0",
    "passlib>=1.7.4",
    "fastapi-cli>=0.0.8",
    "httpx>=0.28.1",
    "langchain>=0.3.27",
    "langchain-core>=0.3.76",
    "langchain-google-genai>=2.1.12",
    "asyncpg>=0.30.0",
    "sqlalchemy>=2.0.44",
    "alembic>=1.17.0",
    "slowapi>=0.1.9",
    "argon2-cffi>=25.1.0",
    "greenlet>=3.0.0",
]

[dependency-groups]
dev = [
    "ruff>=0.12.9",
    "pre-commit>=4.0.0",
    "pytest>=8.3.0",
    "pytest-asyncio>=0.24.0",
    "httpx>=0.28.0",
    "pytest-cov>=7.0.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["app"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
asyncio_mode = "auto"
addopts = "-v --tb=short"

[tool.ruff]
# Same as Black
line-length = 88
indent-width = 4

# Assume Python 3.12
target-version = "py312"

[tool.ruff.lint]

# Allow fix for all enabled rules (when `--fix`) is provided.
fixable = ["ALL"]
unfixable = []

[tool.ruff.format]
# Like Black, use double quotes for strings.
quote-style = "double"

# Like Black, indent with spaces, rather than tabs.
indent-style = "space"

# Like Black, respect magic trailing commas.
skip-magic-trailing-comma = false

# Like Black, automatically detect the appropriate line ending.
line-ending = "auto"

[tool.ruff.lint.per-file-ignores]
# Tests can use magic values, assertions, and relative imports
"**/test_*.py" = [
    "PLR2004",
    "S101",
    "TID252",
]

[tool.ruff.lint.isort]
known-first-party = ["app"]
</file>

<file path="apps/web/package.json">
{
  "name": "web",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vue-tsc -b && vite build",
    "preview": "vite preview",
    "lint": "eslint . --ext .vue,.js,.ts,.jsx,.tsx --fix",
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "dependencies": {
    "@vueuse/core": "^13.9.0",
    "class-variance-authority": "^0.7.1",
    "clsx": "^2.1.1",
    "cytoscape": "^3.33.1",
    "lucide-vue-next": "^0.546.0",
    "reka-ui": "^2.5.1",
    "tailwind-merge": "^3.3.1",
    "tw-animate-css": "^1.4.0",
    "vue": "^3.5.22",
    "vue-router": "^4.6.3"
  },
  "devDependencies": {
    "@tailwindcss/vite": "^4.1.14",
    "@types/cytoscape": "^3.31.0",
    "@types/node": "^24.6.0",
    "@vitejs/plugin-vue": "^6.0.1",
    "@vue/tsconfig": "^0.8.1",
    "tailwindcss": "^4.1.14",
    "typescript": "~5.9.3",
    "vite": "npm:rolldown-vite@7.1.14",
    "vue-tsc": "^3.1.0"
  },
  "pnpm": {
    "overrides": {
      "vite": "npm:rolldown-vite@7.1.14"
    }
  }
}
</file>

<file path="apps/api/app/features/auth/routes/api_keys.py">
"""API keys route handlers."""

from typing import Protocol
from uuid import UUID

from fastapi import APIRouter, Depends, HTTPException, status

from app.core.authentication import get_current_user
from app.core.schemas import AuthenticatedUser
from app.db.postgres.auth_session import get_auth_db_session
from app.features.auth.dtos import (
    CreateApiKeyRequest,
    CreateApiKeyResponse,
    ListApiKeysResponse,
)
from app.features.auth.usecases.create_api_key_usecase import CreateApiKeyUseCaseImpl
from app.features.auth.usecases.delete_api_key_usecase import DeleteApiKeyUseCaseImpl
from app.features.auth.usecases.list_api_keys_usecase import ListApiKeysUseCaseImpl


async def get_create_api_key_use_case():
    """Dependency injection for the create API key use case."""
    return CreateApiKeyUseCaseImpl(get_db_session=get_auth_db_session)


async def get_list_api_keys_use_case():
    """Dependency injection for the list API keys use case."""
    return ListApiKeysUseCaseImpl(get_db_session=get_auth_db_session)


async def get_delete_api_key_use_case():
    """Dependency injection for the delete API key use case."""
    return DeleteApiKeyUseCaseImpl(get_db_session=get_auth_db_session)


class CreateApiKeyUseCase(Protocol):
    """Protocol for the create API key use case."""

    async def execute(
        self, request: CreateApiKeyRequest, tenant_id: UUID
    ) -> CreateApiKeyResponse:
        """Create a new API key."""
        ...


class ListApiKeysUseCase(Protocol):
    """Protocol for the list API keys use case."""

    async def execute(self, tenant_id: UUID) -> ListApiKeysResponse:
        """List API keys for a tenant."""
        ...


class DeleteApiKeyUseCase(Protocol):
    """Protocol for the delete API key use case."""

    async def execute(self, api_key_id: str, tenant_id: UUID) -> dict[str, str]:
        """Delete an API key."""
        ...


router = APIRouter()


@router.post("/api-keys", response_model=CreateApiKeyResponse)
async def create_api_key(
    request: CreateApiKeyRequest,
    current_user: AuthenticatedUser = Depends(get_current_user),
    use_case: CreateApiKeyUseCase = Depends(get_create_api_key_use_case),
) -> CreateApiKeyResponse:
    """Create a new API key for programmatic access."""
    if current_user.tenant_id is None:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="User must be associated with a tenant to create API keys",
        )
    return await use_case.execute(request, current_user.tenant_id)


@router.get("/api-keys", response_model=ListApiKeysResponse)
async def list_api_keys(
    current_user: AuthenticatedUser = Depends(get_current_user),
    use_case: ListApiKeysUseCase = Depends(get_list_api_keys_use_case),
) -> ListApiKeysResponse:
    """List all API keys for the current tenant."""
    if current_user.tenant_id is None:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="User must be associated with a tenant to list API keys",
        )
    return await use_case.execute(current_user.tenant_id)


@router.delete("/api-keys/{api_key_id}")
async def delete_api_key(
    api_key_id: str,
    current_user: AuthenticatedUser = Depends(get_current_user),
    use_case: DeleteApiKeyUseCase = Depends(get_delete_api_key_use_case),
) -> dict[str, str]:
    """Delete an API key."""
    if current_user.tenant_id is None:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="User must be associated with a tenant to delete API keys",
        )
    return await use_case.execute(api_key_id, current_user.tenant_id)
</file>

<file path="apps/api/app/features/auth/routes/login.py">
"""Login route handler."""

from typing import Any, Protocol

from fastapi import APIRouter, Depends

from app.core.authentication import create_access_token, verify_password
from app.db.postgres.auth_session import get_auth_db_session
from app.features.auth.dtos.auth_dto import LoginRequest, LoginResponse
from app.features.auth.usecases.login_usecase import LoginUseCaseImpl


class PasswordVerifierImpl:
    """Wrapper for password verification to match protocol."""

    def verify(self, plain_password: str, hashed_password: str) -> bool:
        """Verify a password against its hash."""
        return verify_password(plain_password, hashed_password)


class TokenCreatorImpl:
    """Wrapper for token creation to match protocol."""

    def __call__(self, data: dict[str, Any], expires_delta=None) -> str:
        """Create an access token."""
        return create_access_token(data, expires_delta)


async def get_login_use_case():
    """Dependency injection for the login use case."""
    return LoginUseCaseImpl(
        password_verifier=PasswordVerifierImpl(),
        token_creator=TokenCreatorImpl(),
        get_db_session=get_auth_db_session,
    )


class LoginUseCase(Protocol):
    """Protocol for the login use case."""

    async def execute(self, email: str, password: str) -> LoginResponse:
        """Authenticate user and return token."""
        ...


router = APIRouter()


@router.post("/login", response_model=LoginResponse)
async def login_for_access_token(
    login_data: LoginRequest,
    use_case: LoginUseCase = Depends(get_login_use_case),
) -> LoginResponse:
    """Authenticate user and return JWT access token."""
    return await use_case.execute(login_data.email, login_data.password)
</file>

<file path="apps/web/src/main.ts">
import { createApp } from "vue";

import router from "./router";
import App from "./App.vue";
import "./style.css";

const app = createApp(App);
app.use(router);

// Theme management based on system preference
const applyTheme = (isDark: boolean) => {
  if (isDark) {
    document.documentElement.classList.add("dark");
  } else {
    document.documentElement.classList.remove("dark");
  }
};

// Check system preference with fallback for older browsers
const getSystemThemePreference = (): boolean => {
  if (typeof window !== "undefined" && window.matchMedia) {
    return window.matchMedia("(prefers-color-scheme: dark)").matches;
  }
  // Fallback: default to light theme for older browsers
  return false;
};

// Apply initial theme
applyTheme(getSystemThemePreference());

// Listen for system preference changes (with browser support check)
if (typeof window !== "undefined" && window.matchMedia) {
  const prefersDark = window.matchMedia("(prefers-color-scheme: dark)");

  // Use the modern addEventListener API if available, fallback to addListener
  if (prefersDark.addEventListener) {
    prefersDark.addEventListener("change", (e) => {
      applyTheme(e.matches);
    });
  } else {
    // Fallback for older Safari versions
    prefersDark.addListener((e) => {
      applyTheme(e.matches);
    });
  }
}

app.mount("#app");
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.env.test
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be added to the global gitignore or merged into this project gitignore.  For a PyCharm
#  project, it is generally recommended to include the .idea directory and its contents
#  in version control. However, specific files and directories within .idea might need
#  to be ignored based on project requirements.
.idea/

# Local History for Visual Studio Code
.history/

# Built Visual Studio Code Extensions
*.vsix

# UV package manager
.uv/

# FastAPI specific
.env.local
.env.production
.env.staging

# Database
*.db
*.sqlite
*.sqlite3
instance/

# Logs
logs/
*.log

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Coverage directory used by tools like istanbul
coverage/

# nyc test coverage
.nyc_output

# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)
.grunt

# Bower dependency directory (https://bower.io/)
bower_components

# node_modules (if using any Node.js tools)
node_modules/

# Turborepo cache
.turbo/

# Optional npm cache
.npm

# Optional eslint cache
.eslintcache

# Optional stylelint cache
.stylelintcache

# Microbundle cache
.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# parcel-bundler cache (https://parceljs.org/)
.cache
.parcel-cache

# Next.js build output
.next

# Nuxt.js build / generate output
.nuxt
dist

# Storybook build outputs
.out
.storybook-out
storybook-static

# Temporary folders
tmp/
temp/

# Editor directories and files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# OS generated files
*.swp
*.swo
*~

# Pre-commit hooks
.pre-commit-config.yaml

# Alembic migrations (uncomment if you want to ignore migration files)
# alembic/versions/*.py

# Local development overrides
docker-compose.override.yml
.env.override

# Backup files
*.bak
*.backup
*.old

# Generated documentation
docs/build/

# Test artifacts
.testmondata

# Profiling data
*.prof

.pnpm-store
</file>

<file path="apps/api/app/features/auth/models.py">
"""Database models for authentication and authorization."""

import uuid
from datetime import datetime

from sqlalchemy import Boolean, DateTime, ForeignKey, Integer, String, UniqueConstraint
from sqlalchemy import Enum as SAEnum
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import Mapped, mapped_column, relationship
from sqlalchemy.sql import func

from app.core.schemas import UserRole
from app.db.postgres.auth_session import Base


class Tenant(Base):
    """Tenant model representing isolated organizations."""

    __tablename__ = "tenants"

    id: Mapped[uuid.UUID] = mapped_column(
        UUID(as_uuid=True), primary_key=True, default=uuid.uuid4
    )
    name: Mapped[str] = mapped_column(String(255), nullable=False, unique=True)
    age_graph_name: Mapped[str] = mapped_column(
        String(100), nullable=False, unique=True
    )
    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), server_default=func.now()
    )

    # Relationships
    users: Mapped[list["User"]] = relationship(
        back_populates="tenant", cascade="all, delete-orphan"
    )
    api_keys: Mapped[list["ApiKey"]] = relationship(
        back_populates="tenant", cascade="all, delete-orphan"
    )


class User(Base):
    """User model for authentication."""

    __tablename__ = "users"

    id: Mapped[uuid.UUID] = mapped_column(
        UUID(as_uuid=True), primary_key=True, default=uuid.uuid4
    )
    email: Mapped[str] = mapped_column(String(255), nullable=False, unique=True)
    hashed_password: Mapped[str] = mapped_column(String(255), nullable=False)
    # SUPER_ADMINs are not tied to a single tenant
    tenant_id: Mapped[uuid.UUID | None] = mapped_column(  # <-- Make nullable
        UUID(as_uuid=True),
        ForeignKey("tenants.id"),
        nullable=True,  # <-- Make nullable
    )

    # --- Add the role column ---
    role: Mapped[UserRole] = mapped_column(
        SAEnum(UserRole), nullable=False, default=UserRole.TENANT_USER
    )

    is_active: Mapped[bool] = mapped_column(Boolean, default=True, nullable=False)
    failed_login_attempts: Mapped[int] = mapped_column(
        Integer, default=0, nullable=False
    )
    locked_until: Mapped[datetime | None] = mapped_column(
        DateTime(timezone=True), nullable=True
    )
    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), server_default=func.now()
    )

    # Relationships
    # --- Make tenant relationship optional ---
    tenant: Mapped[Tenant | None] = relationship(
        back_populates="users"
    )  # <-- Make nullable


class ApiKey(Base):
    """API key model for programmatic access."""

    __tablename__ = "api_keys"

    id: Mapped[uuid.UUID] = mapped_column(
        UUID(as_uuid=True), primary_key=True, default=uuid.uuid4
    )
    name: Mapped[str] = mapped_column(String(100), nullable=False)
    key_prefix: Mapped[str] = mapped_column(String(10), nullable=False, unique=True)
    hashed_key: Mapped[str] = mapped_column(String(255), nullable=False)
    tenant_id: Mapped[uuid.UUID] = mapped_column(
        UUID(as_uuid=True), ForeignKey("tenants.id"), nullable=False
    )
    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), server_default=func.now()
    )
    expires_at: Mapped[datetime | None] = mapped_column(
        DateTime(timezone=True), nullable=True
    )
    last_used_at: Mapped[datetime | None] = mapped_column(
        DateTime(timezone=True), nullable=True
    )

    # Relationships
    tenant: Mapped["Tenant"] = relationship(back_populates="api_keys")

    # Constraints
    __table_args__ = (
        UniqueConstraint("name", "tenant_id", name="unique_api_key_name_per_tenant"),
    )
</file>

<file path="apps/api/app/features/auth/usecases/signup_tenant_usecase.py">
"""Use case for signing up a new tenant with user and graph database."""

from collections.abc import Awaitable
from contextlib import AbstractAsyncContextManager
from typing import Callable, Protocol
from uuid import uuid4

import asyncpg
from fastapi import HTTPException, status
from sqlalchemy.exc import IntegrityError
from sqlalchemy.ext.asyncio import AsyncSession

from app.features.auth.dtos import CreateTenantRequest, CreateTenantResponse
from app.features.auth.models import Tenant, User, UserRole


class PasswordHasher(Protocol):
    """Protocol for password hashing operations."""

    def hash(self, secret: str | bytes, **kwargs) -> str:
        """Hash a password or secret."""
        ...


class SignupTenantUseCaseImpl:
    """Implementation of the signup tenant use case."""

    def __init__(
        self,
        password_hasher: PasswordHasher,
        get_db_session: Callable[[], AbstractAsyncContextManager[AsyncSession]],
        get_db_pool: Callable[[], Awaitable[asyncpg.Pool]],
    ):
        """Initialize the use case with dependencies.

        Args:
            password_hasher: Service for hashing passwords
            get_db_session: Function to get database session
            get_db_pool: Function to get graph database pool
        """
        self.password_hasher = password_hasher
        self.get_auth_db_session = get_db_session
        self.get_graph_db_pool = get_db_pool

    async def execute(self, request: CreateTenantRequest) -> CreateTenantResponse:
        """Create a new tenant with an initial user and AGE graph.

        Args:
            request: The signup request containing tenant details

        Returns:
            Response with success message and IDs

        Raises:
            HTTPException: With appropriate status codes for validation and creation errors
        """
        # Validate input
        if len(request.name) < 3 or len(request.name) > 50:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Tenant name must be between 3 and 50 characters",
            )

        if not request.name.replace("-", "").replace("_", "").isalnum():
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Tenant name can only contain alphanumeric characters, hyphens, and underscores",
            )

        if len(request.password) < 8:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Password must be at least 8 characters long",
            )

        async with self.get_auth_db_session() as session:
            async with session.begin():
                try:
                    # Generate unique graph name
                    graph_name = self._generate_unique_graph_name()

                    # Create tenant
                    tenant = Tenant(name=request.name, age_graph_name=graph_name)
                    session.add(tenant)
                    await session.flush()  # Get the tenant ID

                    # Hash password
                    hashed_password = self.password_hasher.hash(request.password)

                    # Create user
                    user = User(
                        email=request.email,
                        hashed_password=hashed_password,
                        tenant_id=tenant.id,
                        role=UserRole.TENANT_ADMIN,  # <-- Set role to TENANT_ADMIN
                    )
                    session.add(user)
                    await session.flush()

                    # Create AGE graph
                    pool = await self.get_graph_db_pool()
                    async with pool.acquire() as conn:
                        await conn.execute("LOAD 'age';")
                        await conn.execute(
                            "SET search_path = ag_catalog, '$user', public;"
                        )
                        await conn.execute("SELECT create_graph($1)", graph_name)

                    return CreateTenantResponse(
                        message="Tenant created successfully",
                        tenant_id=str(tenant.id),
                        user_id=str(user.id),
                    )

                except IntegrityError:
                    await session.rollback()
                    raise HTTPException(
                        status_code=status.HTTP_400_BAD_REQUEST,
                        detail="Tenant name or email already exists",
                    )
                except Exception as e:
                    await session.rollback()
                    # Log the error (in production, use proper logging)
                    print(f"Signup error: {e}")
                    raise HTTPException(
                        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                        detail="Failed to create tenant",
                    )

    def _generate_unique_graph_name(self) -> str:
        """Generate a unique graph name for a tenant."""
        return f"nous_graph_{uuid4().hex}"
</file>

<file path="apps/web/src/features/graph/views/HomeView.vue">
<script setup lang="ts">
import { ref, watch, onMounted, onUnmounted, computed } from "vue";
import cytoscape, { type Core, type ElementDefinition } from "cytoscape";

import { Input } from "@/components/ui/input";
import { Button } from "@/components/ui/button";
import { Navigation } from "@/components/layout/navigation";
import {
  useFindEntityByIdentifier,
  type FindEntityParams,
} from "@/features/graph/api/graphApi";

// --- Cytoscape Refs ---
const cyContainer = ref<HTMLDivElement | null>(null);
const cyInstance = ref<Core | null>(null);

// --- Search State ---
const searchTypeInput = ref("email");
const searchValueInput = ref("");
const searchParams = ref<FindEntityParams>({ type: "", value: "" });

// --- API Data ---
const {
  data: entityData,
  isFetching: isSearching,
  error: searchError,
  execute: executeSearch,
} = useFindEntityByIdentifier(searchParams);
watch(entityData, (newValue) => {
  console.log("entityData has changed to:", newValue);
});

// --- Search Logic ---
const handleSearch = () => {
  if (!searchTypeInput.value.trim() || !searchValueInput.value.trim()) {
    console.warn("Please fill in both type and value");
    return;
  }

  searchParams.value = {
    type: searchTypeInput.value.trim(),
    value: searchValueInput.value.trim(),
  };

  // Force a refetch even if parameters haven't changed
  executeSearch();
};

// --- Cytoscape Data Transformation ---
const cytoscapeElements = computed<ElementDefinition[]>(() => {
  if (!entityData.value) {
    return [];
  }

  const { entity, identifier, facts } = entityData.value;
  const elements: ElementDefinition[] = [];
  const processedSources = new Set<string>();

  // 1. Add Entity Node
  elements.push({
    group: "nodes",
    data: {
      id: entity.id,
      label: `Entity\n(${identifier.identifier.type})`,
      type: "Entity",
      created_at: entity.created_at,
      metadata: entity.metadata,
    },
  });

  // 2. Add Identifier Node
  const identifierId = `identifier-${identifier.identifier.type}-${identifier.identifier.value}`;
  elements.push({
    group: "nodes",
    data: {
      id: identifierId,
      label: identifier.identifier.value,
      type: "Identifier",
      value: identifier.identifier.value,
      identifier_type: identifier.identifier.type,
    },
  });

  // Edge: Entity -> Identifier
  elements.push({
    group: "edges",
    data: {
      id: `edge-entity-identifier-${entity.id}-${identifierId}`,
      source: entity.id,
      target: identifierId,
      label: "HAS_IDENTIFIER",
      ...identifier.relationship,
    },
  });

  // 3. Add Fact and Source Nodes and Edges
  facts.forEach((factWithSource, factIndex) => {
    const { fact, relationship, source } = factWithSource;

    // Generate unique fact ID
    const factId =
      fact.fact_id ||
      `fact-${entity.id}-${fact.type}-${fact.name}-${factIndex}`;

    // Add Fact Node
    elements.push({
      group: "nodes",
      data: {
        id: factId,
        label: `${fact.type}\n${fact.name}`,
        type: "Fact",
        name: fact.name,
        fact_type: fact.type,
        fact_id: fact.fact_id,
      },
    });

    // Edge: Entity -> Fact
    elements.push({
      group: "edges",
      data: {
        id: `edge-entity-fact-${entity.id}-${factId}`,
        source: entity.id,
        target: factId,
        label: relationship.verb,
        ...relationship,
      },
    });

    // Add Source Node (if it exists and not already processed)
    if (source && !processedSources.has(source.id)) {
      processedSources.add(source.id);
      const sourceId = `source-${source.id}`;

      elements.push({
        group: "nodes",
        data: {
          id: sourceId,
          label: `Source\n(${source.content.substring(0, 20)}...)`,
          type: "Source",
          content: source.content,
          timestamp: source.timestamp,
          source_id: source.id,
        },
      });

      // Edge: Fact -> Source
      elements.push({
        group: "edges",
        data: {
          id: `edge-fact-source-${factId}-${sourceId}`,
          source: factId,
          target: sourceId,
          label: "DERIVED_FROM",
        },
      });
    }
  });

  return elements;
});

// --- Cytoscape Initialization ---
onMounted(() => {
  if (cyContainer.value) {
    cyInstance.value = cytoscape({
      container: cyContainer.value,
      elements: [],

      style: [
        {
          selector: "node",
          style: {
            "background-color": "#999",
            label: "data(label)",
            color: "#ffffff",
            "text-valign": "center",
            "text-halign": "center",
            "font-size": "10px",
            "text-wrap": "wrap",
            "text-max-width": "80px",
            width: "60px",
            height: "60px",
            "border-width": 2,
            "border-color": "#fff",
          },
        },
        {
          selector: "edge",
          style: {
            width: 2,
            "line-color": "#bbb",
            "target-arrow-color": "#bbb",
            "target-arrow-shape": "triangle",
            "curve-style": "bezier",
            label: "data(label)",
            color: "#f5c842",
            "font-size": "8px",
            "text-rotation": "autorotate",
            "text-margin-y": -10,
          },
        },
        // Node type styling
        {
          selector: "node[type='Entity']",
          style: { "background-color": "#3b82f6" },
        },
        {
          selector: "node[type='Fact']",
          style: { "background-color": "#10b981" },
        },
        {
          selector: "node[type='Identifier']",
          style: { "background-color": "#f97316" },
        },
        {
          selector: "node[type='Source']",
          style: { "background-color": "#a855f7" },
        },
      ],

      layout: {
        name: "grid",
      },
    });
  }
});

onUnmounted(() => {
  cyInstance.value?.destroy();
});

// --- Cytoscape Updates ---
watch(cytoscapeElements, (newElements, oldElements) => {
  const cy = cyInstance.value;
  if (!cy) return;

  // If this is the first load or elements changed significantly, replace all
  if (!oldElements || oldElements.length === 0) {
    cy.elements().remove();
    cy.add(newElements);

    // Apply layout with animation for new data
    const layout = cy.layout({
      name: "cose",
      fit: true,
      padding: 30,
      animate: true,
      animationDuration: 500,
    });
    layout.run();
  } else {
    // For updates, we could implement more granular updates here
    // For now, we'll do a full replacement but this could be optimized
    cy.elements().remove();
    cy.add(newElements);

    // Apply layout without animation for updates
    const layout = cy.layout({
      name: "cose",
      fit: true,
      padding: 30,
      animate: false,
    });
    layout.run();
  }
});
</script>

<template>
  <div class="min-h-screen bg-background">
    <Navigation />

    <main class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8">
      <div class="mb-8 lg:mb-12">
        <h1
          class="text-2xl sm:text-3xl lg:text-4xl font-bold text-foreground mb-4"
        >
          Knowledge Graph Explorer
        </h1>
        <p class="text-base sm:text-lg text-muted-foreground max-w-3xl">
          Search for entities by their identifiers and explore their
          relationships, facts, and sources.
        </p>
      </div>

      <!-- Search Entity Section -->
      <div
        class="bg-card rounded-lg shadow-md p-4 sm:p-6 lg:p-8 mb-8 lg:mb-12 border border-border"
      >
        <h2 class="text-xl sm:text-2xl font-semibold text-card-foreground mb-6">
          Search Entity
        </h2>

        <div class="grid grid-cols-1 md:grid-cols-3 gap-4 lg:gap-6 items-end">
          <div class="flex flex-col gap-2">
            <label
              for="search-type"
              class="text-sm font-medium text-foreground"
            >
              Identifier Type
            </label>
            <Input
              id="search-type"
              v-model="searchTypeInput"
              placeholder="e.g., email, phone, name"
              class="w-full text-foreground"
            />
          </div>

          <div class="flex flex-col gap-2">
            <label
              for="search-value"
              class="text-sm font-medium text-foreground"
            >
              Identifier Value
            </label>
            <Input
              id="search-value"
              v-model="searchValueInput"
              placeholder="Enter search value"
              class="w-full text-foreground"
              @keyup.enter="handleSearch"
            />
          </div>

          <Button
            @click="handleSearch"
            :disabled="
              isSearching || !searchTypeInput.trim() || !searchValueInput.trim()
            "
            class="w-full md:w-auto h-10"
          >
            <span v-if="isSearching">Searching...</span>
            <span v-else>Search</span>
          </Button>
        </div>

        <!-- Display Search Error -->
        <div
          v-if="searchError"
          class="mt-4 p-3 bg-destructive/10 border border-destructive/20 rounded-md"
        >
          <p class="text-destructive text-sm">
            Search failed:
            {{ searchError.message || "An error occurred while searching." }}
          </p>
        </div>
      </div>

      <!-- Cytoscape Graph Visualization Area -->
      <div
        class="bg-card rounded-lg shadow-md border border-border overflow-hidden relative"
        style="height: 600px"
      >
        <!-- Loading overlay -->
        <div
          v-if="isSearching"
          class="absolute inset-0 bg-background/80 backdrop-blur-sm flex items-center justify-center z-10"
        >
          <div class="text-center">
            <div
              class="animate-spin rounded-full h-8 w-8 border-b-2 border-primary mx-auto mb-2"
            ></div>
            <p class="text-sm text-muted-foreground">Loading graph data...</p>
          </div>
        </div>

        <!-- Cytoscape container -->
        <div ref="cyContainer" class="w-full h-full"></div>

        <!-- Empty state -->
        <div
          v-if="!isSearching && !entityData && !searchError"
          class="absolute inset-0 flex items-center justify-center"
        >
          <div class="text-center text-muted-foreground">
            <p class="text-lg mb-2">No data to display</p>
            <p class="text-sm">
              Search for an entity to visualize its knowledge graph
            </p>
          </div>
        </div>
      </div>
    </main>
  </div>
</template>

<style scoped>
/* Additional custom styles if needed */
</style>
</file>

</files>
